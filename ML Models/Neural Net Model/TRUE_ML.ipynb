{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import tensorflow as tf\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load .env enviroment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv('ALPACA_API_KEY')\n",
    "alpaca_secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "\n",
    "api = tradeapi.REST('PKYSIX5VD8DLHIZOILZS', 'Yv4AYGCNo9puqbXGPq2zF1sNrzy63CWCrWNJnOse', api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "\n",
    "executable_path = {'executable_path': 'chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "def getHeadlines(ticker):\n",
    "    \n",
    "    print(f\"Currently Running {ticker} Ticker!\")\n",
    "    d = {'Headline': [], 'Date': []}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    for x in range(0,25):\n",
    "        print(f\"Loading page number {x}\")\n",
    "        url = f\"https://www.marketwatch.com/investing/stock/{ticker}/moreheadlines?channel=MarketWatch&pageNumber={x}\"\n",
    "        browser.visit(url)\n",
    "        if (browser.is_element_not_present_by_css('h2[class=\"error__header\"]') == False):\n",
    "            print(\"Got To Try a Differnt Link!\")\n",
    "            url = f\"https://www.marketwatch.com/investing/index/{ticker}/moreheadlines?channel=MarketWatch&pageNumber={x}\"\n",
    "            browser.visit(url)\n",
    "        else:\n",
    "            this = 1+1\n",
    "            \n",
    "        if (browser.is_element_not_present_by_css('span[class=\"article__timestamp\"]') == True):\n",
    "            print(f\"OOPS, sorry no more pages! There's only {x} pages for this Ticker!\")\n",
    "            break       \n",
    "        else:\n",
    "            for y in range(0,len(browser.find_by_css('h3[class=\"article__headline\"]'))):\n",
    "                df = df.append({'Headline':browser.find_by_css('h3[class=\"article__headline\"]')[y].text,\n",
    "                                'Date':browser.find_by_css('span[class=\"article__timestamp\"]')[y].text},ignore_index=True)\n",
    "                \n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "    df = df.sort_values(by=['Date'],ascending=True)\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    df.to_csv(f\"NEW_{ticker}_HEADLINES.csv\",header=True,index=False)\n",
    "    print(f\"Done Running {ticker} Ticker! - Total of {len(df)} rows! & {x} pages!\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Running AAPL Ticker!\n",
      "Loading page number 0\n",
      "Loading page number 1\n",
      "Loading page number 2\n",
      "Loading page number 3\n",
      "Loading page number 4\n",
      "Loading page number 5\n",
      "Loading page number 6\n",
      "Loading page number 7\n",
      "Loading page number 8\n",
      "Loading page number 9\n",
      "Loading page number 10\n",
      "Loading page number 11\n",
      "Loading page number 12\n",
      "Loading page number 13\n",
      "Loading page number 14\n",
      "Loading page number 15\n",
      "Loading page number 16\n",
      "Loading page number 17\n",
      "Loading page number 18\n",
      "Loading page number 19\n",
      "Loading page number 20\n",
      "Loading page number 21\n",
      "Loading page number 22\n",
      "Loading page number 23\n",
      "Loading page number 24\n",
      "Loading page number 25\n",
      "Loading page number 26\n",
      "Loading page number 27\n",
      "Loading page number 28\n",
      "Loading page number 29\n",
      "Loading page number 30\n",
      "Loading page number 31\n",
      "Loading page number 32\n",
      "Loading page number 33\n",
      "Loading page number 34\n",
      "Loading page number 35\n",
      "Loading page number 36\n",
      "Loading page number 37\n",
      "Loading page number 38\n",
      "Loading page number 39\n",
      "Loading page number 40\n",
      "Loading page number 41\n",
      "Loading page number 42\n",
      "Loading page number 43\n",
      "Loading page number 44\n",
      "Loading page number 45\n",
      "Loading page number 46\n",
      "Loading page number 47\n",
      "Loading page number 48\n",
      "Loading page number 49\n",
      "Loading page number 50\n",
      "Loading page number 51\n",
      "Loading page number 52\n",
      "Loading page number 53\n",
      "Loading page number 54\n",
      "Loading page number 55\n",
      "Loading page number 56\n",
      "Loading page number 57\n",
      "Loading page number 58\n",
      "Loading page number 59\n",
      "Loading page number 60\n",
      "Loading page number 61\n",
      "Loading page number 62\n",
      "Loading page number 63\n",
      "Loading page number 64\n",
      "Loading page number 65\n",
      "Loading page number 66\n",
      "Loading page number 67\n",
      "Loading page number 68\n",
      "Loading page number 69\n",
      "Loading page number 70\n",
      "Loading page number 71\n",
      "Loading page number 72\n",
      "Loading page number 73\n",
      "Loading page number 74\n",
      "Loading page number 75\n",
      "Loading page number 76\n",
      "Loading page number 77\n",
      "Loading page number 78\n",
      "Loading page number 79\n",
      "Loading page number 80\n",
      "Loading page number 81\n",
      "Loading page number 82\n",
      "Loading page number 83\n",
      "Loading page number 84\n",
      "Loading page number 85\n",
      "Loading page number 86\n",
      "Loading page number 87\n",
      "Loading page number 88\n",
      "Loading page number 89\n",
      "Loading page number 90\n",
      "Loading page number 91\n",
      "Loading page number 92\n",
      "Loading page number 93\n",
      "Loading page number 94\n",
      "Loading page number 95\n",
      "Loading page number 96\n",
      "Loading page number 97\n",
      "Loading page number 98\n",
      "Loading page number 99\n",
      "Loading page number 100\n",
      "Loading page number 101\n",
      "Loading page number 102\n",
      "Loading page number 103\n",
      "Loading page number 104\n",
      "Loading page number 105\n",
      "Loading page number 106\n",
      "Loading page number 107\n",
      "Loading page number 108\n",
      "Loading page number 109\n",
      "Loading page number 110\n",
      "Loading page number 111\n",
      "Loading page number 112\n",
      "Loading page number 113\n",
      "Loading page number 114\n",
      "Loading page number 115\n",
      "Loading page number 116\n",
      "Loading page number 117\n",
      "Loading page number 118\n",
      "Loading page number 119\n",
      "Loading page number 120\n",
      "Loading page number 121\n",
      "Loading page number 122\n",
      "Loading page number 123\n",
      "Loading page number 124\n",
      "Loading page number 125\n",
      "Loading page number 126\n",
      "Loading page number 127\n",
      "Loading page number 128\n",
      "Loading page number 129\n",
      "Loading page number 130\n",
      "Loading page number 131\n",
      "Loading page number 132\n",
      "Loading page number 133\n",
      "Loading page number 134\n",
      "Loading page number 135\n",
      "Loading page number 136\n",
      "Loading page number 137\n",
      "Loading page number 138\n",
      "Loading page number 139\n",
      "Loading page number 140\n",
      "Loading page number 141\n",
      "Loading page number 142\n",
      "Loading page number 143\n",
      "Loading page number 144\n",
      "Loading page number 145\n",
      "Loading page number 146\n",
      "Loading page number 147\n",
      "Loading page number 148\n",
      "Loading page number 149\n",
      "Loading page number 150\n",
      "Loading page number 151\n",
      "Loading page number 152\n",
      "Loading page number 153\n",
      "Loading page number 154\n",
      "Loading page number 155\n",
      "Loading page number 156\n",
      "Loading page number 157\n",
      "Loading page number 158\n",
      "Loading page number 159\n",
      "Loading page number 160\n",
      "Loading page number 161\n",
      "Loading page number 162\n",
      "Loading page number 163\n",
      "Loading page number 164\n",
      "Loading page number 165\n",
      "Loading page number 166\n",
      "Loading page number 167\n",
      "Loading page number 168\n",
      "Loading page number 169\n",
      "Loading page number 170\n",
      "Loading page number 171\n",
      "Loading page number 172\n",
      "Loading page number 173\n",
      "Loading page number 174\n",
      "Loading page number 175\n",
      "Loading page number 176\n",
      "Loading page number 177\n",
      "Loading page number 178\n",
      "Loading page number 179\n",
      "Loading page number 180\n",
      "Loading page number 181\n",
      "Loading page number 182\n",
      "Loading page number 183\n",
      "Loading page number 184\n",
      "Loading page number 185\n",
      "Loading page number 186\n",
      "Loading page number 187\n",
      "Loading page number 188\n",
      "Loading page number 189\n",
      "Loading page number 190\n",
      "Loading page number 191\n",
      "Loading page number 192\n",
      "Loading page number 193\n",
      "Loading page number 194\n",
      "Loading page number 195\n",
      "Loading page number 196\n",
      "Loading page number 197\n",
      "Loading page number 198\n",
      "Loading page number 199\n",
      "Loading page number 200\n",
      "Loading page number 201\n",
      "Loading page number 202\n",
      "Loading page number 203\n",
      "Loading page number 204\n",
      "Loading page number 205\n",
      "Loading page number 206\n",
      "Loading page number 207\n",
      "Loading page number 208\n",
      "Loading page number 209\n",
      "Loading page number 210\n",
      "Loading page number 211\n",
      "Loading page number 212\n",
      "Loading page number 213\n",
      "Loading page number 214\n",
      "Loading page number 215\n",
      "Loading page number 216\n",
      "Loading page number 217\n",
      "Loading page number 218\n",
      "Loading page number 219\n",
      "Loading page number 220\n",
      "Loading page number 221\n",
      "Loading page number 222\n",
      "Loading page number 223\n",
      "Loading page number 224\n",
      "Loading page number 225\n",
      "Loading page number 226\n",
      "Loading page number 227\n",
      "Loading page number 228\n",
      "Loading page number 229\n",
      "Loading page number 230\n",
      "Loading page number 231\n",
      "Loading page number 232\n",
      "Loading page number 233\n",
      "Loading page number 234\n",
      "Loading page number 235\n",
      "Loading page number 236\n",
      "Loading page number 237\n",
      "Loading page number 238\n",
      "Loading page number 239\n",
      "Loading page number 240\n",
      "Loading page number 241\n",
      "Loading page number 242\n",
      "Loading page number 243\n",
      "Loading page number 244\n",
      "Loading page number 245\n",
      "Loading page number 246\n",
      "Loading page number 247\n",
      "Loading page number 248\n",
      "Loading page number 249\n",
      "Loading page number 250\n",
      "Loading page number 251\n",
      "Loading page number 252\n",
      "Loading page number 253\n",
      "Loading page number 254\n",
      "Loading page number 255\n",
      "Loading page number 256\n",
      "Loading page number 257\n",
      "Loading page number 258\n",
      "Loading page number 259\n",
      "Loading page number 260\n",
      "Loading page number 261\n",
      "Loading page number 262\n",
      "Loading page number 263\n",
      "Loading page number 264\n",
      "Loading page number 265\n",
      "Loading page number 266\n",
      "Loading page number 267\n",
      "Loading page number 268\n",
      "Loading page number 269\n",
      "Loading page number 270\n",
      "Loading page number 271\n",
      "Loading page number 272\n",
      "Loading page number 273\n",
      "Loading page number 274\n",
      "Loading page number 275\n",
      "Loading page number 276\n",
      "Loading page number 277\n",
      "Loading page number 278\n",
      "Loading page number 279\n",
      "Loading page number 280\n",
      "Loading page number 281\n",
      "Loading page number 282\n",
      "Loading page number 283\n",
      "Loading page number 284\n",
      "Loading page number 285\n",
      "Loading page number 286\n",
      "Loading page number 287\n",
      "Loading page number 288\n",
      "Loading page number 289\n",
      "Loading page number 290\n",
      "Loading page number 291\n",
      "Loading page number 292\n",
      "Loading page number 293\n",
      "Loading page number 294\n",
      "Loading page number 295\n",
      "Loading page number 296\n",
      "Loading page number 297\n",
      "Loading page number 298\n",
      "Loading page number 299\n",
      "Loading page number 300\n",
      "Loading page number 301\n",
      "Loading page number 302\n",
      "Loading page number 303\n",
      "Loading page number 304\n",
      "Loading page number 305\n",
      "Loading page number 306\n",
      "Loading page number 307\n",
      "Loading page number 308\n",
      "Loading page number 309\n",
      "Loading page number 310\n",
      "Loading page number 311\n",
      "Loading page number 312\n",
      "Loading page number 313\n",
      "Loading page number 314\n",
      "Loading page number 315\n",
      "Loading page number 316\n",
      "Loading page number 317\n",
      "Loading page number 318\n",
      "Loading page number 319\n",
      "Loading page number 320\n",
      "Loading page number 321\n",
      "Loading page number 322\n",
      "Loading page number 323\n",
      "Loading page number 324\n",
      "Loading page number 325\n",
      "Loading page number 326\n",
      "Loading page number 327\n",
      "Loading page number 328\n",
      "Loading page number 329\n",
      "Loading page number 330\n",
      "Loading page number 331\n",
      "Loading page number 332\n",
      "Loading page number 333\n",
      "Loading page number 334\n",
      "Loading page number 335\n",
      "Loading page number 336\n",
      "Loading page number 337\n",
      "Loading page number 338\n",
      "Loading page number 339\n",
      "Loading page number 340\n",
      "Loading page number 341\n",
      "Loading page number 342\n",
      "Loading page number 343\n",
      "Loading page number 344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page number 345\n",
      "Loading page number 346\n",
      "Loading page number 347\n",
      "Loading page number 348\n",
      "Loading page number 349\n",
      "Loading page number 350\n",
      "Loading page number 351\n",
      "Loading page number 352\n",
      "Loading page number 353\n",
      "Loading page number 354\n",
      "Loading page number 355\n",
      "Loading page number 356\n",
      "Loading page number 357\n",
      "Loading page number 358\n",
      "Loading page number 359\n",
      "Loading page number 360\n",
      "Loading page number 361\n",
      "Loading page number 362\n",
      "Loading page number 363\n",
      "Loading page number 364\n",
      "Loading page number 365\n",
      "Loading page number 366\n",
      "Loading page number 367\n",
      "Loading page number 368\n",
      "Loading page number 369\n",
      "Loading page number 370\n",
      "Loading page number 371\n",
      "Loading page number 372\n",
      "Loading page number 373\n",
      "Loading page number 374\n",
      "Loading page number 375\n",
      "Loading page number 376\n",
      "Loading page number 377\n",
      "Loading page number 378\n",
      "Loading page number 379\n",
      "Loading page number 380\n",
      "Loading page number 381\n",
      "Loading page number 382\n",
      "Loading page number 383\n",
      "Loading page number 384\n",
      "Loading page number 385\n",
      "Loading page number 386\n",
      "Loading page number 387\n",
      "Loading page number 388\n",
      "Loading page number 389\n",
      "Loading page number 390\n",
      "Loading page number 391\n",
      "Loading page number 392\n",
      "Loading page number 393\n",
      "Loading page number 394\n",
      "Loading page number 395\n",
      "Loading page number 396\n",
      "Loading page number 397\n",
      "Loading page number 398\n",
      "Loading page number 399\n",
      "Loading page number 400\n",
      "Loading page number 401\n",
      "Loading page number 402\n",
      "Loading page number 403\n",
      "Loading page number 404\n",
      "Loading page number 405\n",
      "Loading page number 406\n",
      "Loading page number 407\n",
      "Loading page number 408\n",
      "Loading page number 409\n",
      "Loading page number 410\n",
      "Loading page number 411\n",
      "Loading page number 412\n",
      "Loading page number 413\n",
      "Loading page number 414\n",
      "Loading page number 415\n",
      "Loading page number 416\n",
      "Loading page number 417\n",
      "Loading page number 418\n",
      "Loading page number 419\n",
      "Loading page number 420\n",
      "Loading page number 421\n",
      "Loading page number 422\n",
      "Loading page number 423\n",
      "Loading page number 424\n",
      "Loading page number 425\n",
      "Loading page number 426\n",
      "Loading page number 427\n",
      "Loading page number 428\n",
      "Loading page number 429\n",
      "Loading page number 430\n",
      "Loading page number 431\n",
      "Loading page number 432\n",
      "Loading page number 433\n",
      "Loading page number 434\n",
      "Loading page number 435\n",
      "Loading page number 436\n",
      "Loading page number 437\n",
      "Loading page number 438\n",
      "Loading page number 439\n",
      "Loading page number 440\n",
      "Loading page number 441\n",
      "Loading page number 442\n",
      "Loading page number 443\n",
      "Loading page number 444\n",
      "Loading page number 445\n",
      "Loading page number 446\n",
      "Loading page number 447\n",
      "Loading page number 448\n",
      "Loading page number 449\n",
      "Loading page number 450\n",
      "Loading page number 451\n",
      "Loading page number 452\n",
      "Loading page number 453\n",
      "Loading page number 454\n",
      "Loading page number 455\n",
      "Loading page number 456\n",
      "Loading page number 457\n",
      "Loading page number 458\n",
      "Loading page number 459\n",
      "Loading page number 460\n",
      "Loading page number 461\n",
      "Loading page number 462\n",
      "Loading page number 463\n",
      "Loading page number 464\n",
      "Loading page number 465\n",
      "Loading page number 466\n",
      "Loading page number 467\n",
      "Loading page number 468\n",
      "Loading page number 469\n",
      "Loading page number 470\n",
      "Loading page number 471\n",
      "Loading page number 472\n",
      "Loading page number 473\n",
      "Loading page number 474\n",
      "Loading page number 475\n",
      "Loading page number 476\n",
      "Loading page number 477\n",
      "Loading page number 478\n",
      "Loading page number 479\n",
      "Loading page number 480\n",
      "Loading page number 481\n",
      "Loading page number 482\n",
      "Loading page number 483\n",
      "Loading page number 484\n",
      "Loading page number 485\n",
      "Loading page number 486\n",
      "Loading page number 487\n",
      "Loading page number 488\n",
      "Loading page number 489\n",
      "Loading page number 490\n",
      "Loading page number 491\n",
      "Loading page number 492\n",
      "Loading page number 493\n",
      "Loading page number 494\n",
      "Loading page number 495\n",
      "Loading page number 496\n",
      "Loading page number 497\n",
      "Loading page number 498\n",
      "Loading page number 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Running AAPL Ticker! - Total of 10000 rows! & 499 pages!\n",
      "Currently Running AMZN Ticker!\n",
      "Loading page number 0\n",
      "Loading page number 1\n",
      "Loading page number 2\n",
      "Loading page number 3\n",
      "Loading page number 4\n",
      "Loading page number 5\n",
      "Loading page number 6\n",
      "Loading page number 7\n",
      "Loading page number 8\n",
      "Loading page number 9\n",
      "Loading page number 10\n",
      "Loading page number 11\n",
      "Loading page number 12\n",
      "Loading page number 13\n",
      "Loading page number 14\n",
      "Loading page number 15\n",
      "Loading page number 16\n",
      "Loading page number 17\n",
      "Loading page number 18\n",
      "Loading page number 19\n",
      "Loading page number 20\n",
      "Loading page number 21\n",
      "Loading page number 22\n",
      "Loading page number 23\n",
      "Loading page number 24\n",
      "Loading page number 25\n",
      "Loading page number 26\n",
      "Loading page number 27\n",
      "Loading page number 28\n",
      "Loading page number 29\n",
      "Loading page number 30\n",
      "Loading page number 31\n",
      "Loading page number 32\n",
      "Loading page number 33\n",
      "Loading page number 34\n",
      "Loading page number 35\n",
      "Loading page number 36\n",
      "Loading page number 37\n",
      "Loading page number 38\n",
      "Loading page number 39\n",
      "Loading page number 40\n",
      "Loading page number 41\n",
      "Loading page number 42\n",
      "Loading page number 43\n",
      "Loading page number 44\n",
      "Loading page number 45\n",
      "Loading page number 46\n",
      "Loading page number 47\n",
      "Loading page number 48\n",
      "Loading page number 49\n",
      "Loading page number 50\n",
      "Loading page number 51\n",
      "Loading page number 52\n",
      "Loading page number 53\n",
      "Loading page number 54\n",
      "Loading page number 55\n",
      "Loading page number 56\n",
      "Loading page number 57\n",
      "Loading page number 58\n",
      "Loading page number 59\n",
      "Loading page number 60\n",
      "Loading page number 61\n",
      "Loading page number 62\n",
      "Loading page number 63\n",
      "Loading page number 64\n",
      "Loading page number 65\n",
      "Loading page number 66\n",
      "Loading page number 67\n",
      "Loading page number 68\n",
      "Loading page number 69\n",
      "Loading page number 70\n",
      "Loading page number 71\n",
      "Loading page number 72\n",
      "Loading page number 73\n",
      "Loading page number 74\n",
      "Loading page number 75\n",
      "Loading page number 76\n",
      "Loading page number 77\n",
      "Loading page number 78\n",
      "Loading page number 79\n",
      "Loading page number 80\n",
      "Loading page number 81\n",
      "Loading page number 82\n",
      "Loading page number 83\n",
      "Loading page number 84\n",
      "Loading page number 85\n",
      "Loading page number 86\n",
      "Loading page number 87\n",
      "Loading page number 88\n",
      "Loading page number 89\n",
      "Loading page number 90\n",
      "Loading page number 91\n",
      "Loading page number 92\n",
      "Loading page number 93\n",
      "Loading page number 94\n",
      "Loading page number 95\n",
      "Loading page number 96\n",
      "Loading page number 97\n",
      "Loading page number 98\n",
      "Loading page number 99\n",
      "Loading page number 100\n",
      "Loading page number 101\n",
      "Loading page number 102\n",
      "Loading page number 103\n",
      "Loading page number 104\n",
      "Loading page number 105\n",
      "Loading page number 106\n",
      "Loading page number 107\n",
      "Loading page number 108\n",
      "Loading page number 109\n",
      "Loading page number 110\n",
      "Loading page number 111\n",
      "Loading page number 112\n",
      "Loading page number 113\n",
      "Loading page number 114\n",
      "Loading page number 115\n",
      "Loading page number 116\n",
      "Loading page number 117\n",
      "Loading page number 118\n",
      "Loading page number 119\n",
      "Loading page number 120\n",
      "Loading page number 121\n",
      "Loading page number 122\n",
      "Loading page number 123\n",
      "Loading page number 124\n",
      "Loading page number 125\n",
      "Loading page number 126\n",
      "Loading page number 127\n",
      "Loading page number 128\n",
      "Loading page number 129\n",
      "Loading page number 130\n",
      "Loading page number 131\n",
      "Loading page number 132\n",
      "Loading page number 133\n",
      "Loading page number 134\n",
      "Loading page number 135\n",
      "Loading page number 136\n",
      "Loading page number 137\n",
      "Loading page number 138\n",
      "Loading page number 139\n",
      "Loading page number 140\n",
      "Loading page number 141\n",
      "Loading page number 142\n",
      "Loading page number 143\n",
      "Loading page number 144\n",
      "Loading page number 145\n",
      "Loading page number 146\n",
      "Loading page number 147\n",
      "Loading page number 148\n",
      "Loading page number 149\n",
      "Loading page number 150\n",
      "Loading page number 151\n",
      "Loading page number 152\n",
      "Loading page number 153\n",
      "Loading page number 154\n",
      "Loading page number 155\n",
      "Loading page number 156\n",
      "Loading page number 157\n",
      "Loading page number 158\n",
      "Loading page number 159\n",
      "Loading page number 160\n",
      "Loading page number 161\n",
      "Loading page number 162\n",
      "Loading page number 163\n",
      "Loading page number 164\n",
      "Loading page number 165\n",
      "Loading page number 166\n",
      "Loading page number 167\n",
      "Loading page number 168\n",
      "Loading page number 169\n",
      "Loading page number 170\n",
      "Loading page number 171\n",
      "Loading page number 172\n",
      "Loading page number 173\n",
      "Loading page number 174\n",
      "Loading page number 175\n",
      "Loading page number 176\n",
      "Loading page number 177\n",
      "Loading page number 178\n",
      "Loading page number 179\n",
      "Loading page number 180\n",
      "Loading page number 181\n",
      "Loading page number 182\n",
      "Loading page number 183\n",
      "Loading page number 184\n",
      "Loading page number 185\n",
      "Loading page number 186\n",
      "Loading page number 187\n",
      "Loading page number 188\n",
      "Loading page number 189\n",
      "Loading page number 190\n",
      "Loading page number 191\n",
      "Loading page number 192\n",
      "Loading page number 193\n",
      "Loading page number 194\n",
      "Loading page number 195\n",
      "Loading page number 196\n",
      "Loading page number 197\n",
      "Loading page number 198\n",
      "Loading page number 199\n",
      "Loading page number 200\n",
      "Loading page number 201\n",
      "Loading page number 202\n",
      "Loading page number 203\n",
      "Loading page number 204\n",
      "Loading page number 205\n",
      "Loading page number 206\n",
      "Loading page number 207\n",
      "Loading page number 208\n",
      "Loading page number 209\n",
      "Loading page number 210\n",
      "Loading page number 211\n",
      "Loading page number 212\n",
      "Loading page number 213\n",
      "Loading page number 214\n",
      "Loading page number 215\n",
      "Loading page number 216\n",
      "Loading page number 217\n",
      "Loading page number 218\n",
      "Loading page number 219\n",
      "Loading page number 220\n",
      "Loading page number 221\n",
      "Loading page number 222\n",
      "Loading page number 223\n",
      "Loading page number 224\n",
      "Loading page number 225\n",
      "Loading page number 226\n",
      "Loading page number 227\n",
      "Loading page number 228\n",
      "Loading page number 229\n",
      "Loading page number 230\n",
      "Loading page number 231\n",
      "Loading page number 232\n",
      "Loading page number 233\n",
      "Loading page number 234\n",
      "Loading page number 235\n",
      "Loading page number 236\n",
      "Loading page number 237\n",
      "Loading page number 238\n",
      "Loading page number 239\n",
      "Loading page number 240\n",
      "Loading page number 241\n",
      "Loading page number 242\n",
      "Loading page number 243\n",
      "Loading page number 244\n",
      "Loading page number 245\n",
      "Loading page number 246\n",
      "Loading page number 247\n",
      "Loading page number 248\n",
      "Loading page number 249\n",
      "Loading page number 250\n",
      "Loading page number 251\n",
      "Loading page number 252\n",
      "Loading page number 253\n",
      "Loading page number 254\n",
      "Loading page number 255\n",
      "Loading page number 256\n",
      "Loading page number 257\n",
      "Loading page number 258\n",
      "Loading page number 259\n",
      "Loading page number 260\n",
      "Loading page number 261\n",
      "Loading page number 262\n",
      "Loading page number 263\n",
      "Loading page number 264\n",
      "Loading page number 265\n",
      "Loading page number 266\n",
      "Loading page number 267\n",
      "Loading page number 268\n",
      "Loading page number 269\n",
      "Loading page number 270\n",
      "Loading page number 271\n",
      "Loading page number 272\n",
      "Loading page number 273\n",
      "Loading page number 274\n",
      "Loading page number 275\n",
      "Loading page number 276\n",
      "Loading page number 277\n",
      "Loading page number 278\n",
      "Loading page number 279\n",
      "Loading page number 280\n",
      "Loading page number 281\n",
      "Loading page number 282\n",
      "Loading page number 283\n",
      "Loading page number 284\n",
      "Loading page number 285\n",
      "Loading page number 286\n",
      "Loading page number 287\n",
      "Loading page number 288\n",
      "Loading page number 289\n",
      "Loading page number 290\n",
      "Loading page number 291\n",
      "Loading page number 292\n",
      "Loading page number 293\n",
      "Loading page number 294\n",
      "Loading page number 295\n",
      "Loading page number 296\n",
      "Loading page number 297\n",
      "Loading page number 298\n",
      "Loading page number 299\n",
      "Loading page number 300\n",
      "Loading page number 301\n",
      "Loading page number 302\n",
      "Loading page number 303\n",
      "Loading page number 304\n",
      "Loading page number 305\n",
      "Loading page number 306\n",
      "Loading page number 307\n",
      "Loading page number 308\n",
      "Loading page number 309\n",
      "Loading page number 310\n",
      "Loading page number 311\n",
      "Loading page number 312\n",
      "Loading page number 313\n",
      "Loading page number 314\n",
      "Loading page number 315\n",
      "Loading page number 316\n",
      "Loading page number 317\n",
      "Loading page number 318\n",
      "Loading page number 319\n",
      "Loading page number 320\n",
      "Loading page number 321\n",
      "Loading page number 322\n",
      "Loading page number 323\n",
      "Loading page number 324\n",
      "Loading page number 325\n",
      "Loading page number 326\n",
      "Loading page number 327\n",
      "Loading page number 328\n",
      "Loading page number 329\n",
      "Loading page number 330\n",
      "Loading page number 331\n",
      "Loading page number 332\n",
      "Loading page number 333\n",
      "Loading page number 334\n",
      "Loading page number 335\n",
      "Loading page number 336\n",
      "Loading page number 337\n",
      "Loading page number 338\n",
      "Loading page number 339\n",
      "Loading page number 340\n",
      "Loading page number 341\n",
      "Loading page number 342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page number 343\n",
      "Loading page number 344\n",
      "Loading page number 345\n",
      "Loading page number 346\n",
      "Loading page number 347\n",
      "Loading page number 348\n",
      "Loading page number 349\n",
      "Loading page number 350\n",
      "Loading page number 351\n",
      "Loading page number 352\n",
      "Loading page number 353\n",
      "Loading page number 354\n",
      "Loading page number 355\n",
      "Loading page number 356\n",
      "Loading page number 357\n",
      "Loading page number 358\n",
      "Loading page number 359\n",
      "Loading page number 360\n",
      "Loading page number 361\n",
      "Loading page number 362\n",
      "Loading page number 363\n",
      "Loading page number 364\n",
      "Loading page number 365\n",
      "Loading page number 366\n",
      "Loading page number 367\n",
      "Loading page number 368\n",
      "Loading page number 369\n",
      "Loading page number 370\n",
      "Loading page number 371\n",
      "Loading page number 372\n",
      "Loading page number 373\n",
      "Loading page number 374\n",
      "Loading page number 375\n",
      "Loading page number 376\n",
      "Loading page number 377\n",
      "Loading page number 378\n",
      "Loading page number 379\n",
      "Loading page number 380\n",
      "Loading page number 381\n",
      "Loading page number 382\n",
      "Loading page number 383\n",
      "Loading page number 384\n",
      "Loading page number 385\n",
      "Loading page number 386\n",
      "Loading page number 387\n",
      "Loading page number 388\n",
      "Loading page number 389\n",
      "Loading page number 390\n",
      "Loading page number 391\n",
      "Loading page number 392\n",
      "Loading page number 393\n",
      "Loading page number 394\n",
      "Loading page number 395\n",
      "Loading page number 396\n",
      "Loading page number 397\n",
      "Loading page number 398\n",
      "Loading page number 399\n",
      "Loading page number 400\n",
      "Loading page number 401\n",
      "Loading page number 402\n",
      "Loading page number 403\n",
      "Loading page number 404\n",
      "Loading page number 405\n",
      "Loading page number 406\n",
      "Loading page number 407\n",
      "Loading page number 408\n",
      "Loading page number 409\n",
      "Loading page number 410\n",
      "Loading page number 411\n",
      "Loading page number 412\n",
      "Loading page number 413\n",
      "Loading page number 414\n",
      "Loading page number 415\n",
      "Loading page number 416\n",
      "Loading page number 417\n",
      "Loading page number 418\n",
      "Loading page number 419\n",
      "Loading page number 420\n",
      "Loading page number 421\n",
      "Loading page number 422\n",
      "Loading page number 423\n",
      "Loading page number 424\n",
      "Loading page number 425\n",
      "Loading page number 426\n",
      "Loading page number 427\n",
      "Loading page number 428\n",
      "Loading page number 429\n",
      "Loading page number 430\n",
      "Loading page number 431\n",
      "Loading page number 432\n",
      "Loading page number 433\n",
      "Loading page number 434\n",
      "Loading page number 435\n",
      "Loading page number 436\n",
      "Loading page number 437\n",
      "Loading page number 438\n",
      "Loading page number 439\n",
      "Loading page number 440\n",
      "Loading page number 441\n",
      "Loading page number 442\n",
      "Loading page number 443\n",
      "Loading page number 444\n",
      "Loading page number 445\n",
      "Loading page number 446\n",
      "Loading page number 447\n",
      "Loading page number 448\n",
      "Loading page number 449\n",
      "Loading page number 450\n",
      "Loading page number 451\n",
      "Loading page number 452\n",
      "Loading page number 453\n",
      "Loading page number 454\n",
      "Loading page number 455\n",
      "Loading page number 456\n",
      "Loading page number 457\n",
      "Loading page number 458\n",
      "Loading page number 459\n",
      "Loading page number 460\n",
      "Loading page number 461\n",
      "Loading page number 462\n",
      "Loading page number 463\n",
      "Loading page number 464\n",
      "Loading page number 465\n",
      "Loading page number 466\n",
      "Loading page number 467\n",
      "Loading page number 468\n",
      "Loading page number 469\n",
      "Loading page number 470\n",
      "Loading page number 471\n",
      "Loading page number 472\n",
      "Loading page number 473\n",
      "Loading page number 474\n",
      "Loading page number 475\n",
      "Loading page number 476\n",
      "Loading page number 477\n",
      "Loading page number 478\n",
      "Loading page number 479\n",
      "Loading page number 480\n",
      "Loading page number 481\n",
      "Loading page number 482\n",
      "Loading page number 483\n",
      "Loading page number 484\n",
      "Loading page number 485\n",
      "Loading page number 486\n",
      "Loading page number 487\n",
      "Loading page number 488\n",
      "Loading page number 489\n",
      "Loading page number 490\n",
      "Loading page number 491\n",
      "Loading page number 492\n",
      "Loading page number 493\n",
      "Loading page number 494\n",
      "Loading page number 495\n",
      "Loading page number 496\n",
      "Loading page number 497\n",
      "Loading page number 498\n",
      "Loading page number 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Running AMZN Ticker! - Total of 9999 rows! & 499 pages!\n",
      "Currently Running TSLA Ticker!\n",
      "Loading page number 0\n",
      "Loading page number 1\n",
      "Loading page number 2\n",
      "Loading page number 3\n",
      "Loading page number 4\n",
      "Loading page number 5\n",
      "Loading page number 6\n",
      "Loading page number 7\n",
      "Loading page number 8\n",
      "Loading page number 9\n",
      "Loading page number 10\n",
      "Loading page number 11\n",
      "Loading page number 12\n",
      "Loading page number 13\n",
      "Loading page number 14\n",
      "Loading page number 15\n",
      "Loading page number 16\n",
      "Loading page number 17\n",
      "Loading page number 18\n",
      "Loading page number 19\n",
      "Loading page number 20\n",
      "Loading page number 21\n",
      "Loading page number 22\n",
      "Loading page number 23\n",
      "Loading page number 24\n",
      "Loading page number 25\n",
      "Loading page number 26\n",
      "Loading page number 27\n",
      "Loading page number 28\n",
      "Loading page number 29\n",
      "Loading page number 30\n",
      "Loading page number 31\n",
      "Loading page number 32\n",
      "Loading page number 33\n",
      "Loading page number 34\n",
      "Loading page number 35\n",
      "Loading page number 36\n",
      "Loading page number 37\n",
      "Loading page number 38\n",
      "Loading page number 39\n",
      "Loading page number 40\n",
      "Loading page number 41\n",
      "Loading page number 42\n",
      "Loading page number 43\n",
      "Loading page number 44\n",
      "Loading page number 45\n",
      "Loading page number 46\n",
      "Loading page number 47\n",
      "Loading page number 48\n",
      "Loading page number 49\n",
      "Loading page number 50\n",
      "Loading page number 51\n",
      "Loading page number 52\n",
      "Loading page number 53\n",
      "Loading page number 54\n",
      "Loading page number 55\n",
      "Loading page number 56\n",
      "Loading page number 57\n",
      "Loading page number 58\n",
      "Loading page number 59\n",
      "Loading page number 60\n",
      "Loading page number 61\n",
      "Loading page number 62\n",
      "Loading page number 63\n",
      "Loading page number 64\n",
      "Loading page number 65\n",
      "Loading page number 66\n",
      "Loading page number 67\n",
      "Loading page number 68\n",
      "Loading page number 69\n",
      "Loading page number 70\n",
      "Loading page number 71\n",
      "Loading page number 72\n",
      "Loading page number 73\n",
      "Loading page number 74\n",
      "Loading page number 75\n",
      "Loading page number 76\n",
      "Loading page number 77\n",
      "Loading page number 78\n",
      "Loading page number 79\n",
      "Loading page number 80\n",
      "Loading page number 81\n",
      "Loading page number 82\n",
      "Loading page number 83\n",
      "Loading page number 84\n",
      "Loading page number 85\n",
      "Loading page number 86\n",
      "Loading page number 87\n",
      "Loading page number 88\n",
      "Loading page number 89\n",
      "Loading page number 90\n",
      "Loading page number 91\n",
      "Loading page number 92\n",
      "Loading page number 93\n",
      "Loading page number 94\n",
      "Loading page number 95\n",
      "Loading page number 96\n",
      "Loading page number 97\n",
      "Loading page number 98\n",
      "Loading page number 99\n",
      "Loading page number 100\n",
      "Loading page number 101\n",
      "Loading page number 102\n",
      "Loading page number 103\n",
      "Loading page number 104\n",
      "Loading page number 105\n",
      "Loading page number 106\n",
      "Loading page number 107\n",
      "Loading page number 108\n",
      "Loading page number 109\n",
      "Loading page number 110\n",
      "Loading page number 111\n",
      "Loading page number 112\n",
      "Loading page number 113\n",
      "Loading page number 114\n",
      "Loading page number 115\n",
      "Loading page number 116\n",
      "Loading page number 117\n",
      "Loading page number 118\n",
      "Loading page number 119\n",
      "Loading page number 120\n",
      "Loading page number 121\n",
      "Loading page number 122\n",
      "Loading page number 123\n",
      "Loading page number 124\n",
      "Loading page number 125\n",
      "Loading page number 126\n",
      "Loading page number 127\n",
      "Loading page number 128\n",
      "Loading page number 129\n",
      "Loading page number 130\n",
      "Loading page number 131\n",
      "Loading page number 132\n",
      "Loading page number 133\n",
      "Loading page number 134\n",
      "Loading page number 135\n",
      "Loading page number 136\n",
      "Loading page number 137\n",
      "Loading page number 138\n",
      "Loading page number 139\n",
      "Loading page number 140\n",
      "Loading page number 141\n",
      "Loading page number 142\n",
      "Loading page number 143\n",
      "Loading page number 144\n",
      "Loading page number 145\n",
      "Loading page number 146\n",
      "Loading page number 147\n",
      "Loading page number 148\n",
      "Loading page number 149\n",
      "Loading page number 150\n",
      "Loading page number 151\n",
      "Loading page number 152\n",
      "Loading page number 153\n",
      "Loading page number 154\n",
      "Loading page number 155\n",
      "Loading page number 156\n",
      "Loading page number 157\n",
      "Loading page number 158\n",
      "Loading page number 159\n",
      "Loading page number 160\n",
      "Loading page number 161\n",
      "Loading page number 162\n",
      "Loading page number 163\n",
      "Loading page number 164\n",
      "Loading page number 165\n",
      "Loading page number 166\n",
      "Loading page number 167\n",
      "Loading page number 168\n",
      "Loading page number 169\n",
      "Loading page number 170\n",
      "Loading page number 171\n",
      "Loading page number 172\n",
      "Loading page number 173\n",
      "Loading page number 174\n",
      "Loading page number 175\n",
      "Loading page number 176\n",
      "Loading page number 177\n",
      "Loading page number 178\n",
      "Loading page number 179\n",
      "Loading page number 180\n",
      "Loading page number 181\n",
      "Loading page number 182\n",
      "Loading page number 183\n",
      "Loading page number 184\n",
      "Loading page number 185\n",
      "Loading page number 186\n",
      "Loading page number 187\n",
      "Loading page number 188\n",
      "Loading page number 189\n",
      "Loading page number 190\n",
      "Loading page number 191\n",
      "Loading page number 192\n",
      "Loading page number 193\n",
      "Loading page number 194\n",
      "Loading page number 195\n",
      "Loading page number 196\n",
      "Loading page number 197\n",
      "Loading page number 198\n",
      "Loading page number 199\n",
      "Loading page number 200\n",
      "Loading page number 201\n",
      "Loading page number 202\n",
      "Loading page number 203\n",
      "Loading page number 204\n",
      "Loading page number 205\n",
      "Loading page number 206\n",
      "Loading page number 207\n",
      "Loading page number 208\n",
      "Loading page number 209\n",
      "Loading page number 210\n",
      "Loading page number 211\n",
      "Loading page number 212\n",
      "Loading page number 213\n",
      "Loading page number 214\n",
      "Loading page number 215\n",
      "Loading page number 216\n",
      "Loading page number 217\n",
      "Loading page number 218\n",
      "Loading page number 219\n",
      "Loading page number 220\n",
      "Loading page number 221\n",
      "Loading page number 222\n",
      "Loading page number 223\n",
      "Loading page number 224\n",
      "Loading page number 225\n",
      "Loading page number 226\n",
      "Loading page number 227\n",
      "Loading page number 228\n",
      "Loading page number 229\n",
      "Loading page number 230\n",
      "Loading page number 231\n",
      "Loading page number 232\n",
      "Loading page number 233\n",
      "Loading page number 234\n",
      "Loading page number 235\n",
      "Loading page number 236\n",
      "Loading page number 237\n",
      "Loading page number 238\n",
      "Loading page number 239\n",
      "Loading page number 240\n",
      "Loading page number 241\n",
      "Loading page number 242\n",
      "Loading page number 243\n",
      "Loading page number 244\n",
      "Loading page number 245\n",
      "Loading page number 246\n",
      "Loading page number 247\n",
      "Loading page number 248\n",
      "Loading page number 249\n",
      "Loading page number 250\n",
      "Loading page number 251\n",
      "Loading page number 252\n",
      "Loading page number 253\n",
      "Loading page number 254\n",
      "Loading page number 255\n",
      "Loading page number 256\n",
      "Loading page number 257\n",
      "Loading page number 258\n",
      "Loading page number 259\n",
      "Loading page number 260\n",
      "Loading page number 261\n",
      "Loading page number 262\n",
      "Loading page number 263\n",
      "Loading page number 264\n",
      "Loading page number 265\n",
      "Loading page number 266\n",
      "Loading page number 267\n",
      "Loading page number 268\n",
      "Loading page number 269\n",
      "Loading page number 270\n",
      "Loading page number 271\n",
      "Loading page number 272\n",
      "Loading page number 273\n",
      "Loading page number 274\n",
      "Loading page number 275\n",
      "Loading page number 276\n",
      "Loading page number 277\n",
      "Loading page number 278\n",
      "Loading page number 279\n",
      "Loading page number 280\n",
      "Loading page number 281\n",
      "Loading page number 282\n",
      "Loading page number 283\n",
      "Loading page number 284\n",
      "Loading page number 285\n",
      "Loading page number 286\n",
      "Loading page number 287\n",
      "Loading page number 288\n",
      "Loading page number 289\n",
      "Loading page number 290\n",
      "Loading page number 291\n",
      "Loading page number 292\n",
      "Loading page number 293\n",
      "Loading page number 294\n",
      "Loading page number 295\n",
      "Loading page number 296\n",
      "Loading page number 297\n",
      "Loading page number 298\n",
      "Loading page number 299\n",
      "Loading page number 300\n",
      "Loading page number 301\n",
      "Loading page number 302\n",
      "Loading page number 303\n",
      "Loading page number 304\n",
      "Loading page number 305\n",
      "Loading page number 306\n",
      "Loading page number 307\n",
      "Loading page number 308\n",
      "Loading page number 309\n",
      "Loading page number 310\n",
      "Loading page number 311\n",
      "Loading page number 312\n",
      "Loading page number 313\n",
      "Loading page number 314\n",
      "Loading page number 315\n",
      "Loading page number 316\n",
      "Loading page number 317\n",
      "Loading page number 318\n",
      "Loading page number 319\n",
      "Loading page number 320\n",
      "Loading page number 321\n",
      "Loading page number 322\n",
      "Loading page number 323\n",
      "Loading page number 324\n",
      "Loading page number 325\n",
      "Loading page number 326\n",
      "Loading page number 327\n",
      "Loading page number 328\n",
      "Loading page number 329\n",
      "Loading page number 330\n",
      "Loading page number 331\n",
      "Loading page number 332\n",
      "Loading page number 333\n",
      "Loading page number 334\n",
      "Loading page number 335\n",
      "Loading page number 336\n",
      "Loading page number 337\n",
      "Loading page number 338\n",
      "Loading page number 339\n",
      "Loading page number 340\n",
      "Loading page number 341\n",
      "Loading page number 342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page number 343\n",
      "Loading page number 344\n",
      "Loading page number 345\n",
      "Loading page number 346\n",
      "Loading page number 347\n",
      "Loading page number 348\n",
      "Loading page number 349\n",
      "Loading page number 350\n",
      "Loading page number 351\n",
      "Loading page number 352\n",
      "OOPS, sorry no more pages! There's only 352 pages for this Ticker!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Running TSLA Ticker! - Total of 7023 rows! & 352 pages!\n",
      "Currently Running SPY Ticker!\n",
      "Loading page number 0\n",
      "OOPS, sorry no more pages! There's only 0 pages for this Ticker!\n",
      "Done Running SPY Ticker! - Total of 0 rows! & 0 pages!\n"
     ]
    }
   ],
   "source": [
    "aapl_headlines_df = getHeadlines(\"AAPL\")\n",
    "amzn_headlines_df = getHeadlines(\"AMZN\")\n",
    "tsla_headlines_df = getHeadlines(\"TSLA\")\n",
    "spy_headlines_df = getHeadlines(\"SPX\")\n",
    "\n",
    "docu_headlines_df = getHeadlines(\"DOCU\")\n",
    "nflx_headlines_df = getHeadlines(\"NFLX\")\n",
    "nke_headlines_df = getHeadlines(\"NKE\")\n",
    "pg_headlines_df = getHeadlines(\"PG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_headlines_df = spy_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "aapl_headlines_df = aapl_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "amzn_headlines_df = amzn_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "tsla_headlines_df = tsla_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "docu_headlines_df = docu_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "nflx_headlines_df = nflx_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "nke_headlines_df = nke_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "pg_headlines_df = pg_headlines_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_info_grab(ticker,ticker_headline_df):\n",
    "    \"\"\"\n",
    "    Takes ticker symbol and returns DataFrame with Date, Close, and Pct Change columns.\n",
    "    \"\"\"\n",
    "    # Set timeframe to '1D'\n",
    "    timeframe = \"1D\"\n",
    "\n",
    "    # Set current date and the date from one month ago using the ISO format\n",
    "    past_date = pd.Timestamp(ticker_headline_df['Date'][0], tz=\"America/New_York\").isoformat()\n",
    "    current_date = pd.Timestamp(ticker_headline_df['Date'][len(ticker_headline_df)-1], tz=\"America/New_York\").isoformat()\n",
    "\n",
    "    df = api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit=None,\n",
    "        start=past_date,\n",
    "        end=current_date,\n",
    "        after=None,\n",
    "        until=None,\n",
    "    ).df\n",
    "    df = df.droplevel(axis=1, level=0)\n",
    "    df.index = df.index.date\n",
    "    df['pct change'] = df['close'].pct_change()\n",
    "    df['predicted pct change'] = df['pct change'].shift(periods=-1)\n",
    "    del df['pct change']\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['open', 'high', 'low', 'volume'])\n",
    "    df = df.rename(columns={'index':'Date'})\n",
    "    df = df.set_index('Date')\n",
    "    df = df.sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl_stock_info = stock_info_grab(\"AAPL\",aapl_headlines_df)\n",
    "amzn_stock_info = stock_info_grab(\"AMZN\",amzn_headlines_df)\n",
    "tsla_stock_info = stock_info_grab(\"TSLA\",tsla_headlines_df)\n",
    "spy_stock_info = stock_info_grab(\"SPX\",spy_headlines_df)\n",
    "\n",
    "docu_stock_info = stock_info_grab(\"DOCU\",docu_headlines_df)\n",
    "nflx_stock_info = stock_info_grab(\"NFLX\",nflx_headlines_df)\n",
    "nke_stock_info = stock_info_grab(\"NKE\",nke_headlines_df)\n",
    "pg_stock_info = stock_info_grab(\"PG\",pg_headlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_df(df):\n",
    "    \"\"\"\n",
    "    Takes headlines DataFrame & creates DataFrame with Sentiment columns.\n",
    "    Splits Date & Time, creates Time column and moves Date to Index.\n",
    "    \"\"\"\n",
    "    title_sent = {\n",
    "        \"compound\": [],\n",
    "        \"positive\": [],\n",
    "        \"neutral\": [],\n",
    "        \"negative\": [],\n",
    "        \"sentiment\": [],\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Sentiment scoring with VADER\n",
    "            title_sentiment = analyzer.polarity_scores(row[\"Headline\"])\n",
    "            title_sent[\"compound\"].append(title_sentiment[\"compound\"])\n",
    "            title_sent[\"positive\"].append(title_sentiment[\"pos\"])\n",
    "            title_sent[\"neutral\"].append(title_sentiment[\"neu\"])\n",
    "            title_sent[\"negative\"].append(title_sentiment[\"neg\"])\n",
    "            title_sent[\"sentiment\"].append(get_sentiment(title_sentiment[\"compound\"]))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    title_sent_df = pd.DataFrame(title_sent)\n",
    "    #title_sent_df.head()\n",
    "\n",
    "    headline_sentiment_df = df.join(title_sent_df)\n",
    "    headline_sentiment_df.dropna()\n",
    "    headline_sentiment_df['Date'] = headline_sentiment_df['Date']\n",
    "    headline_sentiment_df['Date'] = headline_sentiment_df['Date']\n",
    "    headline_sentiment_df = headline_sentiment_df.reindex(columns=['Date', 'Headline', 'compound', 'positive', 'neutral', 'negative', 'sentiment'])\n",
    "    headline_sentiment_df['Date'] = pd.to_datetime(headline_sentiment_df['Date'])\n",
    "    headline_sentiment_df = headline_sentiment_df.set_index('Date').sort_index(ascending=False)\n",
    "    \n",
    "    # find average sentiment score by date\n",
    "    headline_scores = headline_sentiment_df.groupby('Date').mean().sort_index(ascending=False)\n",
    "    del headline_scores['sentiment']\n",
    "    \n",
    "    return headline_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_headlines = create_sentiment_df(aapl_headlines_df)\n",
    "amzn_headlines = create_sentiment_df(amzn_headlines_df)\n",
    "tsla_headlines = create_sentiment_df(tsla_headlines_df)\n",
    "spy_headlines = create_sentiment_df(spy_headlines_df)\n",
    "\n",
    "docu_scores = create_sentiment_df(docu_headlines_df)\n",
    "nflx_scores = create_sentiment_df(nflx_headlines_df)\n",
    "nke_scores = create_sentiment_df(nke_headlines_df)\n",
    "pg_scores = create_sentiment_df(pg_headlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent scores distribution across each df poss use histogram, calc meanstd, or percentiles\n",
    "aapl_complete = pd.concat([aapl_scores,aapl_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "amzn_complete = pd.concat([amzn_scores,amzn_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "tsla_complete = pd.concat([tsla_scores,tsla_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "spy_complete = pd.concat([spy_scores,spy_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "\n",
    "docu_complete = pd.concat([docu_scores,docu_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "nflx_complete = pd.concat([nflx_scores,nflx_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "nke_complete = pd.concat([nke_scores,nke_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "pg_complete = pd.concat([pg_scores,pg_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>close</th>\n",
       "      <th>predicted pct change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-16</th>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>120.350</td>\n",
       "      <td>0.000208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-13</th>\n",
       "      <td>-0.043083</td>\n",
       "      <td>0.032667</td>\n",
       "      <td>0.918167</td>\n",
       "      <td>0.049167</td>\n",
       "      <td>119.490</td>\n",
       "      <td>0.007197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-12</th>\n",
       "      <td>0.324450</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>119.235</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-11</th>\n",
       "      <td>0.192778</td>\n",
       "      <td>0.158111</td>\n",
       "      <td>0.782778</td>\n",
       "      <td>0.059111</td>\n",
       "      <td>119.510</td>\n",
       "      <td>-0.002301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-10</th>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.111714</td>\n",
       "      <td>0.888286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>115.970</td>\n",
       "      <td>0.030525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            compound  positive   neutral  negative    close  \\\n",
       "Date                                                          \n",
       "2020-11-16  0.109900  0.062000  0.912200  0.025800  120.350   \n",
       "2020-11-13 -0.043083  0.032667  0.918167  0.049167  119.490   \n",
       "2020-11-12  0.324450  0.228000  0.772000  0.000000  119.235   \n",
       "2020-11-11  0.192778  0.158111  0.782778  0.059111  119.510   \n",
       "2020-11-10  0.187700  0.111714  0.888286  0.000000  115.970   \n",
       "\n",
       "            predicted pct change  \n",
       "Date                              \n",
       "2020-11-16              0.000208  \n",
       "2020-11-13              0.007197  \n",
       "2020-11-12              0.002139  \n",
       "2020-11-11             -0.002301  \n",
       "2020-11-10              0.030525  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_prep(df):\n",
    "    df['SCORE'] = ''\n",
    "    temp = [];\n",
    "    for i in range(round(len(df['compound']))):\n",
    "        start = i\n",
    "        end=(i+1)\n",
    "        temp.append(df['compound'][start:end])\n",
    "\n",
    "    df['ACTION'] = ''\n",
    "    temp2 = [];\n",
    "    for i in range(round(len(df['predicted pct change']))):\n",
    "        start = i\n",
    "        end=(i+1)\n",
    "        temp2.append(df['predicted pct change'][start:end])\n",
    "\n",
    "    for x in range(0,len(temp)):\n",
    "        for integer,row in temp[x].iteritems():\n",
    "            if row >= 0.05:\n",
    "                df.set_value(integer,'SCORE',1)\n",
    "\n",
    "            elif (row <= -0.05):\n",
    "                df.set_value(integer,'SCORE',-1)\n",
    "            else:\n",
    "                df.set_value(integer,'SCORE', 0)\n",
    "\n",
    "    for y in range(0,len(temp2)):\n",
    "        for integer,row in temp2[y].iteritems():\n",
    "            if row >0:\n",
    "                df.set_value(integer,'ACTION','ADD MONEY')\n",
    "\n",
    "            elif (row==0):\n",
    "                df.set_value(integer,'ACTION','HOLD')\n",
    "            else:\n",
    "                df.set_value(integer,'ACTION', 'TAKE MONEY OUT')\n",
    "                \n",
    "    df = df[df['ACTION'] != 'HOLD'].dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "aapl_complete = ml_prep(aapl_complete)\n",
    "tsla_complete = ml_prep(tsla_complete)\n",
    "spy_complete = ml_prep(spy_complete)\n",
    "amzn_complete = amzn_complete(pg_complete)\n",
    "docu_complete = ml_prep(docu_complete)\n",
    "nflx_complete = ml_prep(nflx_complete)\n",
    "nke_complete = ml_prep(nke_complete)\n",
    "pg_complete = ml_prep(pg_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spy_complete = spy_complete[spy_complete['ACTION'] != 'HOLD'].dropna()\n",
    "tsla_complete = tsla_complete[tsla_complete['ACTION'] != 'HOLD'].dropna()\n",
    "aapl_complete = aapl_complete[aapl_complete['ACTION'] != 'HOLD'].dropna()\n",
    "amzn_complete = amzn_complete[amzn_complete['ACTION'] != 'HOLD'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_learning(tickerName, df):\n",
    "    X = df.drop(\"ACTION\", axis=1)\n",
    "    y = df[\"ACTION\"]\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=1, stratify=y)\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    # Step 1: Label-encode data set\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(y_train)\n",
    "    encoded_y_train = label_encoder.transform(y_train)\n",
    "    encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "    # Step 2: Convert encoded labels to one-hot-encoding\n",
    "    y_train_categorical = to_categorical(encoded_y_train)\n",
    "    y_test_categorical = to_categorical(encoded_y_test)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    # Create model and add layers\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=100, activation='linear', input_dim=7))\n",
    "    model.add(Dense(units=100, activation='linear'))\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "    # Compile and fit the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_categorical,\n",
    "        epochs=60,\n",
    "        shuffle=True,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    print(\n",
    "        f\"{tickerName}: Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    encoded_predictions = model.predict_classes(X_test_scaled[:5])\n",
    "    prediction_labels = label_encoder.inverse_transform(encoded_predictions)\n",
    "    print(\"     \")\n",
    "    print(\"######### -----  PREDICTION TIME!!!!! ------ ######### \")\n",
    "    print(\"     \")\n",
    "    \n",
    "    print(f\"Predicted classes: {prediction_labels}\")\n",
    "    print(f\"Actual Labels: {list(y_test[:5])}\")\n",
    "    print(\"     \")\n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    title_sent = {\n",
    "    \"TICKER\": tickerName,\n",
    "    \"Neural Net - ACCURACY\": model_accuracy\n",
    "    }\n",
    "    df = pd.DataFrame(title_sent,index=[0])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1211, 7) (1211,)\n",
      "Epoch 1/60\n",
      "29/29 - 0s - loss: 0.4534 - accuracy: 0.8293\n",
      "Epoch 2/60\n",
      "29/29 - 0s - loss: 0.2529 - accuracy: 0.9482\n",
      "Epoch 3/60\n",
      "29/29 - 0s - loss: 0.1790 - accuracy: 0.9736\n",
      "Epoch 4/60\n",
      "29/29 - 0s - loss: 0.1386 - accuracy: 0.9769\n",
      "Epoch 5/60\n",
      "29/29 - 0s - loss: 0.1149 - accuracy: 0.9747\n",
      "Epoch 6/60\n",
      "29/29 - 0s - loss: 0.0928 - accuracy: 0.9791\n",
      "Epoch 7/60\n",
      "29/29 - 0s - loss: 0.0919 - accuracy: 0.9736\n",
      "Epoch 8/60\n",
      "29/29 - 0s - loss: 0.0762 - accuracy: 0.9780\n",
      "Epoch 9/60\n",
      "29/29 - 0s - loss: 0.0653 - accuracy: 0.9879\n",
      "Epoch 10/60\n",
      "29/29 - 0s - loss: 0.0788 - accuracy: 0.9736\n",
      "Epoch 11/60\n",
      "29/29 - 0s - loss: 0.0599 - accuracy: 0.9802\n",
      "Epoch 12/60\n",
      "29/29 - 0s - loss: 0.0533 - accuracy: 0.9846\n",
      "Epoch 13/60\n",
      "29/29 - 0s - loss: 0.0561 - accuracy: 0.9769\n",
      "Epoch 14/60\n",
      "29/29 - 0s - loss: 0.0535 - accuracy: 0.9802\n",
      "Epoch 15/60\n",
      "29/29 - 0s - loss: 0.0474 - accuracy: 0.9846\n",
      "Epoch 16/60\n",
      "29/29 - 0s - loss: 0.0433 - accuracy: 0.9868\n",
      "Epoch 17/60\n",
      "29/29 - 0s - loss: 0.0413 - accuracy: 0.9846\n",
      "Epoch 18/60\n",
      "29/29 - 0s - loss: 0.0533 - accuracy: 0.9780\n",
      "Epoch 19/60\n",
      "29/29 - 0s - loss: 0.0381 - accuracy: 0.9868\n",
      "Epoch 20/60\n",
      "29/29 - 0s - loss: 0.0364 - accuracy: 0.9879\n",
      "Epoch 21/60\n",
      "29/29 - 0s - loss: 0.0328 - accuracy: 0.9901\n",
      "Epoch 22/60\n",
      "29/29 - 0s - loss: 0.0324 - accuracy: 0.9901\n",
      "Epoch 23/60\n",
      "29/29 - 0s - loss: 0.0311 - accuracy: 0.9890\n",
      "Epoch 24/60\n",
      "29/29 - 0s - loss: 0.0368 - accuracy: 0.9879\n",
      "Epoch 25/60\n",
      "29/29 - 0s - loss: 0.0357 - accuracy: 0.9857\n",
      "Epoch 26/60\n",
      "29/29 - 0s - loss: 0.0307 - accuracy: 0.9879\n",
      "Epoch 27/60\n",
      "29/29 - 0s - loss: 0.0263 - accuracy: 0.9912\n",
      "Epoch 28/60\n",
      "29/29 - 0s - loss: 0.0250 - accuracy: 0.9945\n",
      "Epoch 29/60\n",
      "29/29 - 0s - loss: 0.0327 - accuracy: 0.9868\n",
      "Epoch 30/60\n",
      "29/29 - 0s - loss: 0.0276 - accuracy: 0.9879\n",
      "Epoch 31/60\n",
      "29/29 - 0s - loss: 0.0410 - accuracy: 0.9824\n",
      "Epoch 32/60\n",
      "29/29 - 0s - loss: 0.0334 - accuracy: 0.9868\n",
      "Epoch 33/60\n",
      "29/29 - 0s - loss: 0.0263 - accuracy: 0.9923\n",
      "Epoch 34/60\n",
      "29/29 - 0s - loss: 0.0218 - accuracy: 0.9956\n",
      "Epoch 35/60\n",
      "29/29 - 0s - loss: 0.0227 - accuracy: 0.9923\n",
      "Epoch 36/60\n",
      "29/29 - 0s - loss: 0.0196 - accuracy: 0.9956\n",
      "Epoch 37/60\n",
      "29/29 - 0s - loss: 0.0287 - accuracy: 0.9868\n",
      "Epoch 38/60\n",
      "29/29 - 0s - loss: 0.0261 - accuracy: 0.9879\n",
      "Epoch 39/60\n",
      "29/29 - 0s - loss: 0.0227 - accuracy: 0.9890\n",
      "Epoch 40/60\n",
      "29/29 - 0s - loss: 0.0255 - accuracy: 0.9901\n",
      "Epoch 41/60\n",
      "29/29 - 0s - loss: 0.0245 - accuracy: 0.9901\n",
      "Epoch 42/60\n",
      "29/29 - 0s - loss: 0.0237 - accuracy: 0.9912\n",
      "Epoch 43/60\n",
      "29/29 - 0s - loss: 0.0241 - accuracy: 0.9901\n",
      "Epoch 44/60\n",
      "29/29 - 0s - loss: 0.0179 - accuracy: 0.9956\n",
      "Epoch 45/60\n",
      "29/29 - 0s - loss: 0.0165 - accuracy: 0.9956\n",
      "Epoch 46/60\n",
      "29/29 - 0s - loss: 0.0174 - accuracy: 0.9934\n",
      "Epoch 47/60\n",
      "29/29 - 0s - loss: 0.0178 - accuracy: 0.9956\n",
      "Epoch 48/60\n",
      "29/29 - 0s - loss: 0.0212 - accuracy: 0.9912\n",
      "Epoch 49/60\n",
      "29/29 - 0s - loss: 0.0152 - accuracy: 0.9934\n",
      "Epoch 50/60\n",
      "29/29 - 0s - loss: 0.0183 - accuracy: 0.9923\n",
      "Epoch 51/60\n",
      "29/29 - 0s - loss: 0.0182 - accuracy: 0.9945\n",
      "Epoch 52/60\n",
      "29/29 - 0s - loss: 0.0253 - accuracy: 0.9901\n",
      "Epoch 53/60\n",
      "29/29 - 0s - loss: 0.0194 - accuracy: 0.9934\n",
      "Epoch 54/60\n",
      "29/29 - 0s - loss: 0.0140 - accuracy: 0.9978\n",
      "Epoch 55/60\n",
      "29/29 - 0s - loss: 0.0163 - accuracy: 0.9934\n",
      "Epoch 56/60\n",
      "29/29 - 0s - loss: 0.0177 - accuracy: 0.9923\n",
      "Epoch 57/60\n",
      "29/29 - 0s - loss: 0.0129 - accuracy: 0.9978\n",
      "Epoch 58/60\n",
      "29/29 - 0s - loss: 0.0180 - accuracy: 0.9934\n",
      "Epoch 59/60\n",
      "29/29 - 0s - loss: 0.0151 - accuracy: 0.9945\n",
      "Epoch 60/60\n",
      "29/29 - 0s - loss: 0.0252 - accuracy: 0.9890\n",
      "10/10 - 0s - loss: 0.0155 - accuracy: 0.9934\n",
      "----------------------------------------------------------------------------------------\n",
      "AAPL: Normal Neural Network - Loss: 0.01545373722910881, Accuracy: 0.9933993220329285\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C078CDA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(1612, 7) (1612,)\n",
      "Epoch 1/60\n",
      "38/38 - 0s - loss: 0.3906 - accuracy: 0.8569\n",
      "Epoch 2/60\n",
      "38/38 - 0s - loss: 0.1748 - accuracy: 0.9677\n",
      "Epoch 3/60\n",
      "38/38 - 0s - loss: 0.1208 - accuracy: 0.9785\n",
      "Epoch 4/60\n",
      "38/38 - 0s - loss: 0.0966 - accuracy: 0.9801\n",
      "Epoch 5/60\n",
      "38/38 - 0s - loss: 0.0831 - accuracy: 0.9810\n",
      "Epoch 6/60\n",
      "38/38 - 0s - loss: 0.0681 - accuracy: 0.9835\n",
      "Epoch 7/60\n",
      "38/38 - 0s - loss: 0.0635 - accuracy: 0.9826\n",
      "Epoch 8/60\n",
      "38/38 - 0s - loss: 0.0503 - accuracy: 0.9950\n",
      "Epoch 9/60\n",
      "38/38 - 0s - loss: 0.0539 - accuracy: 0.9851\n",
      "Epoch 10/60\n",
      "38/38 - 0s - loss: 0.0479 - accuracy: 0.9843\n",
      "Epoch 11/60\n",
      "38/38 - 0s - loss: 0.0453 - accuracy: 0.9826\n",
      "Epoch 12/60\n",
      "38/38 - 0s - loss: 0.0399 - accuracy: 0.9909\n",
      "Epoch 13/60\n",
      "38/38 - 0s - loss: 0.0352 - accuracy: 0.9909\n",
      "Epoch 14/60\n",
      "38/38 - 0s - loss: 0.0371 - accuracy: 0.9876\n",
      "Epoch 15/60\n",
      "38/38 - 0s - loss: 0.0326 - accuracy: 0.9876\n",
      "Epoch 16/60\n",
      "38/38 - 0s - loss: 0.0302 - accuracy: 0.9892\n",
      "Epoch 17/60\n",
      "38/38 - 0s - loss: 0.0460 - accuracy: 0.9826\n",
      "Epoch 18/60\n",
      "38/38 - 0s - loss: 0.0309 - accuracy: 0.9909\n",
      "Epoch 19/60\n",
      "38/38 - 0s - loss: 0.0245 - accuracy: 0.9950\n",
      "Epoch 20/60\n",
      "38/38 - 0s - loss: 0.0233 - accuracy: 0.9942\n",
      "Epoch 21/60\n",
      "38/38 - 0s - loss: 0.0318 - accuracy: 0.9859\n",
      "Epoch 22/60\n",
      "38/38 - 0s - loss: 0.0232 - accuracy: 0.9942\n",
      "Epoch 23/60\n",
      "38/38 - 0s - loss: 0.0305 - accuracy: 0.9901\n",
      "Epoch 24/60\n",
      "38/38 - 0s - loss: 0.0423 - accuracy: 0.9801\n",
      "Epoch 25/60\n",
      "38/38 - 0s - loss: 0.0489 - accuracy: 0.9835\n",
      "Epoch 26/60\n",
      "38/38 - 0s - loss: 0.0232 - accuracy: 0.9926\n",
      "Epoch 27/60\n",
      "38/38 - 0s - loss: 0.0238 - accuracy: 0.9868\n",
      "Epoch 28/60\n",
      "38/38 - 0s - loss: 0.0187 - accuracy: 0.9942\n",
      "Epoch 29/60\n",
      "38/38 - 0s - loss: 0.0260 - accuracy: 0.9884\n",
      "Epoch 30/60\n",
      "38/38 - 0s - loss: 0.0205 - accuracy: 0.9917\n",
      "Epoch 31/60\n",
      "38/38 - 0s - loss: 0.0176 - accuracy: 0.9959\n",
      "Epoch 32/60\n",
      "38/38 - 0s - loss: 0.0200 - accuracy: 0.9942\n",
      "Epoch 33/60\n",
      "38/38 - 0s - loss: 0.0236 - accuracy: 0.9917\n",
      "Epoch 34/60\n",
      "38/38 - 0s - loss: 0.0229 - accuracy: 0.9901\n",
      "Epoch 35/60\n",
      "38/38 - 0s - loss: 0.0290 - accuracy: 0.9868\n",
      "Epoch 36/60\n",
      "38/38 - 0s - loss: 0.0206 - accuracy: 0.9926\n",
      "Epoch 37/60\n",
      "38/38 - 0s - loss: 0.0151 - accuracy: 0.9942\n",
      "Epoch 38/60\n",
      "38/38 - 0s - loss: 0.0130 - accuracy: 0.9967\n",
      "Epoch 39/60\n",
      "38/38 - 0s - loss: 0.0160 - accuracy: 0.9926\n",
      "Epoch 40/60\n",
      "38/38 - 0s - loss: 0.0321 - accuracy: 0.9868\n",
      "Epoch 41/60\n",
      "38/38 - 0s - loss: 0.0193 - accuracy: 0.9917\n",
      "Epoch 42/60\n",
      "38/38 - 0s - loss: 0.0137 - accuracy: 0.9942\n",
      "Epoch 43/60\n",
      "38/38 - 0s - loss: 0.0144 - accuracy: 0.9942\n",
      "Epoch 44/60\n",
      "38/38 - 0s - loss: 0.0196 - accuracy: 0.9892\n",
      "Epoch 45/60\n",
      "38/38 - 0s - loss: 0.0160 - accuracy: 0.9934\n",
      "Epoch 46/60\n",
      "38/38 - 0s - loss: 0.0202 - accuracy: 0.9909\n",
      "Epoch 47/60\n",
      "38/38 - 0s - loss: 0.0146 - accuracy: 0.9959\n",
      "Epoch 48/60\n",
      "38/38 - 0s - loss: 0.0156 - accuracy: 0.9942\n",
      "Epoch 49/60\n",
      "38/38 - 0s - loss: 0.0122 - accuracy: 0.9959\n",
      "Epoch 50/60\n",
      "38/38 - 0s - loss: 0.0181 - accuracy: 0.9934\n",
      "Epoch 51/60\n",
      "38/38 - 0s - loss: 0.0146 - accuracy: 0.9926\n",
      "Epoch 52/60\n",
      "38/38 - 0s - loss: 0.0110 - accuracy: 0.9959\n",
      "Epoch 53/60\n",
      "38/38 - 0s - loss: 0.0129 - accuracy: 0.9934\n",
      "Epoch 54/60\n",
      "38/38 - 0s - loss: 0.0095 - accuracy: 0.9967\n",
      "Epoch 55/60\n",
      "38/38 - 0s - loss: 0.0094 - accuracy: 0.9983\n",
      "Epoch 56/60\n",
      "38/38 - 0s - loss: 0.0104 - accuracy: 0.9942\n",
      "Epoch 57/60\n",
      "38/38 - 0s - loss: 0.0121 - accuracy: 0.9950\n",
      "Epoch 58/60\n",
      "38/38 - 0s - loss: 0.0145 - accuracy: 0.9934\n",
      "Epoch 59/60\n",
      "38/38 - 0s - loss: 0.0102 - accuracy: 0.9967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60\n",
      "38/38 - 0s - loss: 0.0121 - accuracy: 0.9950\n",
      "13/13 - 0s - loss: 0.0170 - accuracy: 0.9926\n",
      "----------------------------------------------------------------------------------------\n",
      "TSLA: Normal Neural Network - Loss: 0.017017124220728874, Accuracy: 0.9925558567047119\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C077A4A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT'\n",
      " 'TAKE MONEY OUT']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(1480, 7) (1480,)\n",
      "Epoch 1/60\n",
      "35/35 - 0s - loss: 0.3797 - accuracy: 0.8694\n",
      "Epoch 2/60\n",
      "35/35 - 0s - loss: 0.1634 - accuracy: 0.9730\n",
      "Epoch 3/60\n",
      "35/35 - 0s - loss: 0.1093 - accuracy: 0.9847\n",
      "Epoch 4/60\n",
      "35/35 - 0s - loss: 0.0882 - accuracy: 0.9784\n",
      "Epoch 5/60\n",
      "35/35 - 0s - loss: 0.0719 - accuracy: 0.9856\n",
      "Epoch 6/60\n",
      "35/35 - 0s - loss: 0.0606 - accuracy: 0.9847\n",
      "Epoch 7/60\n",
      "35/35 - 0s - loss: 0.0545 - accuracy: 0.9856\n",
      "Epoch 8/60\n",
      "35/35 - 0s - loss: 0.0519 - accuracy: 0.9865\n",
      "Epoch 9/60\n",
      "35/35 - 0s - loss: 0.0479 - accuracy: 0.9847\n",
      "Epoch 10/60\n",
      "35/35 - 0s - loss: 0.0464 - accuracy: 0.9838\n",
      "Epoch 11/60\n",
      "35/35 - 0s - loss: 0.0382 - accuracy: 0.9910\n",
      "Epoch 12/60\n",
      "35/35 - 0s - loss: 0.0431 - accuracy: 0.9829\n",
      "Epoch 13/60\n",
      "35/35 - 0s - loss: 0.0434 - accuracy: 0.9883\n",
      "Epoch 14/60\n",
      "35/35 - 0s - loss: 0.0358 - accuracy: 0.9865\n",
      "Epoch 15/60\n",
      "35/35 - 0s - loss: 0.0328 - accuracy: 0.9874\n",
      "Epoch 16/60\n",
      "35/35 - 0s - loss: 0.0354 - accuracy: 0.9892\n",
      "Epoch 17/60\n",
      "35/35 - 0s - loss: 0.0322 - accuracy: 0.9865\n",
      "Epoch 18/60\n",
      "35/35 - 0s - loss: 0.0266 - accuracy: 0.9928\n",
      "Epoch 19/60\n",
      "35/35 - 0s - loss: 0.0274 - accuracy: 0.9937\n",
      "Epoch 20/60\n",
      "35/35 - 0s - loss: 0.0255 - accuracy: 0.9883\n",
      "Epoch 21/60\n",
      "35/35 - 0s - loss: 0.0272 - accuracy: 0.9892\n",
      "Epoch 22/60\n",
      "35/35 - 0s - loss: 0.0279 - accuracy: 0.9910\n",
      "Epoch 23/60\n",
      "35/35 - 0s - loss: 0.0249 - accuracy: 0.9892\n",
      "Epoch 24/60\n",
      "35/35 - 0s - loss: 0.0216 - accuracy: 0.9928\n",
      "Epoch 25/60\n",
      "35/35 - 0s - loss: 0.0387 - accuracy: 0.9838\n",
      "Epoch 26/60\n",
      "35/35 - 0s - loss: 0.0247 - accuracy: 0.9919\n",
      "Epoch 27/60\n",
      "35/35 - 0s - loss: 0.0218 - accuracy: 0.9892\n",
      "Epoch 28/60\n",
      "35/35 - 0s - loss: 0.0275 - accuracy: 0.9892\n",
      "Epoch 29/60\n",
      "35/35 - 0s - loss: 0.0267 - accuracy: 0.9892\n",
      "Epoch 30/60\n",
      "35/35 - 0s - loss: 0.0405 - accuracy: 0.9874\n",
      "Epoch 31/60\n",
      "35/35 - 0s - loss: 0.0281 - accuracy: 0.9892\n",
      "Epoch 32/60\n",
      "35/35 - 0s - loss: 0.0240 - accuracy: 0.9883\n",
      "Epoch 33/60\n",
      "35/35 - 0s - loss: 0.0235 - accuracy: 0.9874\n",
      "Epoch 34/60\n",
      "35/35 - 0s - loss: 0.0191 - accuracy: 0.9937\n",
      "Epoch 35/60\n",
      "35/35 - 0s - loss: 0.0361 - accuracy: 0.9820\n",
      "Epoch 36/60\n",
      "35/35 - 0s - loss: 0.0280 - accuracy: 0.9874\n",
      "Epoch 37/60\n",
      "35/35 - 0s - loss: 0.0193 - accuracy: 0.9937\n",
      "Epoch 38/60\n",
      "35/35 - 0s - loss: 0.0196 - accuracy: 0.9910\n",
      "Epoch 39/60\n",
      "35/35 - 0s - loss: 0.0170 - accuracy: 0.9928\n",
      "Epoch 40/60\n",
      "35/35 - 0s - loss: 0.0174 - accuracy: 0.9946\n",
      "Epoch 41/60\n",
      "35/35 - 0s - loss: 0.0225 - accuracy: 0.9892\n",
      "Epoch 42/60\n",
      "35/35 - 0s - loss: 0.0222 - accuracy: 0.9901\n",
      "Epoch 43/60\n",
      "35/35 - 0s - loss: 0.0150 - accuracy: 0.9955\n",
      "Epoch 44/60\n",
      "35/35 - 0s - loss: 0.0137 - accuracy: 0.9946\n",
      "Epoch 45/60\n",
      "35/35 - 0s - loss: 0.0145 - accuracy: 0.9919\n",
      "Epoch 46/60\n",
      "35/35 - 0s - loss: 0.0174 - accuracy: 0.9946\n",
      "Epoch 47/60\n",
      "35/35 - 0s - loss: 0.0273 - accuracy: 0.9874\n",
      "Epoch 48/60\n",
      "35/35 - 0s - loss: 0.0185 - accuracy: 0.9910\n",
      "Epoch 49/60\n",
      "35/35 - 0s - loss: 0.0158 - accuracy: 0.9937\n",
      "Epoch 50/60\n",
      "35/35 - 0s - loss: 0.0181 - accuracy: 0.9919\n",
      "Epoch 51/60\n",
      "35/35 - 0s - loss: 0.0123 - accuracy: 0.9955\n",
      "Epoch 52/60\n",
      "35/35 - 0s - loss: 0.0154 - accuracy: 0.9919\n",
      "Epoch 53/60\n",
      "35/35 - 0s - loss: 0.0129 - accuracy: 0.9955\n",
      "Epoch 54/60\n",
      "35/35 - 0s - loss: 0.0372 - accuracy: 0.9883\n",
      "Epoch 55/60\n",
      "35/35 - 0s - loss: 0.0196 - accuracy: 0.9928\n",
      "Epoch 56/60\n",
      "35/35 - 0s - loss: 0.0251 - accuracy: 0.9901\n",
      "Epoch 57/60\n",
      "35/35 - 0s - loss: 0.0183 - accuracy: 0.9910\n",
      "Epoch 58/60\n",
      "35/35 - 0s - loss: 0.0116 - accuracy: 0.9964\n",
      "Epoch 59/60\n",
      "35/35 - 0s - loss: 0.0144 - accuracy: 0.9910\n",
      "Epoch 60/60\n",
      "35/35 - 0s - loss: 0.0194 - accuracy: 0.9919\n",
      "12/12 - 0s - loss: 0.0151 - accuracy: 0.9973\n",
      "----------------------------------------------------------------------------------------\n",
      "AMZN: Normal Neural Network - Loss: 0.01513748150318861, Accuracy: 0.9972972869873047\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C7FCBE378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'ADD MONEY'\n",
      " 'ADD MONEY']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(187, 7) (187,)\n",
      "Epoch 1/60\n",
      "5/5 - 0s - loss: 0.5583 - accuracy: 0.7786\n",
      "Epoch 2/60\n",
      "5/5 - 0s - loss: 0.4288 - accuracy: 0.8857\n",
      "Epoch 3/60\n",
      "5/5 - 0s - loss: 0.3433 - accuracy: 0.9286\n",
      "Epoch 4/60\n",
      "5/5 - 0s - loss: 0.2768 - accuracy: 0.9429\n",
      "Epoch 5/60\n",
      "5/5 - 0s - loss: 0.2386 - accuracy: 0.9429\n",
      "Epoch 6/60\n",
      "5/5 - 0s - loss: 0.2063 - accuracy: 0.9571\n",
      "Epoch 7/60\n",
      "5/5 - 0s - loss: 0.1801 - accuracy: 0.9786\n",
      "Epoch 8/60\n",
      "5/5 - 0s - loss: 0.1625 - accuracy: 0.9786\n",
      "Epoch 9/60\n",
      "5/5 - 0s - loss: 0.1457 - accuracy: 0.9786\n",
      "Epoch 10/60\n",
      "5/5 - 0s - loss: 0.1302 - accuracy: 0.9857\n",
      "Epoch 11/60\n",
      "5/5 - 0s - loss: 0.1187 - accuracy: 0.9929\n",
      "Epoch 12/60\n",
      "5/5 - 0s - loss: 0.1078 - accuracy: 0.9929\n",
      "Epoch 13/60\n",
      "5/5 - 0s - loss: 0.0993 - accuracy: 0.9929\n",
      "Epoch 14/60\n",
      "5/5 - 0s - loss: 0.0911 - accuracy: 1.0000\n",
      "Epoch 15/60\n",
      "5/5 - 0s - loss: 0.0850 - accuracy: 1.0000\n",
      "Epoch 16/60\n",
      "5/5 - 0s - loss: 0.0797 - accuracy: 0.9929\n",
      "Epoch 17/60\n",
      "5/5 - 0s - loss: 0.0747 - accuracy: 0.9929\n",
      "Epoch 18/60\n",
      "5/5 - 0s - loss: 0.0702 - accuracy: 0.9929\n",
      "Epoch 19/60\n",
      "5/5 - 0s - loss: 0.0675 - accuracy: 0.9929\n",
      "Epoch 20/60\n",
      "5/5 - 0s - loss: 0.0634 - accuracy: 1.0000\n",
      "Epoch 21/60\n",
      "5/5 - 0s - loss: 0.0585 - accuracy: 1.0000\n",
      "Epoch 22/60\n",
      "5/5 - 0s - loss: 0.0568 - accuracy: 1.0000\n",
      "Epoch 23/60\n",
      "5/5 - 0s - loss: 0.0536 - accuracy: 0.9929\n",
      "Epoch 24/60\n",
      "5/5 - 0s - loss: 0.0520 - accuracy: 0.9929\n",
      "Epoch 25/60\n",
      "5/5 - 0s - loss: 0.0498 - accuracy: 0.9929\n",
      "Epoch 26/60\n",
      "5/5 - 0s - loss: 0.0485 - accuracy: 0.9929\n",
      "Epoch 27/60\n",
      "5/5 - 0s - loss: 0.0454 - accuracy: 0.9929\n",
      "Epoch 28/60\n",
      "5/5 - 0s - loss: 0.0431 - accuracy: 1.0000\n",
      "Epoch 29/60\n",
      "5/5 - 0s - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 30/60\n",
      "5/5 - 0s - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 31/60\n",
      "5/5 - 0s - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 32/60\n",
      "5/5 - 0s - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 33/60\n",
      "5/5 - 0s - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 34/60\n",
      "5/5 - 0s - loss: 0.0351 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/60\n",
      "5/5 - 0s - loss: 0.0334 - accuracy: 1.0000\n",
      "Epoch 36/60\n",
      "5/5 - 0s - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 37/60\n",
      "5/5 - 0s - loss: 0.0327 - accuracy: 1.0000\n",
      "Epoch 38/60\n",
      "5/5 - 0s - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 39/60\n",
      "5/5 - 0s - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 40/60\n",
      "5/5 - 0s - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 41/60\n",
      "5/5 - 0s - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 42/60\n",
      "5/5 - 0s - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 43/60\n",
      "5/5 - 0s - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 44/60\n",
      "5/5 - 0s - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 45/60\n",
      "5/5 - 0s - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 46/60\n",
      "5/5 - 0s - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 47/60\n",
      "5/5 - 0s - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 48/60\n",
      "5/5 - 0s - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 49/60\n",
      "5/5 - 0s - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 50/60\n",
      "5/5 - 0s - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 51/60\n",
      "5/5 - 0s - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 52/60\n",
      "5/5 - 0s - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 53/60\n",
      "5/5 - 0s - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 54/60\n",
      "5/5 - 0s - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 55/60\n",
      "5/5 - 0s - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 56/60\n",
      "5/5 - 0s - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 57/60\n",
      "5/5 - 0s - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 58/60\n",
      "5/5 - 0s - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 59/60\n",
      "5/5 - 0s - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 60/60\n",
      "5/5 - 0s - loss: 0.0183 - accuracy: 1.0000\n",
      "2/2 - 0s - loss: 0.0468 - accuracy: 0.9787\n",
      "----------------------------------------------------------------------------------------\n",
      "SPY: Normal Neural Network - Loss: 0.04675118997693062, Accuracy: 0.978723406791687\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C02FF1AE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(68, 7) (68,)\n",
      "Epoch 1/60\n",
      "2/2 - 0s - loss: 0.8012 - accuracy: 0.3922\n",
      "Epoch 2/60\n",
      "2/2 - 0s - loss: 0.5969 - accuracy: 0.6863\n",
      "Epoch 3/60\n",
      "2/2 - 0s - loss: 0.5043 - accuracy: 0.9020\n",
      "Epoch 4/60\n",
      "2/2 - 0s - loss: 0.4574 - accuracy: 0.8627\n",
      "Epoch 5/60\n",
      "2/2 - 0s - loss: 0.4226 - accuracy: 0.8431\n",
      "Epoch 6/60\n",
      "2/2 - 0s - loss: 0.3892 - accuracy: 0.8824\n",
      "Epoch 7/60\n",
      "2/2 - 0s - loss: 0.3593 - accuracy: 0.9412\n",
      "Epoch 8/60\n",
      "2/2 - 0s - loss: 0.3340 - accuracy: 0.9412\n",
      "Epoch 9/60\n",
      "2/2 - 0s - loss: 0.3089 - accuracy: 0.9412\n",
      "Epoch 10/60\n",
      "2/2 - 0s - loss: 0.2896 - accuracy: 0.9412\n",
      "Epoch 11/60\n",
      "2/2 - 0s - loss: 0.2744 - accuracy: 0.9412\n",
      "Epoch 12/60\n",
      "2/2 - 0s - loss: 0.2601 - accuracy: 0.9216\n",
      "Epoch 13/60\n",
      "2/2 - 0s - loss: 0.2488 - accuracy: 0.9216\n",
      "Epoch 14/60\n",
      "2/2 - 0s - loss: 0.2363 - accuracy: 0.9608\n",
      "Epoch 15/60\n",
      "2/2 - 0s - loss: 0.2260 - accuracy: 0.9804\n",
      "Epoch 16/60\n",
      "2/2 - 0s - loss: 0.2145 - accuracy: 1.0000\n",
      "Epoch 17/60\n",
      "2/2 - 0s - loss: 0.2054 - accuracy: 1.0000\n",
      "Epoch 18/60\n",
      "2/2 - 0s - loss: 0.1955 - accuracy: 1.0000\n",
      "Epoch 19/60\n",
      "2/2 - 0s - loss: 0.1880 - accuracy: 1.0000\n",
      "Epoch 20/60\n",
      "2/2 - 0s - loss: 0.1808 - accuracy: 1.0000\n",
      "Epoch 21/60\n",
      "2/2 - 0s - loss: 0.1752 - accuracy: 1.0000\n",
      "Epoch 22/60\n",
      "2/2 - 0s - loss: 0.1689 - accuracy: 1.0000\n",
      "Epoch 23/60\n",
      "2/2 - 0s - loss: 0.1620 - accuracy: 1.0000\n",
      "Epoch 24/60\n",
      "2/2 - 0s - loss: 0.1551 - accuracy: 1.0000\n",
      "Epoch 25/60\n",
      "2/2 - 0s - loss: 0.1501 - accuracy: 1.0000\n",
      "Epoch 26/60\n",
      "2/2 - 0s - loss: 0.1443 - accuracy: 1.0000\n",
      "Epoch 27/60\n",
      "2/2 - 0s - loss: 0.1392 - accuracy: 1.0000\n",
      "Epoch 28/60\n",
      "2/2 - 0s - loss: 0.1337 - accuracy: 1.0000\n",
      "Epoch 29/60\n",
      "2/2 - 0s - loss: 0.1295 - accuracy: 1.0000\n",
      "Epoch 30/60\n",
      "2/2 - 0s - loss: 0.1255 - accuracy: 1.0000\n",
      "Epoch 31/60\n",
      "2/2 - 0s - loss: 0.1222 - accuracy: 1.0000\n",
      "Epoch 32/60\n",
      "2/2 - 0s - loss: 0.1190 - accuracy: 1.0000\n",
      "Epoch 33/60\n",
      "2/2 - 0s - loss: 0.1135 - accuracy: 1.0000\n",
      "Epoch 34/60\n",
      "2/2 - 0s - loss: 0.1084 - accuracy: 1.0000\n",
      "Epoch 35/60\n",
      "2/2 - 0s - loss: 0.1054 - accuracy: 1.0000\n",
      "Epoch 36/60\n",
      "2/2 - 0s - loss: 0.1022 - accuracy: 1.0000\n",
      "Epoch 37/60\n",
      "2/2 - 0s - loss: 0.0989 - accuracy: 1.0000\n",
      "Epoch 38/60\n",
      "2/2 - 0s - loss: 0.0960 - accuracy: 1.0000\n",
      "Epoch 39/60\n",
      "2/2 - 0s - loss: 0.0929 - accuracy: 1.0000\n",
      "Epoch 40/60\n",
      "2/2 - 0s - loss: 0.0895 - accuracy: 1.0000\n",
      "Epoch 41/60\n",
      "2/2 - 0s - loss: 0.0888 - accuracy: 1.0000\n",
      "Epoch 42/60\n",
      "2/2 - 0s - loss: 0.0846 - accuracy: 1.0000\n",
      "Epoch 43/60\n",
      "2/2 - 0s - loss: 0.0819 - accuracy: 1.0000\n",
      "Epoch 44/60\n",
      "2/2 - 0s - loss: 0.0788 - accuracy: 1.0000\n",
      "Epoch 45/60\n",
      "2/2 - 0s - loss: 0.0779 - accuracy: 1.0000\n",
      "Epoch 46/60\n",
      "2/2 - 0s - loss: 0.0748 - accuracy: 1.0000\n",
      "Epoch 47/60\n",
      "2/2 - 0s - loss: 0.0725 - accuracy: 1.0000\n",
      "Epoch 48/60\n",
      "2/2 - 0s - loss: 0.0700 - accuracy: 1.0000\n",
      "Epoch 49/60\n",
      "2/2 - 0s - loss: 0.0680 - accuracy: 1.0000\n",
      "Epoch 50/60\n",
      "2/2 - 0s - loss: 0.0659 - accuracy: 1.0000\n",
      "Epoch 51/60\n",
      "2/2 - 0s - loss: 0.0636 - accuracy: 1.0000\n",
      "Epoch 52/60\n",
      "2/2 - 0s - loss: 0.0619 - accuracy: 1.0000\n",
      "Epoch 53/60\n",
      "2/2 - 0s - loss: 0.0603 - accuracy: 1.0000\n",
      "Epoch 54/60\n",
      "2/2 - 0s - loss: 0.0590 - accuracy: 1.0000\n",
      "Epoch 55/60\n",
      "2/2 - 0s - loss: 0.0577 - accuracy: 1.0000\n",
      "Epoch 56/60\n",
      "2/2 - 0s - loss: 0.0558 - accuracy: 1.0000\n",
      "Epoch 57/60\n",
      "2/2 - 0s - loss: 0.0542 - accuracy: 1.0000\n",
      "Epoch 58/60\n",
      "2/2 - 0s - loss: 0.0528 - accuracy: 1.0000\n",
      "Epoch 59/60\n",
      "2/2 - 0s - loss: 0.0516 - accuracy: 1.0000\n",
      "Epoch 60/60\n",
      "2/2 - 0s - loss: 0.0501 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0941 - accuracy: 0.9412\n",
      "----------------------------------------------------------------------------------------\n",
      "DOCU: Normal Neural Network - Loss: 0.09413062781095505, Accuracy: 0.9411764740943909\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C7FE4E620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(1981, 7) (1981,)\n",
      "Epoch 1/60\n",
      "47/47 - 0s - loss: 0.5992 - accuracy: 0.7064\n",
      "Epoch 2/60\n",
      "47/47 - 0s - loss: 0.4188 - accuracy: 0.8788\n",
      "Epoch 3/60\n",
      "47/47 - 0s - loss: 0.2914 - accuracy: 0.9185\n",
      "Epoch 4/60\n",
      "47/47 - 0s - loss: 0.2028 - accuracy: 0.9576\n",
      "Epoch 5/60\n",
      "47/47 - 0s - loss: 0.1844 - accuracy: 0.9219\n",
      "Epoch 6/60\n",
      "47/47 - 0s - loss: 0.1383 - accuracy: 0.9609\n",
      "Epoch 7/60\n",
      "47/47 - 0s - loss: 0.1288 - accuracy: 0.9582\n",
      "Epoch 8/60\n",
      "47/47 - 0s - loss: 0.1066 - accuracy: 0.9710\n",
      "Epoch 9/60\n",
      "47/47 - 0s - loss: 0.1035 - accuracy: 0.9663\n",
      "Epoch 10/60\n",
      "47/47 - 0s - loss: 0.0988 - accuracy: 0.9636\n",
      "Epoch 11/60\n",
      "47/47 - 0s - loss: 0.0812 - accuracy: 0.9737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/60\n",
      "47/47 - 0s - loss: 0.0767 - accuracy: 0.9724\n",
      "Epoch 13/60\n",
      "47/47 - 0s - loss: 0.0750 - accuracy: 0.9751\n",
      "Epoch 14/60\n",
      "47/47 - 0s - loss: 0.0684 - accuracy: 0.9805\n",
      "Epoch 15/60\n",
      "47/47 - 0s - loss: 0.0596 - accuracy: 0.9825\n",
      "Epoch 16/60\n",
      "47/47 - 0s - loss: 0.0624 - accuracy: 0.9811\n",
      "Epoch 17/60\n",
      "47/47 - 0s - loss: 0.0583 - accuracy: 0.9791\n",
      "Epoch 18/60\n",
      "47/47 - 0s - loss: 0.0570 - accuracy: 0.9825\n",
      "Epoch 19/60\n",
      "47/47 - 0s - loss: 0.0510 - accuracy: 0.9838\n",
      "Epoch 20/60\n",
      "47/47 - 0s - loss: 0.0472 - accuracy: 0.9859\n",
      "Epoch 21/60\n",
      "47/47 - 0s - loss: 0.0668 - accuracy: 0.9710\n",
      "Epoch 22/60\n",
      "47/47 - 0s - loss: 0.0595 - accuracy: 0.9737\n",
      "Epoch 23/60\n",
      "47/47 - 0s - loss: 0.0401 - accuracy: 0.9899\n",
      "Epoch 24/60\n",
      "47/47 - 0s - loss: 0.0520 - accuracy: 0.9798\n",
      "Epoch 25/60\n",
      "47/47 - 0s - loss: 0.0679 - accuracy: 0.9758\n",
      "Epoch 26/60\n",
      "47/47 - 0s - loss: 0.0413 - accuracy: 0.9886\n",
      "Epoch 27/60\n",
      "47/47 - 0s - loss: 0.0390 - accuracy: 0.9872\n",
      "Epoch 28/60\n",
      "47/47 - 0s - loss: 0.0421 - accuracy: 0.9811\n",
      "Epoch 29/60\n",
      "47/47 - 0s - loss: 0.0416 - accuracy: 0.9838\n",
      "Epoch 30/60\n",
      "47/47 - 0s - loss: 0.0471 - accuracy: 0.9832\n",
      "Epoch 31/60\n",
      "47/47 - 0s - loss: 0.0570 - accuracy: 0.9744\n",
      "Epoch 32/60\n",
      "47/47 - 0s - loss: 0.0415 - accuracy: 0.9811\n",
      "Epoch 33/60\n",
      "47/47 - 0s - loss: 0.0422 - accuracy: 0.9832\n",
      "Epoch 34/60\n",
      "47/47 - 0s - loss: 0.0443 - accuracy: 0.9832\n",
      "Epoch 35/60\n",
      "47/47 - 0s - loss: 0.0450 - accuracy: 0.9872\n",
      "Epoch 36/60\n",
      "47/47 - 0s - loss: 0.0450 - accuracy: 0.9805\n",
      "Epoch 37/60\n",
      "47/47 - 0s - loss: 0.0422 - accuracy: 0.9845\n",
      "Epoch 38/60\n",
      "47/47 - 0s - loss: 0.0493 - accuracy: 0.9785\n",
      "Epoch 39/60\n",
      "47/47 - 0s - loss: 0.0461 - accuracy: 0.9838\n",
      "Epoch 40/60\n",
      "47/47 - 0s - loss: 0.0423 - accuracy: 0.9845\n",
      "Epoch 41/60\n",
      "47/47 - 0s - loss: 0.0391 - accuracy: 0.9832\n",
      "Epoch 42/60\n",
      "47/47 - 0s - loss: 0.0364 - accuracy: 0.9872\n",
      "Epoch 43/60\n",
      "47/47 - 0s - loss: 0.0321 - accuracy: 0.9879\n",
      "Epoch 44/60\n",
      "47/47 - 0s - loss: 0.0454 - accuracy: 0.9805\n",
      "Epoch 45/60\n",
      "47/47 - 0s - loss: 0.0402 - accuracy: 0.9838\n",
      "Epoch 46/60\n",
      "47/47 - 0s - loss: 0.0315 - accuracy: 0.9886\n",
      "Epoch 47/60\n",
      "47/47 - 0s - loss: 0.0488 - accuracy: 0.9798\n",
      "Epoch 48/60\n",
      "47/47 - 0s - loss: 0.0388 - accuracy: 0.9832\n",
      "Epoch 49/60\n",
      "47/47 - 0s - loss: 0.0483 - accuracy: 0.9825\n",
      "Epoch 50/60\n",
      "47/47 - 0s - loss: 0.0312 - accuracy: 0.9879\n",
      "Epoch 51/60\n",
      "47/47 - 0s - loss: 0.0252 - accuracy: 0.9919\n",
      "Epoch 52/60\n",
      "47/47 - 0s - loss: 0.0275 - accuracy: 0.9899\n",
      "Epoch 53/60\n",
      "47/47 - 0s - loss: 0.0418 - accuracy: 0.9845\n",
      "Epoch 54/60\n",
      "47/47 - 0s - loss: 0.0363 - accuracy: 0.9879\n",
      "Epoch 55/60\n",
      "47/47 - 0s - loss: 0.0351 - accuracy: 0.9838\n",
      "Epoch 56/60\n",
      "47/47 - 0s - loss: 0.0369 - accuracy: 0.9872\n",
      "Epoch 57/60\n",
      "47/47 - 0s - loss: 0.0261 - accuracy: 0.9892\n",
      "Epoch 58/60\n",
      "47/47 - 0s - loss: 0.0358 - accuracy: 0.9859\n",
      "Epoch 59/60\n",
      "47/47 - 0s - loss: 0.0231 - accuracy: 0.9933\n",
      "Epoch 60/60\n",
      "47/47 - 0s - loss: 0.0269 - accuracy: 0.9886\n",
      "16/16 - 0s - loss: 0.0152 - accuracy: 0.9940\n",
      "----------------------------------------------------------------------------------------\n",
      "NFLX: Normal Neural Network - Loss: 0.01516908872872591, Accuracy: 0.9939516186714172\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C078782F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(1145, 7) (1145,)\n",
      "Epoch 1/60\n",
      "27/27 - 0s - loss: 0.5585 - accuracy: 0.7611\n",
      "Epoch 2/60\n",
      "27/27 - 0s - loss: 0.3828 - accuracy: 0.9021\n",
      "Epoch 3/60\n",
      "27/27 - 0s - loss: 0.2815 - accuracy: 0.9452\n",
      "Epoch 4/60\n",
      "27/27 - 0s - loss: 0.2194 - accuracy: 0.9545\n",
      "Epoch 5/60\n",
      "27/27 - 0s - loss: 0.1896 - accuracy: 0.9429\n",
      "Epoch 6/60\n",
      "27/27 - 0s - loss: 0.1697 - accuracy: 0.9417\n",
      "Epoch 7/60\n",
      "27/27 - 0s - loss: 0.1330 - accuracy: 0.9732\n",
      "Epoch 8/60\n",
      "27/27 - 0s - loss: 0.1272 - accuracy: 0.9615\n",
      "Epoch 9/60\n",
      "27/27 - 0s - loss: 0.1063 - accuracy: 0.9837\n",
      "Epoch 10/60\n",
      "27/27 - 0s - loss: 0.0946 - accuracy: 0.9848\n",
      "Epoch 11/60\n",
      "27/27 - 0s - loss: 0.0856 - accuracy: 0.9814\n",
      "Epoch 12/60\n",
      "27/27 - 0s - loss: 0.0768 - accuracy: 0.9918\n",
      "Epoch 13/60\n",
      "27/27 - 0s - loss: 0.0766 - accuracy: 0.9837\n",
      "Epoch 14/60\n",
      "27/27 - 0s - loss: 0.0759 - accuracy: 0.9790\n",
      "Epoch 15/60\n",
      "27/27 - 0s - loss: 0.0752 - accuracy: 0.9744\n",
      "Epoch 16/60\n",
      "27/27 - 0s - loss: 0.0709 - accuracy: 0.9779\n",
      "Epoch 17/60\n",
      "27/27 - 0s - loss: 0.0655 - accuracy: 0.9767\n",
      "Epoch 18/60\n",
      "27/27 - 0s - loss: 0.0575 - accuracy: 0.9872\n",
      "Epoch 19/60\n",
      "27/27 - 0s - loss: 0.0552 - accuracy: 0.9883\n",
      "Epoch 20/60\n",
      "27/27 - 0s - loss: 0.0494 - accuracy: 0.9930\n",
      "Epoch 21/60\n",
      "27/27 - 0s - loss: 0.0480 - accuracy: 0.9907\n",
      "Epoch 22/60\n",
      "27/27 - 0s - loss: 0.0517 - accuracy: 0.9860\n",
      "Epoch 23/60\n",
      "27/27 - 0s - loss: 0.0457 - accuracy: 0.9895\n",
      "Epoch 24/60\n",
      "27/27 - 0s - loss: 0.0484 - accuracy: 0.9872\n",
      "Epoch 25/60\n",
      "27/27 - 0s - loss: 0.0476 - accuracy: 0.9814\n",
      "Epoch 26/60\n",
      "27/27 - 0s - loss: 0.0436 - accuracy: 0.9883\n",
      "Epoch 27/60\n",
      "27/27 - 0s - loss: 0.0373 - accuracy: 0.9907\n",
      "Epoch 28/60\n",
      "27/27 - 0s - loss: 0.0381 - accuracy: 0.9907\n",
      "Epoch 29/60\n",
      "27/27 - 0s - loss: 0.0517 - accuracy: 0.9802\n",
      "Epoch 30/60\n",
      "27/27 - 0s - loss: 0.0400 - accuracy: 0.9872\n",
      "Epoch 31/60\n",
      "27/27 - 0s - loss: 0.0467 - accuracy: 0.9837\n",
      "Epoch 32/60\n",
      "27/27 - 0s - loss: 0.0613 - accuracy: 0.9744\n",
      "Epoch 33/60\n",
      "27/27 - 0s - loss: 0.0456 - accuracy: 0.9825\n",
      "Epoch 34/60\n",
      "27/27 - 0s - loss: 0.0314 - accuracy: 0.9942\n",
      "Epoch 35/60\n",
      "27/27 - 0s - loss: 0.0303 - accuracy: 0.9918\n",
      "Epoch 36/60\n",
      "27/27 - 0s - loss: 0.0266 - accuracy: 0.9953\n",
      "Epoch 37/60\n",
      "27/27 - 0s - loss: 0.0297 - accuracy: 0.9883\n",
      "Epoch 38/60\n",
      "27/27 - 0s - loss: 0.0306 - accuracy: 0.9907\n",
      "Epoch 39/60\n",
      "27/27 - 0s - loss: 0.0309 - accuracy: 0.9872\n",
      "Epoch 40/60\n",
      "27/27 - 0s - loss: 0.0256 - accuracy: 0.9918\n",
      "Epoch 41/60\n",
      "27/27 - 0s - loss: 0.0223 - accuracy: 0.9907\n",
      "Epoch 42/60\n",
      "27/27 - 0s - loss: 0.0255 - accuracy: 0.9930\n",
      "Epoch 43/60\n",
      "27/27 - 0s - loss: 0.0260 - accuracy: 0.9942\n",
      "Epoch 44/60\n",
      "27/27 - 0s - loss: 0.0242 - accuracy: 0.9930\n",
      "Epoch 45/60\n",
      "27/27 - 0s - loss: 0.0278 - accuracy: 0.9918\n",
      "Epoch 46/60\n",
      "27/27 - 0s - loss: 0.0276 - accuracy: 0.9918\n",
      "Epoch 47/60\n",
      "27/27 - 0s - loss: 0.0172 - accuracy: 0.9977\n",
      "Epoch 48/60\n",
      "27/27 - 0s - loss: 0.0182 - accuracy: 0.9965\n",
      "Epoch 49/60\n",
      "27/27 - 0s - loss: 0.0219 - accuracy: 0.9930\n",
      "Epoch 50/60\n",
      "27/27 - 0s - loss: 0.0180 - accuracy: 0.9965\n",
      "Epoch 51/60\n",
      "27/27 - 0s - loss: 0.0196 - accuracy: 0.9953\n",
      "Epoch 52/60\n",
      "27/27 - 0s - loss: 0.0190 - accuracy: 0.9942\n",
      "Epoch 53/60\n",
      "27/27 - 0s - loss: 0.0170 - accuracy: 0.9988\n",
      "Epoch 54/60\n",
      "27/27 - 0s - loss: 0.0163 - accuracy: 0.9965\n",
      "Epoch 55/60\n",
      "27/27 - 0s - loss: 0.0176 - accuracy: 0.9930\n",
      "Epoch 56/60\n",
      "27/27 - 0s - loss: 0.0254 - accuracy: 0.9883\n",
      "Epoch 57/60\n",
      "27/27 - 0s - loss: 0.0409 - accuracy: 0.9837\n",
      "Epoch 58/60\n",
      "27/27 - 0s - loss: 0.0235 - accuracy: 0.9918\n",
      "Epoch 59/60\n",
      "27/27 - 0s - loss: 0.0189 - accuracy: 0.9942\n",
      "Epoch 60/60\n",
      "27/27 - 0s - loss: 0.0337 - accuracy: 0.9895\n",
      "9/9 - 0s - loss: 0.0162 - accuracy: 0.9930\n",
      "----------------------------------------------------------------------------------------\n",
      "NKE: Normal Neural Network - Loss: 0.016197336837649345, Accuracy: 0.99303138256073\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C0304B730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'TAKE MONEY OUT']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(946, 7) (946,)\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.4103 - accuracy: 0.8886\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.1866 - accuracy: 0.9788\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.1278 - accuracy: 0.9803\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.0958 - accuracy: 0.9873\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.0758 - accuracy: 0.9901\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.0716 - accuracy: 0.9929\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.0577 - accuracy: 0.9859\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.0593 - accuracy: 0.9803\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.0449 - accuracy: 0.9944\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.0476 - accuracy: 0.9831\n",
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.0415 - accuracy: 0.9901\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.0356 - accuracy: 0.9915\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.0368 - accuracy: 0.9901\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.0316 - accuracy: 0.9929\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.0297 - accuracy: 0.9887\n",
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.0273 - accuracy: 0.9915\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.0295 - accuracy: 0.9887\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.0317 - accuracy: 0.9859\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.0266 - accuracy: 0.9901\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.0341 - accuracy: 0.9831\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.0258 - accuracy: 0.9901\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.0282 - accuracy: 0.9887\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.0331 - accuracy: 0.9873\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.0218 - accuracy: 0.9944\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.0178 - accuracy: 0.9972\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.0164 - accuracy: 0.9944\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.0205 - accuracy: 0.9901\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.0282 - accuracy: 0.9887\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.0148 - accuracy: 0.9958\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.0178 - accuracy: 0.9929\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.0142 - accuracy: 0.9972\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.0141 - accuracy: 0.9958\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.0154 - accuracy: 0.9958\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.0143 - accuracy: 0.9972\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.0150 - accuracy: 0.9944\n",
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.0130 - accuracy: 0.9972\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.0146 - accuracy: 0.9944\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.0162 - accuracy: 0.9944\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.0119 - accuracy: 0.9986\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.0107 - accuracy: 0.9972\n",
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.0197 - accuracy: 0.9929\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.0283 - accuracy: 0.9901\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.0204 - accuracy: 0.9944\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.0110 - accuracy: 0.9972\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.0121 - accuracy: 0.9972\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.0245 - accuracy: 0.9873\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.0231 - accuracy: 0.9859\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.0120 - accuracy: 0.9944\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.0155 - accuracy: 0.9929\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.0102 - accuracy: 0.9986\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.0097 - accuracy: 0.9972\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.0149 - accuracy: 0.9929\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.0531 - accuracy: 0.9803\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.0363 - accuracy: 0.9873\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.0118 - accuracy: 0.9929\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.0087 - accuracy: 0.9972\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.0077 - accuracy: 0.9972\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.0092 - accuracy: 0.9944\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.0085 - accuracy: 0.9972\n",
      "8/8 - 0s - loss: 0.0275 - accuracy: 0.9916\n",
      "----------------------------------------------------------------------------------------\n",
      "PG: Normal Neural Network - Loss: 0.0274726003408432, Accuracy: 0.9915611743927002\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C052DA400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "AAPL_Deep = deep_learning('AAPL',aapl_complete)\n",
    "TSLA_Deep = deep_learning('TSLA',tsla_complete)\n",
    "AMZN_Deep = deep_learning('AMZN',amzn_complete)\n",
    "SPY_Deep = deep_learning('SPY',spy_complete)\n",
    "DOCU_Deep = deep_learning('DOCU',docu_complete)\n",
    "NFLX_Deep = deep_learning('NFLX',nflx_complete)\n",
    "NKE_Deep = deep_learning('NKE',nke_complete)\n",
    "PG_Deep = deep_learning('PG',pg_complete)\n",
    "\n",
    "NN_SUMMARY = pd.concat([AAPL_Deep,TSLA_Deep,\n",
    "                        AMZN_Deep,SPY_Deep,\n",
    "                       DOCU_Deep,NFLX_Deep,\n",
    "                       NKE_Deep,PG_Deep]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKER</th>\n",
       "      <th>Neural Net - ACCURACY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.993399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.992556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.997297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPY</td>\n",
       "      <td>0.978723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOCU</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NFLX</td>\n",
       "      <td>0.993952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NKE</td>\n",
       "      <td>0.993031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PG</td>\n",
       "      <td>0.991561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TICKER  Neural Net - ACCURACY\n",
       "0   AAPL               0.993399\n",
       "1   TSLA               0.992556\n",
       "2   AMZN               0.997297\n",
       "3    SPY               0.978723\n",
       "4   DOCU               0.941176\n",
       "5   NFLX               0.993952\n",
       "6    NKE               0.993031\n",
       "7     PG               0.991561"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(tickerName, df):\n",
    "    target = df[\"ACTION\"]\n",
    "    target_names = [\"negative\", \"positive\"]\n",
    "    \n",
    "    test2=df\n",
    "    data = df.drop(\"ACTION\", axis=1).dropna()\n",
    "    feature_names = data.columns\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)\n",
    "    \n",
    "    # Support vector machine linear classifier\n",
    "    from sklearn.svm import SVC \n",
    "    model = SVC(kernel='linear')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('-------------------------------------------------------------------------------------')\n",
    "    # Model Accuracy\n",
    "    print('Test Acc: %.3f' % model.score(X_test, y_test))\n",
    "    print('-------------------------------------')\n",
    "    \n",
    "    # Calculate classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    predictions = model.predict(X_test)\n",
    "    print(classification_report(y_test, predictions,\n",
    "                            target_names=target_names))\n",
    "    \n",
    "    title_sent = {\n",
    "    \"TICKER\": tickerName,\n",
    "    \"SVM - ACCURACY\": model.score(X_test, y_test)\n",
    "    }\n",
    "\n",
    "    df2 = pd.DataFrame(title_sent,index=[0])\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "Test Acc: 0.841\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.96      0.86       201\n",
      "    positive       0.94      0.73      0.82       202\n",
      "\n",
      "    accuracy                           0.84       403\n",
      "   macro avg       0.86      0.84      0.84       403\n",
      "weighted avg       0.86      0.84      0.84       403\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "Test Acc: 0.489\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      1.00      0.66        23\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.24      0.50      0.33        47\n",
      "weighted avg       0.24      0.49      0.32        47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "Test Acc: 0.531\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      1.00      0.69       160\n",
      "    positive       1.00      0.01      0.01       143\n",
      "\n",
      "    accuracy                           0.53       303\n",
      "   macro avg       0.76      0.50      0.35       303\n",
      "weighted avg       0.75      0.53      0.37       303\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "Test Acc: 0.622\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.97      0.74       204\n",
      "    positive       0.84      0.19      0.31       166\n",
      "\n",
      "    accuracy                           0.62       370\n",
      "   macro avg       0.72      0.58      0.53       370\n",
      "weighted avg       0.71      0.62      0.55       370\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "Test Acc: 0.588\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.70      0.67        10\n",
      "    positive       0.50      0.43      0.46         7\n",
      "\n",
      "    accuracy                           0.59        17\n",
      "   macro avg       0.57      0.56      0.56        17\n",
      "weighted avg       0.58      0.59      0.58        17\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "Test Acc: 0.863\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.85      0.86       236\n",
      "    positive       0.87      0.87      0.87       260\n",
      "\n",
      "    accuracy                           0.86       496\n",
      "   macro avg       0.86      0.86      0.86       496\n",
      "weighted avg       0.86      0.86      0.86       496\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "Test Acc: 0.523\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68       148\n",
      "    positive       1.00      0.01      0.03       139\n",
      "\n",
      "    accuracy                           0.52       287\n",
      "   macro avg       0.76      0.51      0.36       287\n",
      "weighted avg       0.75      0.52      0.37       287\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "Test Acc: 0.540\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.69      0.61       127\n",
      "    positive       0.51      0.37      0.43       110\n",
      "\n",
      "    accuracy                           0.54       237\n",
      "   macro avg       0.53      0.53      0.52       237\n",
      "weighted avg       0.53      0.54      0.53       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TSLA_SVM = svm('TSLA',tsla_complete)\n",
    "SPY_SVM = svm('SPY',spy_complete)\n",
    "AAPL_SVM = svm(\"AAPL\",aapl_complete)\n",
    "AMZN_SVM = svm(\"AMZN\",amzn_complete)\n",
    "DOCU_SVM = svm('DOCU',docu_complete)\n",
    "NFLX_SVM = svm('NFLX',nflx_complete)\n",
    "NKE_SVM = svm(\"NKE\",nke_complete)\n",
    "PG_SVM = svm(\"PG\",pg_complete)\n",
    "\n",
    "SVM_SUMMARY = pd.concat([AAPL_SVM,TSLA_SVM,\n",
    "                         AMZN_SVM,SPY_SVM,\n",
    "                        DOCU_SVM,NFLX_SVM,\n",
    "                        NKE_SVM,PG_SVM]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKER</th>\n",
       "      <th>Neural Net - ACCURACY</th>\n",
       "      <th>SVM - ACCURACY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.993399</td>\n",
       "      <td>0.531353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.992556</td>\n",
       "      <td>0.841191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.997297</td>\n",
       "      <td>0.621622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPY</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.489362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOCU</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NFLX</td>\n",
       "      <td>0.993952</td>\n",
       "      <td>0.862903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NKE</td>\n",
       "      <td>0.993031</td>\n",
       "      <td>0.522648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PG</td>\n",
       "      <td>0.991561</td>\n",
       "      <td>0.540084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TICKER  Neural Net - ACCURACY  SVM - ACCURACY\n",
       "0   AAPL               0.993399        0.531353\n",
       "1   TSLA               0.992556        0.841191\n",
       "2   AMZN               0.997297        0.621622\n",
       "3    SPY               0.978723        0.489362\n",
       "4   DOCU               0.941176        0.588235\n",
       "5   NFLX               0.993952        0.862903\n",
       "6    NKE               0.993031        0.522648\n",
       "7     PG               0.991561        0.540084"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_SUMMARY = NN_SUMMARY.merge(SVM_SUMMARY,on='TICKER',how='left')\n",
    "ML_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 8 artists>"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(ML_SUMMARY['TICKER'],ML_SUMMARY['SVM - ACCURACY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 8 artists>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(ML_SUMMARY['TICKER'],ML_SUMMARY['Neural Net - ACCURACY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
