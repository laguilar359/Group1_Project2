{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import tensorflow as tf\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load .env enviroment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv('ALPACA_API_KEY')\n",
    "alpaca_secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "\n",
    "api = tradeapi.REST('PKYSIX5VD8DLHIZOILZS', 'Yv4AYGCNo9puqbXGPq2zF1sNrzy63CWCrWNJnOse', api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET HEADLINE - WEB SCRAPER\n",
    "from splinter import Browser\n",
    "\n",
    "executable_path = {'executable_path': 'chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "def getHeadlines(ticker):\n",
    "    \n",
    "    print(f\"Currently Running {ticker} Ticker!\")\n",
    "    d = {'Headline': [], 'Date': []}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    for x in range(0,25):\n",
    "        print(f\"Loading page number {x}\")\n",
    "        url = f\"https://www.marketwatch.com/investing/stock/{ticker}/moreheadlines?channel=MarketWatch&pageNumber={x}\"\n",
    "        browser.visit(url)\n",
    "        if (browser.is_element_not_present_by_css('h2[class=\"error__header\"]') == False):\n",
    "            print(\"Got To Try a Differnt Link!\")\n",
    "            url = f\"https://www.marketwatch.com/investing/index/{ticker}/moreheadlines?channel=MarketWatch&pageNumber={x}\"\n",
    "            browser.visit(url)\n",
    "        else:\n",
    "            this = 1+1\n",
    "            \n",
    "        if (browser.is_element_not_present_by_css('span[class=\"article__timestamp\"]') == True):\n",
    "            print(f\"OOPS, sorry no more pages! There's only {x} pages for this Ticker!\")\n",
    "            break       \n",
    "        else:\n",
    "            for y in range(0,len(browser.find_by_css('h3[class=\"article__headline\"]'))):\n",
    "                df = df.append({'Headline':browser.find_by_css('h3[class=\"article__headline\"]')[y].text,\n",
    "                                'Date':browser.find_by_css('span[class=\"article__timestamp\"]')[y].text},ignore_index=True)\n",
    "                \n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "    df = df.sort_values(by=['Date'],ascending=True)\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    df.to_csv(f\"NEW_{ticker}_HEADLINES.csv\",header=True,index=False)\n",
    "    print(f\"Done Running {ticker} Ticker! - Total of {len(df)} rows! & {x} pages!\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Running AAPL Ticker!\n",
      "Loading page number 0\n",
      "Loading page number 1\n",
      "Loading page number 2\n",
      "Loading page number 3\n",
      "Loading page number 4\n",
      "Loading page number 5\n",
      "Loading page number 6\n",
      "Loading page number 7\n",
      "Loading page number 8\n",
      "Loading page number 9\n",
      "Loading page number 10\n",
      "Loading page number 11\n",
      "Loading page number 12\n",
      "Loading page number 13\n",
      "Loading page number 14\n",
      "Loading page number 15\n",
      "Loading page number 16\n",
      "Loading page number 17\n",
      "Loading page number 18\n",
      "Loading page number 19\n",
      "Loading page number 20\n",
      "Loading page number 21\n",
      "Loading page number 22\n",
      "Loading page number 23\n",
      "Loading page number 24\n",
      "Loading page number 25\n",
      "Loading page number 26\n",
      "Loading page number 27\n",
      "Loading page number 28\n",
      "Loading page number 29\n",
      "Loading page number 30\n",
      "Loading page number 31\n",
      "Loading page number 32\n",
      "Loading page number 33\n",
      "Loading page number 34\n",
      "Loading page number 35\n",
      "Loading page number 36\n",
      "Loading page number 37\n",
      "Loading page number 38\n",
      "Loading page number 39\n",
      "Loading page number 40\n",
      "Loading page number 41\n",
      "Loading page number 42\n",
      "Loading page number 43\n",
      "Loading page number 44\n",
      "Loading page number 45\n",
      "Loading page number 46\n",
      "Loading page number 47\n",
      "Loading page number 48\n",
      "Loading page number 49\n",
      "Loading page number 50\n",
      "Loading page number 51\n",
      "Loading page number 52\n",
      "Loading page number 53\n",
      "Loading page number 54\n",
      "Loading page number 55\n",
      "Loading page number 56\n",
      "Loading page number 57\n",
      "Loading page number 58\n",
      "Loading page number 59\n",
      "Loading page number 60\n",
      "Loading page number 61\n",
      "Loading page number 62\n",
      "Loading page number 63\n",
      "Loading page number 64\n",
      "Loading page number 65\n",
      "Loading page number 66\n",
      "Loading page number 67\n",
      "Loading page number 68\n",
      "Loading page number 69\n",
      "Loading page number 70\n",
      "Loading page number 71\n",
      "Loading page number 72\n",
      "Loading page number 73\n",
      "Loading page number 74\n",
      "Loading page number 75\n",
      "Loading page number 76\n",
      "Loading page number 77\n",
      "Loading page number 78\n",
      "Loading page number 79\n",
      "Loading page number 80\n",
      "Loading page number 81\n",
      "Loading page number 82\n",
      "Loading page number 83\n",
      "Loading page number 84\n",
      "Loading page number 85\n",
      "Loading page number 86\n",
      "Loading page number 87\n",
      "Loading page number 88\n",
      "Loading page number 89\n",
      "Loading page number 90\n",
      "Loading page number 91\n",
      "Loading page number 92\n",
      "Loading page number 93\n",
      "Loading page number 94\n",
      "Loading page number 95\n",
      "Loading page number 96\n",
      "Loading page number 97\n",
      "Loading page number 98\n",
      "Loading page number 99\n",
      "Loading page number 100\n",
      "Loading page number 101\n",
      "Loading page number 102\n",
      "Loading page number 103\n",
      "Loading page number 104\n",
      "Loading page number 105\n",
      "Loading page number 106\n",
      "Loading page number 107\n",
      "Loading page number 108\n",
      "Loading page number 109\n",
      "Loading page number 110\n",
      "Loading page number 111\n",
      "Loading page number 112\n",
      "Loading page number 113\n",
      "Loading page number 114\n",
      "Loading page number 115\n",
      "Loading page number 116\n",
      "Loading page number 117\n",
      "Loading page number 118\n",
      "Loading page number 119\n",
      "Loading page number 120\n",
      "Loading page number 121\n",
      "Loading page number 122\n",
      "Loading page number 123\n",
      "Loading page number 124\n",
      "Loading page number 125\n",
      "Loading page number 126\n",
      "Loading page number 127\n",
      "Loading page number 128\n",
      "Loading page number 129\n",
      "Loading page number 130\n",
      "Loading page number 131\n",
      "Loading page number 132\n",
      "Loading page number 133\n",
      "Loading page number 134\n",
      "Loading page number 135\n",
      "Loading page number 136\n",
      "Loading page number 137\n",
      "Loading page number 138\n",
      "Loading page number 139\n",
      "Loading page number 140\n",
      "Loading page number 141\n",
      "Loading page number 142\n",
      "Loading page number 143\n",
      "Loading page number 144\n",
      "Loading page number 145\n",
      "Loading page number 146\n",
      "Loading page number 147\n",
      "Loading page number 148\n",
      "Loading page number 149\n",
      "Loading page number 150\n",
      "Loading page number 151\n",
      "Loading page number 152\n",
      "Loading page number 153\n",
      "Loading page number 154\n",
      "Loading page number 155\n",
      "Loading page number 156\n",
      "Loading page number 157\n",
      "Loading page number 158\n",
      "Loading page number 159\n",
      "Loading page number 160\n",
      "Loading page number 161\n",
      "Loading page number 162\n",
      "Loading page number 163\n",
      "Loading page number 164\n",
      "Loading page number 165\n",
      "Loading page number 166\n",
      "Loading page number 167\n",
      "Loading page number 168\n",
      "Loading page number 169\n",
      "Loading page number 170\n",
      "Loading page number 171\n",
      "Loading page number 172\n",
      "Loading page number 173\n",
      "Loading page number 174\n",
      "Loading page number 175\n",
      "Loading page number 176\n",
      "Loading page number 177\n",
      "Loading page number 178\n",
      "Loading page number 179\n",
      "Loading page number 180\n",
      "Loading page number 181\n",
      "Loading page number 182\n",
      "Loading page number 183\n",
      "Loading page number 184\n",
      "Loading page number 185\n",
      "Loading page number 186\n",
      "Loading page number 187\n",
      "Loading page number 188\n",
      "Loading page number 189\n",
      "Loading page number 190\n",
      "Loading page number 191\n",
      "Loading page number 192\n",
      "Loading page number 193\n",
      "Loading page number 194\n",
      "Loading page number 195\n",
      "Loading page number 196\n",
      "Loading page number 197\n",
      "Loading page number 198\n",
      "Loading page number 199\n",
      "Loading page number 200\n",
      "Loading page number 201\n",
      "Loading page number 202\n",
      "Loading page number 203\n",
      "Loading page number 204\n",
      "Loading page number 205\n",
      "Loading page number 206\n",
      "Loading page number 207\n",
      "Loading page number 208\n",
      "Loading page number 209\n",
      "Loading page number 210\n",
      "Loading page number 211\n",
      "Loading page number 212\n",
      "Loading page number 213\n",
      "Loading page number 214\n",
      "Loading page number 215\n",
      "Loading page number 216\n",
      "Loading page number 217\n",
      "Loading page number 218\n",
      "Loading page number 219\n",
      "Loading page number 220\n",
      "Loading page number 221\n",
      "Loading page number 222\n",
      "Loading page number 223\n",
      "Loading page number 224\n",
      "Loading page number 225\n",
      "Loading page number 226\n",
      "Loading page number 227\n",
      "Loading page number 228\n",
      "Loading page number 229\n",
      "Loading page number 230\n",
      "Loading page number 231\n",
      "Loading page number 232\n",
      "Loading page number 233\n",
      "Loading page number 234\n",
      "Loading page number 235\n",
      "Loading page number 236\n",
      "Loading page number 237\n",
      "Loading page number 238\n",
      "Loading page number 239\n",
      "Loading page number 240\n",
      "Loading page number 241\n",
      "Loading page number 242\n",
      "Loading page number 243\n",
      "Loading page number 244\n",
      "Loading page number 245\n",
      "Loading page number 246\n",
      "Loading page number 247\n",
      "Loading page number 248\n",
      "Loading page number 249\n",
      "Loading page number 250\n",
      "Loading page number 251\n",
      "Loading page number 252\n",
      "Loading page number 253\n",
      "Loading page number 254\n",
      "Loading page number 255\n",
      "Loading page number 256\n",
      "Loading page number 257\n",
      "Loading page number 258\n",
      "Loading page number 259\n",
      "Loading page number 260\n",
      "Loading page number 261\n",
      "Loading page number 262\n",
      "Loading page number 263\n",
      "Loading page number 264\n",
      "Loading page number 265\n",
      "Loading page number 266\n",
      "Loading page number 267\n",
      "Loading page number 268\n",
      "Loading page number 269\n",
      "Loading page number 270\n",
      "Loading page number 271\n",
      "Loading page number 272\n",
      "Loading page number 273\n",
      "Loading page number 274\n",
      "Loading page number 275\n",
      "Loading page number 276\n",
      "Loading page number 277\n",
      "Loading page number 278\n",
      "Loading page number 279\n",
      "Loading page number 280\n",
      "Loading page number 281\n",
      "Loading page number 282\n",
      "Loading page number 283\n",
      "Loading page number 284\n",
      "Loading page number 285\n",
      "Loading page number 286\n",
      "Loading page number 287\n",
      "Loading page number 288\n",
      "Loading page number 289\n",
      "Loading page number 290\n",
      "Loading page number 291\n",
      "Loading page number 292\n",
      "Loading page number 293\n",
      "Loading page number 294\n",
      "Loading page number 295\n",
      "Loading page number 296\n",
      "Loading page number 297\n",
      "Loading page number 298\n",
      "Loading page number 299\n",
      "Loading page number 300\n",
      "Loading page number 301\n",
      "Loading page number 302\n",
      "Loading page number 303\n",
      "Loading page number 304\n",
      "Loading page number 305\n",
      "Loading page number 306\n",
      "Loading page number 307\n",
      "Loading page number 308\n",
      "Loading page number 309\n",
      "Loading page number 310\n",
      "Loading page number 311\n",
      "Loading page number 312\n",
      "Loading page number 313\n",
      "Loading page number 314\n",
      "Loading page number 315\n",
      "Loading page number 316\n",
      "Loading page number 317\n",
      "Loading page number 318\n",
      "Loading page number 319\n",
      "Loading page number 320\n",
      "Loading page number 321\n",
      "Loading page number 322\n",
      "Loading page number 323\n",
      "Loading page number 324\n",
      "Loading page number 325\n",
      "Loading page number 326\n",
      "Loading page number 327\n",
      "Loading page number 328\n",
      "Loading page number 329\n",
      "Loading page number 330\n",
      "Loading page number 331\n",
      "Loading page number 332\n",
      "Loading page number 333\n",
      "Loading page number 334\n",
      "Loading page number 335\n",
      "Loading page number 336\n",
      "Loading page number 337\n",
      "Loading page number 338\n",
      "Loading page number 339\n",
      "Loading page number 340\n",
      "Loading page number 341\n",
      "Loading page number 342\n",
      "Loading page number 343\n",
      "Loading page number 344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page number 345\n",
      "Loading page number 346\n",
      "Loading page number 347\n",
      "Loading page number 348\n",
      "Loading page number 349\n",
      "Loading page number 350\n",
      "Loading page number 351\n",
      "Loading page number 352\n",
      "Loading page number 353\n",
      "Loading page number 354\n",
      "Loading page number 355\n",
      "Loading page number 356\n",
      "Loading page number 357\n",
      "Loading page number 358\n",
      "Loading page number 359\n",
      "Loading page number 360\n",
      "Loading page number 361\n",
      "Loading page number 362\n",
      "Loading page number 363\n",
      "Loading page number 364\n",
      "Loading page number 365\n",
      "Loading page number 366\n",
      "Loading page number 367\n",
      "Loading page number 368\n",
      "Loading page number 369\n",
      "Loading page number 370\n",
      "Loading page number 371\n",
      "Loading page number 372\n",
      "Loading page number 373\n",
      "Loading page number 374\n",
      "Loading page number 375\n",
      "Loading page number 376\n",
      "Loading page number 377\n",
      "Loading page number 378\n",
      "Loading page number 379\n",
      "Loading page number 380\n",
      "Loading page number 381\n",
      "Loading page number 382\n",
      "Loading page number 383\n",
      "Loading page number 384\n",
      "Loading page number 385\n",
      "Loading page number 386\n",
      "Loading page number 387\n",
      "Loading page number 388\n",
      "Loading page number 389\n",
      "Loading page number 390\n",
      "Loading page number 391\n",
      "Loading page number 392\n",
      "Loading page number 393\n",
      "Loading page number 394\n",
      "Loading page number 395\n",
      "Loading page number 396\n",
      "Loading page number 397\n",
      "Loading page number 398\n",
      "Loading page number 399\n",
      "Loading page number 400\n",
      "Loading page number 401\n",
      "Loading page number 402\n",
      "Loading page number 403\n",
      "Loading page number 404\n",
      "Loading page number 405\n",
      "Loading page number 406\n",
      "Loading page number 407\n",
      "Loading page number 408\n",
      "Loading page number 409\n",
      "Loading page number 410\n",
      "Loading page number 411\n",
      "Loading page number 412\n",
      "Loading page number 413\n",
      "Loading page number 414\n",
      "Loading page number 415\n",
      "Loading page number 416\n",
      "Loading page number 417\n",
      "Loading page number 418\n",
      "Loading page number 419\n",
      "Loading page number 420\n",
      "Loading page number 421\n",
      "Loading page number 422\n",
      "Loading page number 423\n",
      "Loading page number 424\n",
      "Loading page number 425\n",
      "Loading page number 426\n",
      "Loading page number 427\n",
      "Loading page number 428\n",
      "Loading page number 429\n",
      "Loading page number 430\n",
      "Loading page number 431\n",
      "Loading page number 432\n",
      "Loading page number 433\n",
      "Loading page number 434\n",
      "Loading page number 435\n",
      "Loading page number 436\n",
      "Loading page number 437\n",
      "Loading page number 438\n",
      "Loading page number 439\n",
      "Loading page number 440\n",
      "Loading page number 441\n",
      "Loading page number 442\n",
      "Loading page number 443\n",
      "Loading page number 444\n",
      "Loading page number 445\n",
      "Loading page number 446\n",
      "Loading page number 447\n",
      "Loading page number 448\n",
      "Loading page number 449\n",
      "Loading page number 450\n",
      "Loading page number 451\n",
      "Loading page number 452\n",
      "Loading page number 453\n",
      "Loading page number 454\n",
      "Loading page number 455\n",
      "Loading page number 456\n",
      "Loading page number 457\n",
      "Loading page number 458\n",
      "Loading page number 459\n",
      "Loading page number 460\n",
      "Loading page number 461\n",
      "Loading page number 462\n",
      "Loading page number 463\n",
      "Loading page number 464\n",
      "Loading page number 465\n",
      "Loading page number 466\n",
      "Loading page number 467\n",
      "Loading page number 468\n",
      "Loading page number 469\n",
      "Loading page number 470\n",
      "Loading page number 471\n",
      "Loading page number 472\n",
      "Loading page number 473\n",
      "Loading page number 474\n",
      "Loading page number 475\n",
      "Loading page number 476\n",
      "Loading page number 477\n",
      "Loading page number 478\n",
      "Loading page number 479\n",
      "Loading page number 480\n",
      "Loading page number 481\n",
      "Loading page number 482\n",
      "Loading page number 483\n",
      "Loading page number 484\n",
      "Loading page number 485\n",
      "Loading page number 486\n",
      "Loading page number 487\n",
      "Loading page number 488\n",
      "Loading page number 489\n",
      "Loading page number 490\n",
      "Loading page number 491\n",
      "Loading page number 492\n",
      "Loading page number 493\n",
      "Loading page number 494\n",
      "Loading page number 495\n",
      "Loading page number 496\n",
      "Loading page number 497\n",
      "Loading page number 498\n",
      "Loading page number 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Running AAPL Ticker! - Total of 10000 rows! & 499 pages!\n",
      "Currently Running AMZN Ticker!\n",
      "Loading page number 0\n",
      "Loading page number 1\n",
      "Loading page number 2\n",
      "Loading page number 3\n",
      "Loading page number 4\n",
      "Loading page number 5\n",
      "Loading page number 6\n",
      "Loading page number 7\n",
      "Loading page number 8\n",
      "Loading page number 9\n",
      "Loading page number 10\n",
      "Loading page number 11\n",
      "Loading page number 12\n",
      "Loading page number 13\n",
      "Loading page number 14\n",
      "Loading page number 15\n",
      "Loading page number 16\n",
      "Loading page number 17\n",
      "Loading page number 18\n",
      "Loading page number 19\n",
      "Loading page number 20\n",
      "Loading page number 21\n",
      "Loading page number 22\n",
      "Loading page number 23\n",
      "Loading page number 24\n",
      "Loading page number 25\n",
      "Loading page number 26\n",
      "Loading page number 27\n",
      "Loading page number 28\n",
      "Loading page number 29\n",
      "Loading page number 30\n",
      "Loading page number 31\n",
      "Loading page number 32\n",
      "Loading page number 33\n",
      "Loading page number 34\n",
      "Loading page number 35\n",
      "Loading page number 36\n",
      "Loading page number 37\n",
      "Loading page number 38\n",
      "Loading page number 39\n",
      "Loading page number 40\n",
      "Loading page number 41\n",
      "Loading page number 42\n",
      "Loading page number 43\n",
      "Loading page number 44\n",
      "Loading page number 45\n",
      "Loading page number 46\n",
      "Loading page number 47\n",
      "Loading page number 48\n",
      "Loading page number 49\n",
      "Loading page number 50\n",
      "Loading page number 51\n",
      "Loading page number 52\n",
      "Loading page number 53\n",
      "Loading page number 54\n",
      "Loading page number 55\n",
      "Loading page number 56\n",
      "Loading page number 57\n",
      "Loading page number 58\n",
      "Loading page number 59\n",
      "Loading page number 60\n",
      "Loading page number 61\n",
      "Loading page number 62\n",
      "Loading page number 63\n",
      "Loading page number 64\n",
      "Loading page number 65\n",
      "Loading page number 66\n",
      "Loading page number 67\n",
      "Loading page number 68\n",
      "Loading page number 69\n",
      "Loading page number 70\n",
      "Loading page number 71\n",
      "Loading page number 72\n",
      "Loading page number 73\n",
      "Loading page number 74\n",
      "Loading page number 75\n",
      "Loading page number 76\n",
      "Loading page number 77\n",
      "Loading page number 78\n",
      "Loading page number 79\n",
      "Loading page number 80\n",
      "Loading page number 81\n",
      "Loading page number 82\n",
      "Loading page number 83\n",
      "Loading page number 84\n",
      "Loading page number 85\n",
      "Loading page number 86\n",
      "Loading page number 87\n",
      "Loading page number 88\n",
      "Loading page number 89\n",
      "Loading page number 90\n",
      "Loading page number 91\n",
      "Loading page number 92\n",
      "Loading page number 93\n",
      "Loading page number 94\n",
      "Loading page number 95\n",
      "Loading page number 96\n",
      "Loading page number 97\n",
      "Loading page number 98\n",
      "Loading page number 99\n",
      "Loading page number 100\n",
      "Loading page number 101\n",
      "Loading page number 102\n",
      "Loading page number 103\n",
      "Loading page number 104\n",
      "Loading page number 105\n",
      "Loading page number 106\n",
      "Loading page number 107\n",
      "Loading page number 108\n",
      "Loading page number 109\n",
      "Loading page number 110\n",
      "Loading page number 111\n",
      "Loading page number 112\n",
      "Loading page number 113\n",
      "Loading page number 114\n",
      "Loading page number 115\n",
      "Loading page number 116\n",
      "Loading page number 117\n",
      "Loading page number 118\n",
      "Loading page number 119\n",
      "Loading page number 120\n",
      "Loading page number 121\n",
      "Loading page number 122\n",
      "Loading page number 123\n",
      "Loading page number 124\n",
      "Loading page number 125\n",
      "Loading page number 126\n",
      "Loading page number 127\n",
      "Loading page number 128\n",
      "Loading page number 129\n",
      "Loading page number 130\n",
      "Loading page number 131\n",
      "Loading page number 132\n",
      "Loading page number 133\n",
      "Loading page number 134\n",
      "Loading page number 135\n",
      "Loading page number 136\n",
      "Loading page number 137\n",
      "Loading page number 138\n",
      "Loading page number 139\n",
      "Loading page number 140\n",
      "Loading page number 141\n",
      "Loading page number 142\n",
      "Loading page number 143\n",
      "Loading page number 144\n",
      "Loading page number 145\n",
      "Loading page number 146\n",
      "Loading page number 147\n",
      "Loading page number 148\n",
      "Loading page number 149\n",
      "Loading page number 150\n",
      "Loading page number 151\n",
      "Loading page number 152\n",
      "Loading page number 153\n",
      "Loading page number 154\n",
      "Loading page number 155\n",
      "Loading page number 156\n",
      "Loading page number 157\n",
      "Loading page number 158\n",
      "Loading page number 159\n",
      "Loading page number 160\n",
      "Loading page number 161\n",
      "Loading page number 162\n",
      "Loading page number 163\n",
      "Loading page number 164\n",
      "Loading page number 165\n",
      "Loading page number 166\n",
      "Loading page number 167\n",
      "Loading page number 168\n",
      "Loading page number 169\n",
      "Loading page number 170\n",
      "Loading page number 171\n",
      "Loading page number 172\n",
      "Loading page number 173\n",
      "Loading page number 174\n",
      "Loading page number 175\n",
      "Loading page number 176\n",
      "Loading page number 177\n",
      "Loading page number 178\n",
      "Loading page number 179\n",
      "Loading page number 180\n",
      "Loading page number 181\n",
      "Loading page number 182\n",
      "Loading page number 183\n",
      "Loading page number 184\n",
      "Loading page number 185\n",
      "Loading page number 186\n",
      "Loading page number 187\n",
      "Loading page number 188\n",
      "Loading page number 189\n",
      "Loading page number 190\n",
      "Loading page number 191\n",
      "Loading page number 192\n",
      "Loading page number 193\n",
      "Loading page number 194\n",
      "Loading page number 195\n",
      "Loading page number 196\n",
      "Loading page number 197\n",
      "Loading page number 198\n",
      "Loading page number 199\n",
      "Loading page number 200\n",
      "Loading page number 201\n",
      "Loading page number 202\n",
      "Loading page number 203\n",
      "Loading page number 204\n",
      "Loading page number 205\n",
      "Loading page number 206\n",
      "Loading page number 207\n",
      "Loading page number 208\n",
      "Loading page number 209\n",
      "Loading page number 210\n",
      "Loading page number 211\n",
      "Loading page number 212\n",
      "Loading page number 213\n",
      "Loading page number 214\n",
      "Loading page number 215\n",
      "Loading page number 216\n",
      "Loading page number 217\n",
      "Loading page number 218\n",
      "Loading page number 219\n",
      "Loading page number 220\n",
      "Loading page number 221\n",
      "Loading page number 222\n",
      "Loading page number 223\n",
      "Loading page number 224\n",
      "Loading page number 225\n",
      "Loading page number 226\n",
      "Loading page number 227\n",
      "Loading page number 228\n",
      "Loading page number 229\n",
      "Loading page number 230\n",
      "Loading page number 231\n",
      "Loading page number 232\n",
      "Loading page number 233\n",
      "Loading page number 234\n",
      "Loading page number 235\n",
      "Loading page number 236\n",
      "Loading page number 237\n",
      "Loading page number 238\n",
      "Loading page number 239\n",
      "Loading page number 240\n",
      "Loading page number 241\n",
      "Loading page number 242\n",
      "Loading page number 243\n",
      "Loading page number 244\n",
      "Loading page number 245\n",
      "Loading page number 246\n",
      "Loading page number 247\n",
      "Loading page number 248\n",
      "Loading page number 249\n",
      "Loading page number 250\n",
      "Loading page number 251\n",
      "Loading page number 252\n",
      "Loading page number 253\n",
      "Loading page number 254\n",
      "Loading page number 255\n",
      "Loading page number 256\n",
      "Loading page number 257\n",
      "Loading page number 258\n",
      "Loading page number 259\n",
      "Loading page number 260\n",
      "Loading page number 261\n",
      "Loading page number 262\n",
      "Loading page number 263\n",
      "Loading page number 264\n",
      "Loading page number 265\n",
      "Loading page number 266\n",
      "Loading page number 267\n",
      "Loading page number 268\n",
      "Loading page number 269\n",
      "Loading page number 270\n",
      "Loading page number 271\n",
      "Loading page number 272\n",
      "Loading page number 273\n",
      "Loading page number 274\n",
      "Loading page number 275\n",
      "Loading page number 276\n",
      "Loading page number 277\n",
      "Loading page number 278\n",
      "Loading page number 279\n",
      "Loading page number 280\n",
      "Loading page number 281\n",
      "Loading page number 282\n",
      "Loading page number 283\n",
      "Loading page number 284\n",
      "Loading page number 285\n",
      "Loading page number 286\n",
      "Loading page number 287\n",
      "Loading page number 288\n",
      "Loading page number 289\n",
      "Loading page number 290\n",
      "Loading page number 291\n",
      "Loading page number 292\n",
      "Loading page number 293\n",
      "Loading page number 294\n",
      "Loading page number 295\n",
      "Loading page number 296\n",
      "Loading page number 297\n",
      "Loading page number 298\n",
      "Loading page number 299\n",
      "Loading page number 300\n",
      "Loading page number 301\n",
      "Loading page number 302\n",
      "Loading page number 303\n",
      "Loading page number 304\n",
      "Loading page number 305\n",
      "Loading page number 306\n",
      "Loading page number 307\n",
      "Loading page number 308\n",
      "Loading page number 309\n",
      "Loading page number 310\n",
      "Loading page number 311\n",
      "Loading page number 312\n",
      "Loading page number 313\n",
      "Loading page number 314\n",
      "Loading page number 315\n",
      "Loading page number 316\n",
      "Loading page number 317\n",
      "Loading page number 318\n",
      "Loading page number 319\n",
      "Loading page number 320\n",
      "Loading page number 321\n",
      "Loading page number 322\n",
      "Loading page number 323\n",
      "Loading page number 324\n",
      "Loading page number 325\n",
      "Loading page number 326\n",
      "Loading page number 327\n",
      "Loading page number 328\n",
      "Loading page number 329\n",
      "Loading page number 330\n",
      "Loading page number 331\n",
      "Loading page number 332\n",
      "Loading page number 333\n",
      "Loading page number 334\n",
      "Loading page number 335\n",
      "Loading page number 336\n",
      "Loading page number 337\n",
      "Loading page number 338\n",
      "Loading page number 339\n",
      "Loading page number 340\n",
      "Loading page number 341\n",
      "Loading page number 342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page number 343\n",
      "Loading page number 344\n",
      "Loading page number 345\n",
      "Loading page number 346\n",
      "Loading page number 347\n",
      "Loading page number 348\n",
      "Loading page number 349\n",
      "Loading page number 350\n",
      "Loading page number 351\n",
      "Loading page number 352\n",
      "Loading page number 353\n",
      "Loading page number 354\n",
      "Loading page number 355\n",
      "Loading page number 356\n",
      "Loading page number 357\n",
      "Loading page number 358\n",
      "Loading page number 359\n",
      "Loading page number 360\n",
      "Loading page number 361\n",
      "Loading page number 362\n",
      "Loading page number 363\n",
      "Loading page number 364\n",
      "Loading page number 365\n",
      "Loading page number 366\n",
      "Loading page number 367\n",
      "Loading page number 368\n",
      "Loading page number 369\n",
      "Loading page number 370\n",
      "Loading page number 371\n",
      "Loading page number 372\n",
      "Loading page number 373\n",
      "Loading page number 374\n",
      "Loading page number 375\n",
      "Loading page number 376\n",
      "Loading page number 377\n",
      "Loading page number 378\n",
      "Loading page number 379\n",
      "Loading page number 380\n",
      "Loading page number 381\n",
      "Loading page number 382\n",
      "Loading page number 383\n",
      "Loading page number 384\n",
      "Loading page number 385\n",
      "Loading page number 386\n",
      "Loading page number 387\n",
      "Loading page number 388\n",
      "Loading page number 389\n",
      "Loading page number 390\n",
      "Loading page number 391\n",
      "Loading page number 392\n",
      "Loading page number 393\n",
      "Loading page number 394\n",
      "Loading page number 395\n",
      "Loading page number 396\n",
      "Loading page number 397\n",
      "Loading page number 398\n",
      "Loading page number 399\n",
      "Loading page number 400\n",
      "Loading page number 401\n",
      "Loading page number 402\n",
      "Loading page number 403\n",
      "Loading page number 404\n",
      "Loading page number 405\n",
      "Loading page number 406\n",
      "Loading page number 407\n",
      "Loading page number 408\n",
      "Loading page number 409\n",
      "Loading page number 410\n",
      "Loading page number 411\n",
      "Loading page number 412\n",
      "Loading page number 413\n",
      "Loading page number 414\n",
      "Loading page number 415\n",
      "Loading page number 416\n",
      "Loading page number 417\n",
      "Loading page number 418\n",
      "Loading page number 419\n",
      "Loading page number 420\n",
      "Loading page number 421\n",
      "Loading page number 422\n",
      "Loading page number 423\n",
      "Loading page number 424\n",
      "Loading page number 425\n",
      "Loading page number 426\n",
      "Loading page number 427\n",
      "Loading page number 428\n",
      "Loading page number 429\n",
      "Loading page number 430\n",
      "Loading page number 431\n",
      "Loading page number 432\n",
      "Loading page number 433\n",
      "Loading page number 434\n",
      "Loading page number 435\n",
      "Loading page number 436\n",
      "Loading page number 437\n",
      "Loading page number 438\n",
      "Loading page number 439\n",
      "Loading page number 440\n",
      "Loading page number 441\n",
      "Loading page number 442\n",
      "Loading page number 443\n",
      "Loading page number 444\n",
      "Loading page number 445\n",
      "Loading page number 446\n",
      "Loading page number 447\n",
      "Loading page number 448\n",
      "Loading page number 449\n",
      "Loading page number 450\n",
      "Loading page number 451\n",
      "Loading page number 452\n",
      "Loading page number 453\n",
      "Loading page number 454\n",
      "Loading page number 455\n",
      "Loading page number 456\n",
      "Loading page number 457\n",
      "Loading page number 458\n",
      "Loading page number 459\n",
      "Loading page number 460\n",
      "Loading page number 461\n",
      "Loading page number 462\n",
      "Loading page number 463\n",
      "Loading page number 464\n",
      "Loading page number 465\n",
      "Loading page number 466\n",
      "Loading page number 467\n",
      "Loading page number 468\n",
      "Loading page number 469\n",
      "Loading page number 470\n",
      "Loading page number 471\n",
      "Loading page number 472\n",
      "Loading page number 473\n",
      "Loading page number 474\n",
      "Loading page number 475\n",
      "Loading page number 476\n",
      "Loading page number 477\n",
      "Loading page number 478\n",
      "Loading page number 479\n",
      "Loading page number 480\n",
      "Loading page number 481\n",
      "Loading page number 482\n",
      "Loading page number 483\n",
      "Loading page number 484\n",
      "Loading page number 485\n",
      "Loading page number 486\n",
      "Loading page number 487\n",
      "Loading page number 488\n",
      "Loading page number 489\n",
      "Loading page number 490\n",
      "Loading page number 491\n",
      "Loading page number 492\n",
      "Loading page number 493\n",
      "Loading page number 494\n",
      "Loading page number 495\n",
      "Loading page number 496\n",
      "Loading page number 497\n",
      "Loading page number 498\n",
      "Loading page number 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Running AMZN Ticker! - Total of 9999 rows! & 499 pages!\n",
      "Currently Running TSLA Ticker!\n",
      "Loading page number 0\n",
      "Loading page number 1\n",
      "Loading page number 2\n",
      "Loading page number 3\n",
      "Loading page number 4\n",
      "Loading page number 5\n",
      "Loading page number 6\n",
      "Loading page number 7\n",
      "Loading page number 8\n",
      "Loading page number 9\n",
      "Loading page number 10\n",
      "Loading page number 11\n",
      "Loading page number 12\n",
      "Loading page number 13\n",
      "Loading page number 14\n",
      "Loading page number 15\n",
      "Loading page number 16\n",
      "Loading page number 17\n",
      "Loading page number 18\n",
      "Loading page number 19\n",
      "Loading page number 20\n",
      "Loading page number 21\n",
      "Loading page number 22\n",
      "Loading page number 23\n",
      "Loading page number 24\n",
      "Loading page number 25\n",
      "Loading page number 26\n",
      "Loading page number 27\n",
      "Loading page number 28\n",
      "Loading page number 29\n",
      "Loading page number 30\n",
      "Loading page number 31\n",
      "Loading page number 32\n",
      "Loading page number 33\n",
      "Loading page number 34\n",
      "Loading page number 35\n",
      "Loading page number 36\n",
      "Loading page number 37\n",
      "Loading page number 38\n",
      "Loading page number 39\n",
      "Loading page number 40\n",
      "Loading page number 41\n",
      "Loading page number 42\n",
      "Loading page number 43\n",
      "Loading page number 44\n",
      "Loading page number 45\n",
      "Loading page number 46\n",
      "Loading page number 47\n",
      "Loading page number 48\n",
      "Loading page number 49\n",
      "Loading page number 50\n",
      "Loading page number 51\n",
      "Loading page number 52\n",
      "Loading page number 53\n",
      "Loading page number 54\n",
      "Loading page number 55\n",
      "Loading page number 56\n",
      "Loading page number 57\n",
      "Loading page number 58\n",
      "Loading page number 59\n",
      "Loading page number 60\n",
      "Loading page number 61\n",
      "Loading page number 62\n",
      "Loading page number 63\n",
      "Loading page number 64\n",
      "Loading page number 65\n",
      "Loading page number 66\n",
      "Loading page number 67\n",
      "Loading page number 68\n",
      "Loading page number 69\n",
      "Loading page number 70\n",
      "Loading page number 71\n",
      "Loading page number 72\n",
      "Loading page number 73\n",
      "Loading page number 74\n",
      "Loading page number 75\n",
      "Loading page number 76\n",
      "Loading page number 77\n",
      "Loading page number 78\n",
      "Loading page number 79\n",
      "Loading page number 80\n",
      "Loading page number 81\n",
      "Loading page number 82\n",
      "Loading page number 83\n",
      "Loading page number 84\n",
      "Loading page number 85\n",
      "Loading page number 86\n",
      "Loading page number 87\n",
      "Loading page number 88\n",
      "Loading page number 89\n",
      "Loading page number 90\n",
      "Loading page number 91\n",
      "Loading page number 92\n",
      "Loading page number 93\n",
      "Loading page number 94\n",
      "Loading page number 95\n",
      "Loading page number 96\n",
      "Loading page number 97\n",
      "Loading page number 98\n",
      "Loading page number 99\n",
      "Loading page number 100\n",
      "Loading page number 101\n",
      "Loading page number 102\n",
      "Loading page number 103\n",
      "Loading page number 104\n",
      "Loading page number 105\n",
      "Loading page number 106\n",
      "Loading page number 107\n",
      "Loading page number 108\n",
      "Loading page number 109\n",
      "Loading page number 110\n",
      "Loading page number 111\n",
      "Loading page number 112\n",
      "Loading page number 113\n",
      "Loading page number 114\n",
      "Loading page number 115\n",
      "Loading page number 116\n",
      "Loading page number 117\n",
      "Loading page number 118\n",
      "Loading page number 119\n",
      "Loading page number 120\n",
      "Loading page number 121\n",
      "Loading page number 122\n",
      "Loading page number 123\n",
      "Loading page number 124\n",
      "Loading page number 125\n",
      "Loading page number 126\n",
      "Loading page number 127\n",
      "Loading page number 128\n",
      "Loading page number 129\n",
      "Loading page number 130\n",
      "Loading page number 131\n",
      "Loading page number 132\n",
      "Loading page number 133\n",
      "Loading page number 134\n",
      "Loading page number 135\n",
      "Loading page number 136\n",
      "Loading page number 137\n",
      "Loading page number 138\n",
      "Loading page number 139\n",
      "Loading page number 140\n",
      "Loading page number 141\n",
      "Loading page number 142\n",
      "Loading page number 143\n",
      "Loading page number 144\n",
      "Loading page number 145\n",
      "Loading page number 146\n",
      "Loading page number 147\n",
      "Loading page number 148\n",
      "Loading page number 149\n",
      "Loading page number 150\n",
      "Loading page number 151\n",
      "Loading page number 152\n",
      "Loading page number 153\n",
      "Loading page number 154\n",
      "Loading page number 155\n",
      "Loading page number 156\n",
      "Loading page number 157\n",
      "Loading page number 158\n",
      "Loading page number 159\n",
      "Loading page number 160\n",
      "Loading page number 161\n",
      "Loading page number 162\n",
      "Loading page number 163\n",
      "Loading page number 164\n",
      "Loading page number 165\n",
      "Loading page number 166\n",
      "Loading page number 167\n",
      "Loading page number 168\n",
      "Loading page number 169\n",
      "Loading page number 170\n",
      "Loading page number 171\n",
      "Loading page number 172\n",
      "Loading page number 173\n",
      "Loading page number 174\n",
      "Loading page number 175\n",
      "Loading page number 176\n",
      "Loading page number 177\n",
      "Loading page number 178\n",
      "Loading page number 179\n",
      "Loading page number 180\n",
      "Loading page number 181\n",
      "Loading page number 182\n",
      "Loading page number 183\n",
      "Loading page number 184\n",
      "Loading page number 185\n",
      "Loading page number 186\n",
      "Loading page number 187\n",
      "Loading page number 188\n",
      "Loading page number 189\n",
      "Loading page number 190\n",
      "Loading page number 191\n",
      "Loading page number 192\n",
      "Loading page number 193\n",
      "Loading page number 194\n",
      "Loading page number 195\n",
      "Loading page number 196\n",
      "Loading page number 197\n",
      "Loading page number 198\n",
      "Loading page number 199\n",
      "Loading page number 200\n",
      "Loading page number 201\n",
      "Loading page number 202\n",
      "Loading page number 203\n",
      "Loading page number 204\n",
      "Loading page number 205\n",
      "Loading page number 206\n",
      "Loading page number 207\n",
      "Loading page number 208\n",
      "Loading page number 209\n",
      "Loading page number 210\n",
      "Loading page number 211\n",
      "Loading page number 212\n",
      "Loading page number 213\n",
      "Loading page number 214\n",
      "Loading page number 215\n",
      "Loading page number 216\n",
      "Loading page number 217\n",
      "Loading page number 218\n",
      "Loading page number 219\n",
      "Loading page number 220\n",
      "Loading page number 221\n",
      "Loading page number 222\n",
      "Loading page number 223\n",
      "Loading page number 224\n",
      "Loading page number 225\n",
      "Loading page number 226\n",
      "Loading page number 227\n",
      "Loading page number 228\n",
      "Loading page number 229\n",
      "Loading page number 230\n",
      "Loading page number 231\n",
      "Loading page number 232\n",
      "Loading page number 233\n",
      "Loading page number 234\n",
      "Loading page number 235\n",
      "Loading page number 236\n",
      "Loading page number 237\n",
      "Loading page number 238\n",
      "Loading page number 239\n",
      "Loading page number 240\n",
      "Loading page number 241\n",
      "Loading page number 242\n",
      "Loading page number 243\n",
      "Loading page number 244\n",
      "Loading page number 245\n",
      "Loading page number 246\n",
      "Loading page number 247\n",
      "Loading page number 248\n",
      "Loading page number 249\n",
      "Loading page number 250\n",
      "Loading page number 251\n",
      "Loading page number 252\n",
      "Loading page number 253\n",
      "Loading page number 254\n",
      "Loading page number 255\n",
      "Loading page number 256\n",
      "Loading page number 257\n",
      "Loading page number 258\n",
      "Loading page number 259\n",
      "Loading page number 260\n",
      "Loading page number 261\n",
      "Loading page number 262\n",
      "Loading page number 263\n",
      "Loading page number 264\n",
      "Loading page number 265\n",
      "Loading page number 266\n",
      "Loading page number 267\n",
      "Loading page number 268\n",
      "Loading page number 269\n",
      "Loading page number 270\n",
      "Loading page number 271\n",
      "Loading page number 272\n",
      "Loading page number 273\n",
      "Loading page number 274\n",
      "Loading page number 275\n",
      "Loading page number 276\n",
      "Loading page number 277\n",
      "Loading page number 278\n",
      "Loading page number 279\n",
      "Loading page number 280\n",
      "Loading page number 281\n",
      "Loading page number 282\n",
      "Loading page number 283\n",
      "Loading page number 284\n",
      "Loading page number 285\n",
      "Loading page number 286\n",
      "Loading page number 287\n",
      "Loading page number 288\n",
      "Loading page number 289\n",
      "Loading page number 290\n",
      "Loading page number 291\n",
      "Loading page number 292\n",
      "Loading page number 293\n",
      "Loading page number 294\n",
      "Loading page number 295\n",
      "Loading page number 296\n",
      "Loading page number 297\n",
      "Loading page number 298\n",
      "Loading page number 299\n",
      "Loading page number 300\n",
      "Loading page number 301\n",
      "Loading page number 302\n",
      "Loading page number 303\n",
      "Loading page number 304\n",
      "Loading page number 305\n",
      "Loading page number 306\n",
      "Loading page number 307\n",
      "Loading page number 308\n",
      "Loading page number 309\n",
      "Loading page number 310\n",
      "Loading page number 311\n",
      "Loading page number 312\n",
      "Loading page number 313\n",
      "Loading page number 314\n",
      "Loading page number 315\n",
      "Loading page number 316\n",
      "Loading page number 317\n",
      "Loading page number 318\n",
      "Loading page number 319\n",
      "Loading page number 320\n",
      "Loading page number 321\n",
      "Loading page number 322\n",
      "Loading page number 323\n",
      "Loading page number 324\n",
      "Loading page number 325\n",
      "Loading page number 326\n",
      "Loading page number 327\n",
      "Loading page number 328\n",
      "Loading page number 329\n",
      "Loading page number 330\n",
      "Loading page number 331\n",
      "Loading page number 332\n",
      "Loading page number 333\n",
      "Loading page number 334\n",
      "Loading page number 335\n",
      "Loading page number 336\n",
      "Loading page number 337\n",
      "Loading page number 338\n",
      "Loading page number 339\n",
      "Loading page number 340\n",
      "Loading page number 341\n",
      "Loading page number 342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page number 343\n",
      "Loading page number 344\n",
      "Loading page number 345\n",
      "Loading page number 346\n",
      "Loading page number 347\n",
      "Loading page number 348\n",
      "Loading page number 349\n",
      "Loading page number 350\n",
      "Loading page number 351\n",
      "Loading page number 352\n",
      "OOPS, sorry no more pages! There's only 352 pages for this Ticker!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1206: UnknownTimezoneWarning: tzname ET identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  category=UnknownTimezoneWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Running TSLA Ticker! - Total of 7023 rows! & 352 pages!\n",
      "Currently Running SPY Ticker!\n",
      "Loading page number 0\n",
      "OOPS, sorry no more pages! There's only 0 pages for this Ticker!\n",
      "Done Running SPY Ticker! - Total of 0 rows! & 0 pages!\n"
     ]
    }
   ],
   "source": [
    "#GET HEADLINES\n",
    "aapl_headlines_df = getHeadlines(\"AAPL\")\n",
    "amzn_headlines_df = getHeadlines(\"AMZN\")\n",
    "tsla_headlines_df = getHeadlines(\"TSLA\")\n",
    "spy_headlines_df = getHeadlines(\"SPX\")\n",
    "\n",
    "docu_headlines_df = getHeadlines(\"DOCU\")\n",
    "nflx_headlines_df = getHeadlines(\"NFLX\")\n",
    "nke_headlines_df = getHeadlines(\"NKE\")\n",
    "pg_headlines_df = getHeadlines(\"PG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_headlines_df = spy_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "aapl_headlines_df = aapl_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "amzn_headlines_df = amzn_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "tsla_headlines_df = tsla_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "docu_headlines_df = docu_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "nflx_headlines_df = nflx_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "nke_headlines_df = nke_headlines_df.drop_duplicates().reset_index(drop=True)\n",
    "pg_headlines_df = pg_headlines_df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OPTIONAL - COVID\n",
    "# spy_covid = spy_headlines_df[spy_headlines_df['Headline'].str.contains(\"Coronavirus\") | spy_headlines_df['Headline'].str.contains(\"COVID\")].reset_index(drop=True)\n",
    "# appl_covid = aapl_headlines_df[aapl_headlines_df['Headline'].str.contains(\"Coronavirus\") | aapl_headlines_df['Headline'].str.contains(\"COVID\")].reset_index(drop=True)\n",
    "# amzn_covid = amzn_headlines_df[amzn_headlines_df['Headline'].str.contains(\"Coronavirus\") | amzn_headlines_df['Headline'].str.contains(\"COVID\")].reset_index(drop=True)\n",
    "# tsla_covid = tsla_headlines_df[tsla_headlines_df['Headline'].str.contains(\"Coronavirus\") | tsla_headlines_df['Headline'].str.contains(\"COVID\")].reset_index(drop=True)\n",
    "# docu_covid = docu_headlines_df[docu_headlines_df['Headline'].str.contains(\"Coronavirus\") | docu_headlines_df['Headline'].str.contains(\"COVID\")].reset_index(drop=True)\n",
    "# nflx_covid = nflx_headlines_df[nflx_headlines_df['Headline'].str.contains(\"Coronavirus\") | nflx_headlines_df['Headline'].str.contains(\"COVID\")].reset_index(drop=True)\n",
    "# nke_covid = nke_headlines_df[nke_headlines_df['Headline'].str.contains(\"Coronavirus\") | nke_headlines_df['Headline'].str.contains(\"COVID\")].reset_index(drop=True)\n",
    "# pg_covid = pg_headlines_df[pg_headlines_df['Headline'].str.contains(\"Coronavirus\") | pg_headlines_df['Headline'].str.contains(\"COVID\")].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET STOCKS\n",
    "def stock_info_grab(ticker,ticker_headline_df):\n",
    "    \"\"\"\n",
    "    Takes ticker symbol and returns DataFrame with Date, Close, and Pct Change columns.\n",
    "    \"\"\"\n",
    "    # Set timeframe to '1D'\n",
    "    timeframe = \"1D\"\n",
    "\n",
    "    # Set current date and the date from one month ago using the ISO format\n",
    "    past_date = pd.Timestamp(ticker_headline_df['Date'][0], tz=\"America/New_York\").isoformat()\n",
    "    current_date = pd.Timestamp(ticker_headline_df['Date'][len(ticker_headline_df)-1], tz=\"America/New_York\").isoformat()\n",
    "\n",
    "    df = api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit=None,\n",
    "        start=past_date,\n",
    "        end=current_date,\n",
    "        after=None,\n",
    "        until=None,\n",
    "    ).df\n",
    "    df = df.droplevel(axis=1, level=0)\n",
    "    df.index = df.index.date\n",
    "    df['pct change'] = df['close'].pct_change()\n",
    "    df['predicted pct change'] = df['pct change'].shift(periods=-1)\n",
    "    del df['pct change']\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['open', 'high', 'low', 'volume'])\n",
    "    df = df.rename(columns={'index':'Date'})\n",
    "    df = df.set_index('Date')\n",
    "    df = df.sort_index(ascending=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl_stock_info = stock_info_grab(\"AAPL\",aapl_headlines_df)\n",
    "amzn_stock_info = stock_info_grab(\"AMZN\",amzn_headlines_df)\n",
    "tsla_stock_info = stock_info_grab(\"TSLA\",tsla_headlines_df)\n",
    "spy_stock_info = stock_info_grab(\"SPY\",spy_headlines_df)\n",
    "\n",
    "docu_stock_info = stock_info_grab(\"DOCU\",docu_headlines_df)\n",
    "nflx_stock_info = stock_info_grab(\"NFLX\",nflx_headlines_df)\n",
    "nke_stock_info = stock_info_grab(\"NKE\",nke_headlines_df)\n",
    "pg_stock_info = stock_info_grab(\"PG\",pg_headlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_df(df):\n",
    "    \"\"\"\n",
    "    Takes headlines DataFrame & creates DataFrame with Sentiment columns.\n",
    "    Splits Date & Time, creates Time column and moves Date to Index.\n",
    "    \"\"\"\n",
    "    title_sent = {\n",
    "        \"compound\": [],\n",
    "        \"positive\": [],\n",
    "        \"neutral\": [],\n",
    "        \"negative\": [],\n",
    "        \"sentiment\": [],\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Sentiment scoring with VADER\n",
    "            title_sentiment = analyzer.polarity_scores(row[\"Headline\"])\n",
    "            title_sent[\"compound\"].append(title_sentiment[\"compound\"])\n",
    "            title_sent[\"positive\"].append(title_sentiment[\"pos\"])\n",
    "            title_sent[\"neutral\"].append(title_sentiment[\"neu\"])\n",
    "            title_sent[\"negative\"].append(title_sentiment[\"neg\"])\n",
    "            title_sent[\"sentiment\"].append(get_sentiment(title_sentiment[\"compound\"]))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    title_sent_df = pd.DataFrame(title_sent)\n",
    "    #title_sent_df.head()\n",
    "\n",
    "    headline_sentiment_df = df.join(title_sent_df)\n",
    "    headline_sentiment_df.dropna()\n",
    "    headline_sentiment_df['Date'] = headline_sentiment_df['Date']\n",
    "    headline_sentiment_df['Date'] = headline_sentiment_df['Date']\n",
    "    headline_sentiment_df = headline_sentiment_df.reindex(columns=['Date', 'Headline', 'compound', 'positive', 'neutral', 'negative', 'sentiment'])\n",
    "    headline_sentiment_df['Date'] = pd.to_datetime(headline_sentiment_df['Date'])\n",
    "    headline_sentiment_df = headline_sentiment_df.set_index('Date').sort_index(ascending=False)\n",
    "    \n",
    "    # find average sentiment score by date\n",
    "    headline_scores = headline_sentiment_df.groupby('Date').mean().sort_index(ascending=False)\n",
    "    del headline_scores['sentiment']\n",
    "    \n",
    "    return headline_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_scores = create_sentiment_df(aapl_headlines_df)\n",
    "amzn_scores = create_sentiment_df(amzn_headlines_df)\n",
    "tsla_scores = create_sentiment_df(tsla_headlines_df)\n",
    "spy_scores = create_sentiment_df(spy_headlines_df)\n",
    "\n",
    "docu_scores = create_sentiment_df(docu_headlines_df)\n",
    "nflx_scores = create_sentiment_df(nflx_headlines_df)\n",
    "nke_scores = create_sentiment_df(nke_headlines_df)\n",
    "pg_scores = create_sentiment_df(pg_headlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>predicted pct change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-11-16</th>\n",
       "      <td>362.4300</td>\n",
       "      <td>-0.004939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-13</th>\n",
       "      <td>357.3300</td>\n",
       "      <td>0.014273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-12</th>\n",
       "      <td>353.2800</td>\n",
       "      <td>0.011464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-11</th>\n",
       "      <td>356.6200</td>\n",
       "      <td>-0.009366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-10</th>\n",
       "      <td>354.0700</td>\n",
       "      <td>0.007202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-09</th>\n",
       "      <td>354.5400</td>\n",
       "      <td>-0.001326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-06</th>\n",
       "      <td>350.1900</td>\n",
       "      <td>0.012422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05</th>\n",
       "      <td>350.2100</td>\n",
       "      <td>-0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-04</th>\n",
       "      <td>343.4950</td>\n",
       "      <td>0.019549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>335.9700</td>\n",
       "      <td>0.022398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-02</th>\n",
       "      <td>330.2100</td>\n",
       "      <td>0.017443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-30</th>\n",
       "      <td>326.5300</td>\n",
       "      <td>0.011270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-29</th>\n",
       "      <td>329.9900</td>\n",
       "      <td>-0.010485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-28</th>\n",
       "      <td>326.6700</td>\n",
       "      <td>0.010163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-27</th>\n",
       "      <td>338.2500</td>\n",
       "      <td>-0.034235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-26</th>\n",
       "      <td>339.4150</td>\n",
       "      <td>-0.003432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-23</th>\n",
       "      <td>345.7600</td>\n",
       "      <td>-0.018351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-22</th>\n",
       "      <td>344.6300</td>\n",
       "      <td>0.003279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-21</th>\n",
       "      <td>342.6900</td>\n",
       "      <td>0.005661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20</th>\n",
       "      <td>343.3400</td>\n",
       "      <td>-0.001893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-19</th>\n",
       "      <td>342.0200</td>\n",
       "      <td>0.003859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-16</th>\n",
       "      <td>347.2500</td>\n",
       "      <td>-0.015061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-15</th>\n",
       "      <td>347.5100</td>\n",
       "      <td>-0.000748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-14</th>\n",
       "      <td>347.9700</td>\n",
       "      <td>-0.001322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-13</th>\n",
       "      <td>350.1500</td>\n",
       "      <td>-0.006226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-12</th>\n",
       "      <td>352.4300</td>\n",
       "      <td>-0.006469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-09</th>\n",
       "      <td>346.8400</td>\n",
       "      <td>0.016117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-08</th>\n",
       "      <td>343.7300</td>\n",
       "      <td>0.009048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-07</th>\n",
       "      <td>340.7300</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-06</th>\n",
       "      <td>334.9400</td>\n",
       "      <td>0.017287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-12</th>\n",
       "      <td>274.0750</td>\n",
       "      <td>0.003302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-11</th>\n",
       "      <td>270.6400</td>\n",
       "      <td>0.012692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-08</th>\n",
       "      <td>270.0800</td>\n",
       "      <td>0.002073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-07</th>\n",
       "      <td>269.9700</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-06</th>\n",
       "      <td>272.7000</td>\n",
       "      <td>-0.010011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-05</th>\n",
       "      <td>273.1300</td>\n",
       "      <td>-0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>271.7800</td>\n",
       "      <td>0.004967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>270.0700</td>\n",
       "      <td>0.006332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>270.2325</td>\n",
       "      <td>-0.000601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>267.4625</td>\n",
       "      <td>0.010357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>263.3100</td>\n",
       "      <td>0.015770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-28</th>\n",
       "      <td>263.6350</td>\n",
       "      <td>-0.001233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>265.7000</td>\n",
       "      <td>-0.007772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>263.6900</td>\n",
       "      <td>0.007623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>263.1000</td>\n",
       "      <td>0.002242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>261.9700</td>\n",
       "      <td>0.004313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-18</th>\n",
       "      <td>266.1600</td>\n",
       "      <td>-0.015742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>262.8800</td>\n",
       "      <td>0.012477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-16</th>\n",
       "      <td>260.9400</td>\n",
       "      <td>0.007435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>260.2100</td>\n",
       "      <td>0.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-14</th>\n",
       "      <td>257.6900</td>\n",
       "      <td>0.009779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>258.7600</td>\n",
       "      <td>-0.004135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>258.9400</td>\n",
       "      <td>-0.000695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>257.7100</td>\n",
       "      <td>0.004773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>256.6500</td>\n",
       "      <td>0.004130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>254.4000</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>252.4100</td>\n",
       "      <td>0.007884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>244.4600</td>\n",
       "      <td>0.032521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>249.8900</td>\n",
       "      <td>-0.021730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>248.5500</td>\n",
       "      <td>0.005391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>475 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               close  predicted pct change\n",
       "Date                                      \n",
       "2020-11-16  362.4300             -0.004939\n",
       "2020-11-13  357.3300              0.014273\n",
       "2020-11-12  353.2800              0.011464\n",
       "2020-11-11  356.6200             -0.009366\n",
       "2020-11-10  354.0700              0.007202\n",
       "2020-11-09  354.5400             -0.001326\n",
       "2020-11-06  350.1900              0.012422\n",
       "2020-11-05  350.2100             -0.000057\n",
       "2020-11-04  343.4950              0.019549\n",
       "2020-11-03  335.9700              0.022398\n",
       "2020-11-02  330.2100              0.017443\n",
       "2020-10-30  326.5300              0.011270\n",
       "2020-10-29  329.9900             -0.010485\n",
       "2020-10-28  326.6700              0.010163\n",
       "2020-10-27  338.2500             -0.034235\n",
       "2020-10-26  339.4150             -0.003432\n",
       "2020-10-23  345.7600             -0.018351\n",
       "2020-10-22  344.6300              0.003279\n",
       "2020-10-21  342.6900              0.005661\n",
       "2020-10-20  343.3400             -0.001893\n",
       "2020-10-19  342.0200              0.003859\n",
       "2020-10-16  347.2500             -0.015061\n",
       "2020-10-15  347.5100             -0.000748\n",
       "2020-10-14  347.9700             -0.001322\n",
       "2020-10-13  350.1500             -0.006226\n",
       "2020-10-12  352.4300             -0.006469\n",
       "2020-10-09  346.8400              0.016117\n",
       "2020-10-08  343.7300              0.009048\n",
       "2020-10-07  340.7300              0.008805\n",
       "2020-10-06  334.9400              0.017287\n",
       "...              ...                   ...\n",
       "2019-02-12  274.0750              0.003302\n",
       "2019-02-11  270.6400              0.012692\n",
       "2019-02-08  270.0800              0.002073\n",
       "2019-02-07  269.9700              0.000407\n",
       "2019-02-06  272.7000             -0.010011\n",
       "2019-02-05  273.1300             -0.001574\n",
       "2019-02-04  271.7800              0.004967\n",
       "2019-02-01  270.0700              0.006332\n",
       "2019-01-31  270.2325             -0.000601\n",
       "2019-01-30  267.4625              0.010357\n",
       "2019-01-29  263.3100              0.015770\n",
       "2019-01-28  263.6350             -0.001233\n",
       "2019-01-25  265.7000             -0.007772\n",
       "2019-01-24  263.6900              0.007623\n",
       "2019-01-23  263.1000              0.002242\n",
       "2019-01-22  261.9700              0.004313\n",
       "2019-01-18  266.1600             -0.015742\n",
       "2019-01-17  262.8800              0.012477\n",
       "2019-01-16  260.9400              0.007435\n",
       "2019-01-15  260.2100              0.002805\n",
       "2019-01-14  257.6900              0.009779\n",
       "2019-01-11  258.7600             -0.004135\n",
       "2019-01-10  258.9400             -0.000695\n",
       "2019-01-09  257.7100              0.004773\n",
       "2019-01-08  256.6500              0.004130\n",
       "2019-01-07  254.4000              0.008844\n",
       "2019-01-04  252.4100              0.007884\n",
       "2019-01-03  244.4600              0.032521\n",
       "2019-01-02  249.8900             -0.021730\n",
       "2018-12-31  248.5500              0.005391\n",
       "\n",
       "[475 rows x 2 columns]"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy_stock_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent scores distribution across each df poss use histogram, calc meanstd, or percentiles\n",
    "aapl_complete = pd.concat([aapl_scores,aapl_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "amzn_complete = pd.concat([amzn_scores,amzn_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "tsla_complete = pd.concat([tsla_scores,tsla_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "spy_complete = pd.concat([spy_scores,spy_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "\n",
    "docu_complete = pd.concat([docu_scores,docu_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "nflx_complete = pd.concat([nflx_scores,nflx_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "nke_complete = pd.concat([nke_scores,nke_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)\n",
    "pg_complete = pd.concat([pg_scores,pg_stock_info], join='outer', axis=1).dropna().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>close</th>\n",
       "      <th>predicted pct change</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>ACTION</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-07-02</th>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.8330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>98.47</td>\n",
       "      <td>0.015030</td>\n",
       "      <td>1</td>\n",
       "      <td>ADD MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-29</th>\n",
       "      <td>0.3818</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.8330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>95.91</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>1</td>\n",
       "      <td>ADD MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-26</th>\n",
       "      <td>0.4939</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.8240</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>93.67</td>\n",
       "      <td>0.023914</td>\n",
       "      <td>1</td>\n",
       "      <td>ADD MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>-0.8860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5680</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>87.18</td>\n",
       "      <td>0.014453</td>\n",
       "      <td>-1</td>\n",
       "      <td>ADD MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-02</th>\n",
       "      <td>-0.2249</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.8235</td>\n",
       "      <td>0.1265</td>\n",
       "      <td>80.08</td>\n",
       "      <td>-0.016733</td>\n",
       "      <td>-1</td>\n",
       "      <td>TAKE MONEY OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>82.74</td>\n",
       "      <td>-0.042422</td>\n",
       "      <td>0</td>\n",
       "      <td>TAKE MONEY OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>89.45</td>\n",
       "      <td>0.036221</td>\n",
       "      <td>0</td>\n",
       "      <td>ADD MONEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-26</th>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.8450</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>92.05</td>\n",
       "      <td>-0.038457</td>\n",
       "      <td>-1</td>\n",
       "      <td>TAKE MONEY OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-05</th>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.8590</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.55</td>\n",
       "      <td>-0.002685</td>\n",
       "      <td>1</td>\n",
       "      <td>TAKE MONEY OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-04</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>101.39</td>\n",
       "      <td>-0.008285</td>\n",
       "      <td>0</td>\n",
       "      <td>TAKE MONEY OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-28</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.42</td>\n",
       "      <td>-0.005776</td>\n",
       "      <td>0</td>\n",
       "      <td>TAKE MONEY OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            compound  positive  neutral  negative   close  \\\n",
       "Date                                                        \n",
       "2020-07-02    0.2023     0.167   0.8330    0.0000   98.47   \n",
       "2020-06-29    0.3818     0.167   0.8330    0.0000   95.91   \n",
       "2020-06-26    0.4939     0.176   0.8240    0.0000   93.67   \n",
       "2020-05-05   -0.8860     0.000   0.5680    0.4320   87.18   \n",
       "2020-04-02   -0.2249     0.050   0.8235    0.1265   80.08   \n",
       "2020-03-31    0.0000     0.000   1.0000    0.0000   82.74   \n",
       "2020-02-28    0.0000     0.000   1.0000    0.0000   89.45   \n",
       "2020-02-26   -0.2960     0.000   0.8450    0.1550   92.05   \n",
       "2020-02-05    0.3182     0.141   0.8590    0.0000  100.55   \n",
       "2020-02-04    0.0000     0.000   1.0000    0.0000  101.39   \n",
       "2020-01-28    0.0000     0.000   1.0000    0.0000  100.42   \n",
       "\n",
       "            predicted pct change SCORE          ACTION  \n",
       "Date                                                    \n",
       "2020-07-02              0.015030     1       ADD MONEY  \n",
       "2020-06-29              0.022000     1       ADD MONEY  \n",
       "2020-06-26              0.023914     1       ADD MONEY  \n",
       "2020-05-05              0.014453    -1       ADD MONEY  \n",
       "2020-04-02             -0.016733    -1  TAKE MONEY OUT  \n",
       "2020-03-31             -0.042422     0  TAKE MONEY OUT  \n",
       "2020-02-28              0.036221     0       ADD MONEY  \n",
       "2020-02-26             -0.038457    -1  TAKE MONEY OUT  \n",
       "2020-02-05             -0.002685     1  TAKE MONEY OUT  \n",
       "2020-02-04             -0.008285     0  TAKE MONEY OUT  \n",
       "2020-01-28             -0.005776     0  TAKE MONEY OUT  "
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nke_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_prep(df):\n",
    "    df['SCORE'] = ''\n",
    "    temp = [];\n",
    "    for i in range(round(len(df['compound']))):\n",
    "        start = i\n",
    "        end=(i+1)\n",
    "        temp.append(df['compound'][start:end])\n",
    "\n",
    "    df['ACTION'] = ''\n",
    "    temp2 = [];\n",
    "    for i in range(round(len(df['predicted pct change']))):\n",
    "        start = i\n",
    "        end=(i+1)\n",
    "        temp2.append(df['predicted pct change'][start:end])\n",
    "\n",
    "    for x in range(0,len(temp)):\n",
    "        for integer,row in temp[x].iteritems():\n",
    "            if row >= 0.05:\n",
    "                df.set_value(integer,'SCORE',1)\n",
    "\n",
    "            elif (row <= -0.05):\n",
    "                df.set_value(integer,'SCORE',-1)\n",
    "            else:\n",
    "                df.set_value(integer,'SCORE', 0)\n",
    "\n",
    "    for y in range(0,len(temp2)):\n",
    "        for integer,row in temp2[y].iteritems():\n",
    "            if row >0:\n",
    "                df.set_value(integer,'ACTION','ADD MONEY')\n",
    "\n",
    "            elif (row==0):\n",
    "                df.set_value(integer,'ACTION','HOLD')\n",
    "            else:\n",
    "                df.set_value(integer,'ACTION', 'TAKE MONEY OUT')\n",
    "                \n",
    "    df = df[df['ACTION'] != 'HOLD'].dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "aapl_complete = ml_prep(aapl_complete)\n",
    "tsla_complete = ml_prep(tsla_complete)\n",
    "spy_complete = ml_prep(spy_complete)\n",
    "amzn_complete = ml_prep(pg_complete)\n",
    "docu_complete = ml_prep(docu_complete)\n",
    "nflx_complete = ml_prep(nflx_complete)\n",
    "nke_complete = ml_prep(nke_complete)\n",
    "pg_complete = ml_prep(pg_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_learning(tickerName, df):\n",
    "    X = df.drop(columns=[\"ACTION\",'compound','close','predicted pct change'], axis=1)\n",
    "    y = df[\"ACTION\"]\n",
    "    print(X.shape, y.shape)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, random_state=1, stratify=y)\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    # Step 1: Label-encode data set\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(y_train)\n",
    "    encoded_y_train = label_encoder.transform(y_train)\n",
    "    encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "    # Step 2: Convert encoded labels to one-hot-encoding\n",
    "    y_train_categorical = to_categorical(encoded_y_train)\n",
    "    y_test_categorical = to_categorical(encoded_y_test)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    \n",
    "    activations = ['linear','sigmoid','tanh','relu']\n",
    "\n",
    "    title_sent = {\"TICKER\": tickerName}\n",
    "    \n",
    "    df2 = pd.DataFrame(title_sent,index=[0])\n",
    "\n",
    "    # Create model and add layers\n",
    "    \n",
    "    for activation in activations:\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=100, activation=activation, input_dim=4))\n",
    "        model.add(Dense(units=100, activation=activation))\n",
    "        model.add(Dense(units=100, activation=activation))\n",
    "        model.add(Dense(units=100, activation=activation))\n",
    "        model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "\n",
    "        # Compile and fit the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        model.fit(\n",
    "            X_train_scaled,\n",
    "            y_train_categorical,\n",
    "            epochs=60,\n",
    "            shuffle=True,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        model_loss, model_accuracy = model.evaluate(\n",
    "        X_test_scaled, y_test_categorical, verbose=2)\n",
    "\n",
    "        print(\"----------------------------------------------------------------------------------------\")\n",
    "        print(\n",
    "            f\"{tickerName}: Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "        print(\"----------------------------------------------------------------------------------------\")\n",
    "\n",
    "        encoded_predictions = model.predict_classes(X_test_scaled[:5])\n",
    "        prediction_labels = label_encoder.inverse_transform(encoded_predictions)\n",
    "        print(\"     \")\n",
    "        print(\"######### -----  PREDICTION TIME!!!!! ------ ######### \")\n",
    "        print(\"     \")\n",
    "\n",
    "        print(f\"Predicted classes: {prediction_labels}\")\n",
    "        print(f\"Actual Labels: {list(y_test[:5])}\")\n",
    "        print(\"     \")\n",
    "        print(\"----------------------------------------------------------------------------------------\")\n",
    "        result = []\n",
    "        result.append(model_accuracy)\n",
    "        df2[f\"NN - {activation}\"] = result\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1211, 4) (1211,)\n",
      "Epoch 1/60\n",
      "29/29 - 0s - loss: 0.7264 - accuracy: 0.4956\n",
      "Epoch 2/60\n",
      "29/29 - 0s - loss: 0.7005 - accuracy: 0.5077\n",
      "Epoch 3/60\n",
      "29/29 - 0s - loss: 0.6956 - accuracy: 0.5363\n",
      "Epoch 4/60\n",
      "29/29 - 0s - loss: 0.6969 - accuracy: 0.5374\n",
      "Epoch 5/60\n",
      "29/29 - 0s - loss: 0.6956 - accuracy: 0.5209\n",
      "Epoch 6/60\n",
      "29/29 - 0s - loss: 0.6926 - accuracy: 0.5396\n",
      "Epoch 7/60\n",
      "29/29 - 0s - loss: 0.6905 - accuracy: 0.5275\n",
      "Epoch 8/60\n",
      "29/29 - 0s - loss: 0.6945 - accuracy: 0.5396\n",
      "Epoch 9/60\n",
      "29/29 - 0s - loss: 0.6908 - accuracy: 0.5352\n",
      "Epoch 10/60\n",
      "29/29 - 0s - loss: 0.6908 - accuracy: 0.5341\n",
      "Epoch 11/60\n",
      "29/29 - 0s - loss: 0.6903 - accuracy: 0.5407\n",
      "Epoch 12/60\n",
      "29/29 - 0s - loss: 0.6907 - accuracy: 0.5308\n",
      "Epoch 13/60\n",
      "29/29 - 0s - loss: 0.6902 - accuracy: 0.5584\n",
      "Epoch 14/60\n",
      "29/29 - 0s - loss: 0.6914 - accuracy: 0.5441\n",
      "Epoch 15/60\n",
      "29/29 - 0s - loss: 0.6901 - accuracy: 0.5419\n",
      "Epoch 16/60\n",
      "29/29 - 0s - loss: 0.6900 - accuracy: 0.5430\n",
      "Epoch 17/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5430\n",
      "Epoch 18/60\n",
      "29/29 - 0s - loss: 0.6907 - accuracy: 0.5407\n",
      "Epoch 19/60\n",
      "29/29 - 0s - loss: 0.6897 - accuracy: 0.5441\n",
      "Epoch 20/60\n",
      "29/29 - 0s - loss: 0.6904 - accuracy: 0.5573\n",
      "Epoch 21/60\n",
      "29/29 - 0s - loss: 0.6898 - accuracy: 0.5595\n",
      "Epoch 22/60\n",
      "29/29 - 0s - loss: 0.6888 - accuracy: 0.5485\n",
      "Epoch 23/60\n",
      "29/29 - 0s - loss: 0.6905 - accuracy: 0.5540\n",
      "Epoch 24/60\n",
      "29/29 - 0s - loss: 0.6891 - accuracy: 0.5430\n",
      "Epoch 25/60\n",
      "29/29 - 0s - loss: 0.6904 - accuracy: 0.5099\n",
      "Epoch 26/60\n",
      "29/29 - 0s - loss: 0.6973 - accuracy: 0.5264\n",
      "Epoch 27/60\n",
      "29/29 - 0s - loss: 0.6913 - accuracy: 0.5385\n",
      "Epoch 28/60\n",
      "29/29 - 0s - loss: 0.6908 - accuracy: 0.5286\n",
      "Epoch 29/60\n",
      "29/29 - 0s - loss: 0.6906 - accuracy: 0.5385\n",
      "Epoch 30/60\n",
      "29/29 - 0s - loss: 0.6893 - accuracy: 0.5374\n",
      "Epoch 31/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5551\n",
      "Epoch 32/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5430\n",
      "Epoch 33/60\n",
      "29/29 - 0s - loss: 0.6892 - accuracy: 0.5441\n",
      "Epoch 34/60\n",
      "29/29 - 0s - loss: 0.6892 - accuracy: 0.5430\n",
      "Epoch 35/60\n",
      "29/29 - 0s - loss: 0.6905 - accuracy: 0.5330\n",
      "Epoch 36/60\n",
      "29/29 - 0s - loss: 0.6926 - accuracy: 0.5474\n",
      "Epoch 37/60\n",
      "29/29 - 0s - loss: 0.6898 - accuracy: 0.5441\n",
      "Epoch 38/60\n",
      "29/29 - 0s - loss: 0.6908 - accuracy: 0.5474\n",
      "Epoch 39/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5430\n",
      "Epoch 40/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5463\n",
      "Epoch 41/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5341\n",
      "Epoch 42/60\n",
      "29/29 - 0s - loss: 0.6892 - accuracy: 0.5430\n",
      "Epoch 43/60\n",
      "29/29 - 0s - loss: 0.6888 - accuracy: 0.5407\n",
      "Epoch 44/60\n",
      "29/29 - 0s - loss: 0.6903 - accuracy: 0.5419\n",
      "Epoch 45/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5430\n",
      "Epoch 46/60\n",
      "29/29 - 0s - loss: 0.6893 - accuracy: 0.5474\n",
      "Epoch 47/60\n",
      "29/29 - 0s - loss: 0.6928 - accuracy: 0.5452\n",
      "Epoch 48/60\n",
      "29/29 - 0s - loss: 0.6908 - accuracy: 0.5374\n",
      "Epoch 49/60\n",
      "29/29 - 0s - loss: 0.6889 - accuracy: 0.5430\n",
      "Epoch 50/60\n",
      "29/29 - 0s - loss: 0.6887 - accuracy: 0.5419\n",
      "Epoch 51/60\n",
      "29/29 - 0s - loss: 0.6893 - accuracy: 0.5430\n",
      "Epoch 52/60\n",
      "29/29 - 0s - loss: 0.6891 - accuracy: 0.5419\n",
      "Epoch 53/60\n",
      "29/29 - 0s - loss: 0.6892 - accuracy: 0.5430\n",
      "Epoch 54/60\n",
      "29/29 - 0s - loss: 0.6891 - accuracy: 0.5430\n",
      "Epoch 55/60\n",
      "29/29 - 0s - loss: 0.6888 - accuracy: 0.5463\n",
      "Epoch 56/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5474\n",
      "Epoch 57/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5430\n",
      "Epoch 58/60\n",
      "29/29 - 0s - loss: 0.6902 - accuracy: 0.5407\n",
      "Epoch 59/60\n",
      "29/29 - 0s - loss: 0.6900 - accuracy: 0.5496\n",
      "Epoch 60/60\n",
      "29/29 - 0s - loss: 0.6896 - accuracy: 0.5430\n",
      "10/10 - 0s - loss: 0.6943 - accuracy: 0.5413\n",
      "----------------------------------------------------------------------------------------\n",
      "AAPL: Normal Neural Network - Loss: 0.6942809820175171, Accuracy: 0.5412541031837463\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1C392C80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "29/29 - 0s - loss: 0.7009 - accuracy: 0.5275\n",
      "Epoch 2/60\n",
      "29/29 - 0s - loss: 0.6960 - accuracy: 0.5187\n",
      "Epoch 3/60\n",
      "29/29 - 0s - loss: 0.6952 - accuracy: 0.5253\n",
      "Epoch 4/60\n",
      "29/29 - 0s - loss: 0.6913 - accuracy: 0.5430\n",
      "Epoch 5/60\n",
      "29/29 - 0s - loss: 0.6930 - accuracy: 0.5253\n",
      "Epoch 6/60\n",
      "29/29 - 0s - loss: 0.6964 - accuracy: 0.5121\n",
      "Epoch 7/60\n",
      "29/29 - 0s - loss: 0.6945 - accuracy: 0.5154\n",
      "Epoch 8/60\n",
      "29/29 - 0s - loss: 0.6984 - accuracy: 0.5165\n",
      "Epoch 9/60\n",
      "29/29 - 0s - loss: 0.6926 - accuracy: 0.5231\n",
      "Epoch 10/60\n",
      "29/29 - 0s - loss: 0.6935 - accuracy: 0.5154\n",
      "Epoch 11/60\n",
      "29/29 - 0s - loss: 0.6940 - accuracy: 0.5143\n",
      "Epoch 12/60\n",
      "29/29 - 0s - loss: 0.6909 - accuracy: 0.5253\n",
      "Epoch 13/60\n",
      "29/29 - 0s - loss: 0.6932 - accuracy: 0.5419\n",
      "Epoch 14/60\n",
      "29/29 - 0s - loss: 0.6908 - accuracy: 0.5419\n",
      "Epoch 15/60\n",
      "29/29 - 0s - loss: 0.6932 - accuracy: 0.5474\n",
      "Epoch 16/60\n",
      "29/29 - 0s - loss: 0.6928 - accuracy: 0.5308\n",
      "Epoch 17/60\n",
      "29/29 - 0s - loss: 0.6909 - accuracy: 0.5198\n",
      "Epoch 18/60\n",
      "29/29 - 0s - loss: 0.6919 - accuracy: 0.5176\n",
      "Epoch 19/60\n",
      "29/29 - 0s - loss: 0.6922 - accuracy: 0.5143\n",
      "Epoch 20/60\n",
      "29/29 - 0s - loss: 0.6945 - accuracy: 0.5275\n",
      "Epoch 21/60\n",
      "29/29 - 0s - loss: 0.6909 - accuracy: 0.5154\n",
      "Epoch 22/60\n",
      "29/29 - 0s - loss: 0.6905 - accuracy: 0.5430\n",
      "Epoch 23/60\n",
      "29/29 - 0s - loss: 0.6917 - accuracy: 0.5176\n",
      "Epoch 24/60\n",
      "29/29 - 0s - loss: 0.6931 - accuracy: 0.5430\n",
      "Epoch 25/60\n",
      "29/29 - 0s - loss: 0.6973 - accuracy: 0.5143\n",
      "Epoch 26/60\n",
      "29/29 - 0s - loss: 0.6900 - accuracy: 0.5430\n",
      "Epoch 27/60\n",
      "29/29 - 0s - loss: 0.6913 - accuracy: 0.5297\n",
      "Epoch 28/60\n",
      "29/29 - 0s - loss: 0.6921 - accuracy: 0.5441\n",
      "Epoch 29/60\n",
      "29/29 - 0s - loss: 0.6921 - accuracy: 0.5374\n",
      "Epoch 30/60\n",
      "29/29 - 0s - loss: 0.6910 - accuracy: 0.5209\n",
      "Epoch 31/60\n",
      "29/29 - 0s - loss: 0.6931 - accuracy: 0.5242\n",
      "Epoch 32/60\n",
      "29/29 - 0s - loss: 0.6892 - accuracy: 0.5463\n",
      "Epoch 33/60\n",
      "29/29 - 0s - loss: 0.6919 - accuracy: 0.5441\n",
      "Epoch 34/60\n",
      "29/29 - 0s - loss: 0.6905 - accuracy: 0.5407\n",
      "Epoch 35/60\n",
      "29/29 - 0s - loss: 0.6903 - accuracy: 0.5419\n",
      "Epoch 36/60\n",
      "29/29 - 0s - loss: 0.6897 - accuracy: 0.5407\n",
      "Epoch 37/60\n",
      "29/29 - 0s - loss: 0.6896 - accuracy: 0.5463\n",
      "Epoch 38/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5463\n",
      "Epoch 39/60\n",
      "29/29 - 0s - loss: 0.6883 - accuracy: 0.5485\n",
      "Epoch 40/60\n",
      "29/29 - 0s - loss: 0.6952 - accuracy: 0.5033\n",
      "Epoch 41/60\n",
      "29/29 - 0s - loss: 0.6915 - accuracy: 0.5419\n",
      "Epoch 42/60\n",
      "29/29 - 0s - loss: 0.6890 - accuracy: 0.5396\n",
      "Epoch 43/60\n",
      "29/29 - 0s - loss: 0.6938 - accuracy: 0.5242\n",
      "Epoch 44/60\n",
      "29/29 - 0s - loss: 0.6907 - accuracy: 0.5374\n",
      "Epoch 45/60\n",
      "29/29 - 0s - loss: 0.6901 - accuracy: 0.5430\n",
      "Epoch 46/60\n",
      "29/29 - 0s - loss: 0.6888 - accuracy: 0.5507\n",
      "Epoch 47/60\n",
      "29/29 - 0s - loss: 0.6915 - accuracy: 0.5198\n",
      "Epoch 48/60\n",
      "29/29 - 0s - loss: 0.6938 - accuracy: 0.5242\n",
      "Epoch 49/60\n",
      "29/29 - 0s - loss: 0.6927 - accuracy: 0.5507\n",
      "Epoch 50/60\n",
      "29/29 - 0s - loss: 0.6926 - accuracy: 0.5385\n",
      "Epoch 51/60\n",
      "29/29 - 0s - loss: 0.6886 - accuracy: 0.5496\n",
      "Epoch 52/60\n",
      "29/29 - 0s - loss: 0.6884 - accuracy: 0.5396\n",
      "Epoch 53/60\n",
      "29/29 - 0s - loss: 0.6952 - accuracy: 0.5088\n",
      "Epoch 54/60\n",
      "29/29 - 0s - loss: 0.6898 - accuracy: 0.5308\n",
      "Epoch 55/60\n",
      "29/29 - 0s - loss: 0.6929 - accuracy: 0.5275\n",
      "Epoch 56/60\n",
      "29/29 - 0s - loss: 0.6986 - accuracy: 0.5055\n",
      "Epoch 57/60\n",
      "29/29 - 0s - loss: 0.6924 - accuracy: 0.5088\n",
      "Epoch 58/60\n",
      "29/29 - 0s - loss: 0.6899 - accuracy: 0.5286\n",
      "Epoch 59/60\n",
      "29/29 - 0s - loss: 0.6900 - accuracy: 0.5264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60\n",
      "29/29 - 0s - loss: 0.6906 - accuracy: 0.5463\n",
      "10/10 - 0s - loss: 0.6945 - accuracy: 0.5215\n",
      "----------------------------------------------------------------------------------------\n",
      "AAPL: Normal Neural Network - Loss: 0.6944563984870911, Accuracy: 0.5214521288871765\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1D4A57B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'TAKE MONEY OUT']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "29/29 - 0s - loss: 0.7319 - accuracy: 0.5066\n",
      "Epoch 2/60\n",
      "29/29 - 0s - loss: 0.6931 - accuracy: 0.5066\n",
      "Epoch 3/60\n",
      "29/29 - 0s - loss: 0.6993 - accuracy: 0.5154\n",
      "Epoch 4/60\n",
      "29/29 - 0s - loss: 0.6918 - accuracy: 0.5441\n",
      "Epoch 5/60\n",
      "29/29 - 0s - loss: 0.6938 - accuracy: 0.5297\n",
      "Epoch 6/60\n",
      "29/29 - 0s - loss: 0.6922 - accuracy: 0.5176\n",
      "Epoch 7/60\n",
      "29/29 - 0s - loss: 0.6931 - accuracy: 0.5198\n",
      "Epoch 8/60\n",
      "29/29 - 0s - loss: 0.6958 - accuracy: 0.5286\n",
      "Epoch 9/60\n",
      "29/29 - 0s - loss: 0.6923 - accuracy: 0.5341\n",
      "Epoch 10/60\n",
      "29/29 - 0s - loss: 0.6909 - accuracy: 0.5419\n",
      "Epoch 11/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5385\n",
      "Epoch 12/60\n",
      "29/29 - 0s - loss: 0.6906 - accuracy: 0.5430\n",
      "Epoch 13/60\n",
      "29/29 - 0s - loss: 0.6903 - accuracy: 0.5385\n",
      "Epoch 14/60\n",
      "29/29 - 0s - loss: 0.6903 - accuracy: 0.5562\n",
      "Epoch 15/60\n",
      "29/29 - 0s - loss: 0.6898 - accuracy: 0.5430\n",
      "Epoch 16/60\n",
      "29/29 - 0s - loss: 0.6904 - accuracy: 0.5396\n",
      "Epoch 17/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5529\n",
      "Epoch 18/60\n",
      "29/29 - 0s - loss: 0.6902 - accuracy: 0.5419\n",
      "Epoch 19/60\n",
      "29/29 - 0s - loss: 0.6904 - accuracy: 0.5352\n",
      "Epoch 20/60\n",
      "29/29 - 0s - loss: 0.6893 - accuracy: 0.5474\n",
      "Epoch 21/60\n",
      "29/29 - 0s - loss: 0.6904 - accuracy: 0.5441\n",
      "Epoch 22/60\n",
      "29/29 - 0s - loss: 0.6906 - accuracy: 0.5385\n",
      "Epoch 23/60\n",
      "29/29 - 0s - loss: 0.6907 - accuracy: 0.5319\n",
      "Epoch 24/60\n",
      "29/29 - 0s - loss: 0.6909 - accuracy: 0.5452\n",
      "Epoch 25/60\n",
      "29/29 - 0s - loss: 0.6901 - accuracy: 0.5452\n",
      "Epoch 26/60\n",
      "29/29 - 0s - loss: 0.6898 - accuracy: 0.5407\n",
      "Epoch 27/60\n",
      "29/29 - 0s - loss: 0.6892 - accuracy: 0.5441\n",
      "Epoch 28/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5430\n",
      "Epoch 29/60\n",
      "29/29 - 0s - loss: 0.6906 - accuracy: 0.5319\n",
      "Epoch 30/60\n",
      "29/29 - 0s - loss: 0.6897 - accuracy: 0.5430\n",
      "Epoch 31/60\n",
      "29/29 - 0s - loss: 0.6893 - accuracy: 0.5485\n",
      "Epoch 32/60\n",
      "29/29 - 0s - loss: 0.6941 - accuracy: 0.5363\n",
      "Epoch 33/60\n",
      "29/29 - 0s - loss: 0.6890 - accuracy: 0.5463\n",
      "Epoch 34/60\n",
      "29/29 - 0s - loss: 0.6914 - accuracy: 0.5463\n",
      "Epoch 35/60\n",
      "29/29 - 0s - loss: 0.6893 - accuracy: 0.5496\n",
      "Epoch 36/60\n",
      "29/29 - 0s - loss: 0.6896 - accuracy: 0.5441\n",
      "Epoch 37/60\n",
      "29/29 - 0s - loss: 0.6896 - accuracy: 0.5396\n",
      "Epoch 38/60\n",
      "29/29 - 0s - loss: 0.6907 - accuracy: 0.5330\n",
      "Epoch 39/60\n",
      "29/29 - 0s - loss: 0.6915 - accuracy: 0.5275\n",
      "Epoch 40/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5419\n",
      "Epoch 41/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5396\n",
      "Epoch 42/60\n",
      "29/29 - 0s - loss: 0.6899 - accuracy: 0.5452\n",
      "Epoch 43/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5452\n",
      "Epoch 44/60\n",
      "29/29 - 0s - loss: 0.6907 - accuracy: 0.5452\n",
      "Epoch 45/60\n",
      "29/29 - 0s - loss: 0.6909 - accuracy: 0.5396\n",
      "Epoch 46/60\n",
      "29/29 - 0s - loss: 0.6906 - accuracy: 0.5407\n",
      "Epoch 47/60\n",
      "29/29 - 0s - loss: 0.6896 - accuracy: 0.5352\n",
      "Epoch 48/60\n",
      "29/29 - 0s - loss: 0.6891 - accuracy: 0.5430\n",
      "Epoch 49/60\n",
      "29/29 - 0s - loss: 0.6908 - accuracy: 0.5441\n",
      "Epoch 50/60\n",
      "29/29 - 0s - loss: 0.6896 - accuracy: 0.5430\n",
      "Epoch 51/60\n",
      "29/29 - 0s - loss: 0.6906 - accuracy: 0.5308\n",
      "Epoch 52/60\n",
      "29/29 - 0s - loss: 0.6911 - accuracy: 0.5396\n",
      "Epoch 53/60\n",
      "29/29 - 0s - loss: 0.6893 - accuracy: 0.5430\n",
      "Epoch 54/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5430\n",
      "Epoch 55/60\n",
      "29/29 - 0s - loss: 0.6898 - accuracy: 0.5430\n",
      "Epoch 56/60\n",
      "29/29 - 0s - loss: 0.6895 - accuracy: 0.5419\n",
      "Epoch 57/60\n",
      "29/29 - 0s - loss: 0.6890 - accuracy: 0.5507\n",
      "Epoch 58/60\n",
      "29/29 - 0s - loss: 0.6890 - accuracy: 0.5452\n",
      "Epoch 59/60\n",
      "29/29 - 0s - loss: 0.6887 - accuracy: 0.5430\n",
      "Epoch 60/60\n",
      "29/29 - 0s - loss: 0.6894 - accuracy: 0.5485\n",
      "10/10 - 0s - loss: 0.6935 - accuracy: 0.5413\n",
      "----------------------------------------------------------------------------------------\n",
      "AAPL: Normal Neural Network - Loss: 0.6935070157051086, Accuracy: 0.5412541031837463\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1E815EA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "29/29 - 0s - loss: 0.6944 - accuracy: 0.5143\n",
      "Epoch 2/60\n",
      "29/29 - 0s - loss: 0.6896 - accuracy: 0.5474\n",
      "Epoch 3/60\n",
      "29/29 - 0s - loss: 0.6904 - accuracy: 0.5231\n",
      "Epoch 4/60\n",
      "29/29 - 0s - loss: 0.6904 - accuracy: 0.5463\n",
      "Epoch 5/60\n",
      "29/29 - 0s - loss: 0.6885 - accuracy: 0.5463\n",
      "Epoch 6/60\n",
      "29/29 - 0s - loss: 0.6879 - accuracy: 0.5562\n",
      "Epoch 7/60\n",
      "29/29 - 0s - loss: 0.6870 - accuracy: 0.5430\n",
      "Epoch 8/60\n",
      "29/29 - 0s - loss: 0.6868 - accuracy: 0.5374\n",
      "Epoch 9/60\n",
      "29/29 - 0s - loss: 0.6855 - accuracy: 0.5518\n",
      "Epoch 10/60\n",
      "29/29 - 0s - loss: 0.6856 - accuracy: 0.5396\n",
      "Epoch 11/60\n",
      "29/29 - 0s - loss: 0.6860 - accuracy: 0.5529\n",
      "Epoch 12/60\n",
      "29/29 - 0s - loss: 0.6848 - accuracy: 0.5573\n",
      "Epoch 13/60\n",
      "29/29 - 0s - loss: 0.6834 - accuracy: 0.5529\n",
      "Epoch 14/60\n",
      "29/29 - 0s - loss: 0.6836 - accuracy: 0.5595\n",
      "Epoch 15/60\n",
      "29/29 - 0s - loss: 0.6799 - accuracy: 0.5518\n",
      "Epoch 16/60\n",
      "29/29 - 0s - loss: 0.6814 - accuracy: 0.5584\n",
      "Epoch 17/60\n",
      "29/29 - 0s - loss: 0.6802 - accuracy: 0.5595\n",
      "Epoch 18/60\n",
      "29/29 - 0s - loss: 0.6845 - accuracy: 0.5507\n",
      "Epoch 19/60\n",
      "29/29 - 0s - loss: 0.6809 - accuracy: 0.5716\n",
      "Epoch 20/60\n",
      "29/29 - 0s - loss: 0.6814 - accuracy: 0.5672\n",
      "Epoch 21/60\n",
      "29/29 - 0s - loss: 0.6776 - accuracy: 0.5727\n",
      "Epoch 22/60\n",
      "29/29 - 0s - loss: 0.6778 - accuracy: 0.5661\n",
      "Epoch 23/60\n",
      "29/29 - 0s - loss: 0.6786 - accuracy: 0.5705\n",
      "Epoch 24/60\n",
      "29/29 - 0s - loss: 0.6755 - accuracy: 0.5694\n",
      "Epoch 25/60\n",
      "29/29 - 0s - loss: 0.6780 - accuracy: 0.5562\n",
      "Epoch 26/60\n",
      "29/29 - 0s - loss: 0.6762 - accuracy: 0.5705\n",
      "Epoch 27/60\n",
      "29/29 - 0s - loss: 0.6736 - accuracy: 0.5749\n",
      "Epoch 28/60\n",
      "29/29 - 0s - loss: 0.6759 - accuracy: 0.5727\n",
      "Epoch 29/60\n",
      "29/29 - 0s - loss: 0.6734 - accuracy: 0.5859\n",
      "Epoch 30/60\n",
      "29/29 - 0s - loss: 0.6707 - accuracy: 0.5815\n",
      "Epoch 31/60\n",
      "29/29 - 0s - loss: 0.6705 - accuracy: 0.5793\n",
      "Epoch 32/60\n",
      "29/29 - 0s - loss: 0.6702 - accuracy: 0.5881\n",
      "Epoch 33/60\n",
      "29/29 - 0s - loss: 0.6714 - accuracy: 0.5859\n",
      "Epoch 34/60\n",
      "29/29 - 0s - loss: 0.6690 - accuracy: 0.6068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/60\n",
      "29/29 - 0s - loss: 0.6677 - accuracy: 0.5870\n",
      "Epoch 36/60\n",
      "29/29 - 0s - loss: 0.6697 - accuracy: 0.5925\n",
      "Epoch 37/60\n",
      "29/29 - 0s - loss: 0.6750 - accuracy: 0.5771\n",
      "Epoch 38/60\n",
      "29/29 - 0s - loss: 0.6700 - accuracy: 0.5892\n",
      "Epoch 39/60\n",
      "29/29 - 0s - loss: 0.6740 - accuracy: 0.5837\n",
      "Epoch 40/60\n",
      "29/29 - 0s - loss: 0.6684 - accuracy: 0.5892\n",
      "Epoch 41/60\n",
      "29/29 - 0s - loss: 0.6698 - accuracy: 0.5859\n",
      "Epoch 42/60\n",
      "29/29 - 0s - loss: 0.6672 - accuracy: 0.5815\n",
      "Epoch 43/60\n",
      "29/29 - 0s - loss: 0.6626 - accuracy: 0.5914\n",
      "Epoch 44/60\n",
      "29/29 - 0s - loss: 0.6618 - accuracy: 0.6013\n",
      "Epoch 45/60\n",
      "29/29 - 0s - loss: 0.6615 - accuracy: 0.5980\n",
      "Epoch 46/60\n",
      "29/29 - 0s - loss: 0.6592 - accuracy: 0.6101\n",
      "Epoch 47/60\n",
      "29/29 - 0s - loss: 0.6609 - accuracy: 0.6090\n",
      "Epoch 48/60\n",
      "29/29 - 0s - loss: 0.6557 - accuracy: 0.5991\n",
      "Epoch 49/60\n",
      "29/29 - 0s - loss: 0.6601 - accuracy: 0.6024\n",
      "Epoch 50/60\n",
      "29/29 - 0s - loss: 0.6608 - accuracy: 0.5980\n",
      "Epoch 51/60\n",
      "29/29 - 0s - loss: 0.6573 - accuracy: 0.6024\n",
      "Epoch 52/60\n",
      "29/29 - 0s - loss: 0.6544 - accuracy: 0.6090\n",
      "Epoch 53/60\n",
      "29/29 - 0s - loss: 0.6517 - accuracy: 0.5969\n",
      "Epoch 54/60\n",
      "29/29 - 0s - loss: 0.6539 - accuracy: 0.5936\n",
      "Epoch 55/60\n",
      "29/29 - 0s - loss: 0.6527 - accuracy: 0.5969\n",
      "Epoch 56/60\n",
      "29/29 - 0s - loss: 0.6502 - accuracy: 0.6024\n",
      "Epoch 57/60\n",
      "29/29 - 0s - loss: 0.6468 - accuracy: 0.6145\n",
      "Epoch 58/60\n",
      "29/29 - 0s - loss: 0.6485 - accuracy: 0.6178\n",
      "Epoch 59/60\n",
      "29/29 - 0s - loss: 0.6465 - accuracy: 0.6101\n",
      "Epoch 60/60\n",
      "29/29 - 0s - loss: 0.6458 - accuracy: 0.6046\n",
      "WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "10/10 - 0s - loss: 0.7253 - accuracy: 0.4785\n",
      "----------------------------------------------------------------------------------------\n",
      "AAPL: Normal Neural Network - Loss: 0.7253327369689941, Accuracy: 0.4785478413105011\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1E91BE18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(1612, 4) (1612,)\n",
      "Epoch 1/60\n",
      "38/38 - 0s - loss: 0.7047 - accuracy: 0.5136\n",
      "Epoch 2/60\n",
      "38/38 - 0s - loss: 0.7017 - accuracy: 0.5186\n",
      "Epoch 3/60\n",
      "38/38 - 0s - loss: 0.6997 - accuracy: 0.4797\n",
      "Epoch 4/60\n",
      "38/38 - 0s - loss: 0.6951 - accuracy: 0.5004\n",
      "Epoch 5/60\n",
      "38/38 - 0s - loss: 0.6962 - accuracy: 0.4946\n",
      "Epoch 6/60\n",
      "38/38 - 0s - loss: 0.6955 - accuracy: 0.5087\n",
      "Epoch 7/60\n",
      "38/38 - 0s - loss: 0.6965 - accuracy: 0.5004\n",
      "Epoch 8/60\n",
      "38/38 - 0s - loss: 0.6951 - accuracy: 0.5103\n",
      "Epoch 9/60\n",
      "38/38 - 0s - loss: 0.6957 - accuracy: 0.4888\n",
      "Epoch 10/60\n",
      "38/38 - 0s - loss: 0.6948 - accuracy: 0.4971\n",
      "Epoch 11/60\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.5087\n",
      "Epoch 12/60\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.4963\n",
      "Epoch 13/60\n",
      "38/38 - 0s - loss: 0.6968 - accuracy: 0.4979\n",
      "Epoch 14/60\n",
      "38/38 - 0s - loss: 0.6970 - accuracy: 0.4921\n",
      "Epoch 15/60\n",
      "38/38 - 0s - loss: 0.6947 - accuracy: 0.4988\n",
      "Epoch 16/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.5029\n",
      "Epoch 17/60\n",
      "38/38 - 0s - loss: 0.6936 - accuracy: 0.5103\n",
      "Epoch 18/60\n",
      "38/38 - 0s - loss: 0.6933 - accuracy: 0.5070\n",
      "Epoch 19/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.4830\n",
      "Epoch 20/60\n",
      "38/38 - 0s - loss: 0.6937 - accuracy: 0.4971\n",
      "Epoch 21/60\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.4921\n",
      "Epoch 22/60\n",
      "38/38 - 0s - loss: 0.6992 - accuracy: 0.4930\n",
      "Epoch 23/60\n",
      "38/38 - 0s - loss: 0.6972 - accuracy: 0.4938\n",
      "Epoch 24/60\n",
      "38/38 - 0s - loss: 0.6933 - accuracy: 0.5095\n",
      "Epoch 25/60\n",
      "38/38 - 0s - loss: 0.6946 - accuracy: 0.5021\n",
      "Epoch 26/60\n",
      "38/38 - 0s - loss: 0.6948 - accuracy: 0.5012\n",
      "Epoch 27/60\n",
      "38/38 - 0s - loss: 0.6948 - accuracy: 0.5004\n",
      "Epoch 28/60\n",
      "38/38 - 0s - loss: 0.6940 - accuracy: 0.5012\n",
      "Epoch 29/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.4921\n",
      "Epoch 30/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.5128\n",
      "Epoch 31/60\n",
      "38/38 - 0s - loss: 0.6932 - accuracy: 0.5128\n",
      "Epoch 32/60\n",
      "38/38 - 0s - loss: 0.6933 - accuracy: 0.5128\n",
      "Epoch 33/60\n",
      "38/38 - 0s - loss: 0.6936 - accuracy: 0.4988\n",
      "Epoch 34/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.5079\n",
      "Epoch 35/60\n",
      "38/38 - 0s - loss: 0.6960 - accuracy: 0.5087\n",
      "Epoch 36/60\n",
      "38/38 - 0s - loss: 0.6943 - accuracy: 0.5021\n",
      "Epoch 37/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.5095\n",
      "Epoch 38/60\n",
      "38/38 - 0s - loss: 0.6943 - accuracy: 0.4979\n",
      "Epoch 39/60\n",
      "38/38 - 0s - loss: 0.6930 - accuracy: 0.5037\n",
      "Epoch 40/60\n",
      "38/38 - 0s - loss: 0.6941 - accuracy: 0.4913\n",
      "Epoch 41/60\n",
      "38/38 - 0s - loss: 0.6942 - accuracy: 0.5021\n",
      "Epoch 42/60\n",
      "38/38 - 0s - loss: 0.6939 - accuracy: 0.5095\n",
      "Epoch 43/60\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.4971\n",
      "Epoch 44/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.5112\n",
      "Epoch 45/60\n",
      "38/38 - 0s - loss: 0.6940 - accuracy: 0.5079\n",
      "Epoch 46/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.5045\n",
      "Epoch 47/60\n",
      "38/38 - 0s - loss: 0.6936 - accuracy: 0.4930\n",
      "Epoch 48/60\n",
      "38/38 - 0s - loss: 0.6931 - accuracy: 0.5128\n",
      "Epoch 49/60\n",
      "38/38 - 0s - loss: 0.6931 - accuracy: 0.5128\n",
      "Epoch 50/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.5128\n",
      "Epoch 51/60\n",
      "38/38 - 0s - loss: 0.6934 - accuracy: 0.5070\n",
      "Epoch 52/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.5045\n",
      "Epoch 53/60\n",
      "38/38 - 0s - loss: 0.6946 - accuracy: 0.4979\n",
      "Epoch 54/60\n",
      "38/38 - 0s - loss: 0.6952 - accuracy: 0.4988\n",
      "Epoch 55/60\n",
      "38/38 - 0s - loss: 0.6954 - accuracy: 0.4938\n",
      "Epoch 56/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.5128\n",
      "Epoch 57/60\n",
      "38/38 - 0s - loss: 0.6934 - accuracy: 0.5128\n",
      "Epoch 58/60\n",
      "38/38 - 0s - loss: 0.6937 - accuracy: 0.5087\n",
      "Epoch 59/60\n",
      "38/38 - 0s - loss: 0.6936 - accuracy: 0.5145\n",
      "Epoch 60/60\n",
      "38/38 - 0s - loss: 0.6947 - accuracy: 0.5045\n",
      "13/13 - 0s - loss: 0.6916 - accuracy: 0.5385\n",
      "----------------------------------------------------------------------------------------\n",
      "TSLA: Normal Neural Network - Loss: 0.6915753483772278, Accuracy: 0.5384615659713745\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1E815F28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT' 'TAKE MONEY OUT'\n",
      " 'ADD MONEY']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "38/38 - 0s - loss: 0.6981 - accuracy: 0.4872\n",
      "Epoch 2/60\n",
      "38/38 - 0s - loss: 0.6969 - accuracy: 0.4888\n",
      "Epoch 3/60\n",
      "38/38 - 0s - loss: 0.6960 - accuracy: 0.4971\n",
      "Epoch 4/60\n",
      "38/38 - 0s - loss: 0.6965 - accuracy: 0.4830\n",
      "Epoch 5/60\n",
      "38/38 - 0s - loss: 0.6974 - accuracy: 0.4996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/60\n",
      "38/38 - 0s - loss: 0.6948 - accuracy: 0.5054\n",
      "Epoch 7/60\n",
      "38/38 - 0s - loss: 0.6937 - accuracy: 0.5194\n",
      "Epoch 8/60\n",
      "38/38 - 0s - loss: 0.6994 - accuracy: 0.4988\n",
      "Epoch 9/60\n",
      "38/38 - 0s - loss: 0.6974 - accuracy: 0.4806\n",
      "Epoch 10/60\n",
      "38/38 - 0s - loss: 0.6981 - accuracy: 0.4864\n",
      "Epoch 11/60\n",
      "38/38 - 0s - loss: 0.6963 - accuracy: 0.5211\n",
      "Epoch 12/60\n",
      "38/38 - 0s - loss: 0.6950 - accuracy: 0.4930\n",
      "Epoch 13/60\n",
      "38/38 - 0s - loss: 0.6925 - accuracy: 0.5285\n",
      "Epoch 14/60\n",
      "38/38 - 0s - loss: 0.6976 - accuracy: 0.5004\n",
      "Epoch 15/60\n",
      "38/38 - 0s - loss: 0.6963 - accuracy: 0.4930\n",
      "Epoch 16/60\n",
      "38/38 - 0s - loss: 0.6975 - accuracy: 0.4955\n",
      "Epoch 17/60\n",
      "38/38 - 0s - loss: 0.6962 - accuracy: 0.5128\n",
      "Epoch 18/60\n",
      "38/38 - 0s - loss: 0.6960 - accuracy: 0.4839\n",
      "Epoch 19/60\n",
      "38/38 - 0s - loss: 0.6950 - accuracy: 0.4996\n",
      "Epoch 20/60\n",
      "38/38 - 0s - loss: 0.6960 - accuracy: 0.4855\n",
      "Epoch 21/60\n",
      "38/38 - 0s - loss: 0.6964 - accuracy: 0.5004\n",
      "Epoch 22/60\n",
      "38/38 - 0s - loss: 0.6960 - accuracy: 0.4822\n",
      "Epoch 23/60\n",
      "38/38 - 0s - loss: 0.6953 - accuracy: 0.4996\n",
      "Epoch 24/60\n",
      "38/38 - 0s - loss: 0.6944 - accuracy: 0.5045\n",
      "Epoch 25/60\n",
      "38/38 - 0s - loss: 0.6964 - accuracy: 0.5037\n",
      "Epoch 26/60\n",
      "38/38 - 0s - loss: 0.6981 - accuracy: 0.4773\n",
      "Epoch 27/60\n",
      "38/38 - 0s - loss: 0.6975 - accuracy: 0.4979\n",
      "Epoch 28/60\n",
      "38/38 - 0s - loss: 0.6971 - accuracy: 0.5079\n",
      "Epoch 29/60\n",
      "38/38 - 0s - loss: 0.6957 - accuracy: 0.5004\n",
      "Epoch 30/60\n",
      "38/38 - 0s - loss: 0.6952 - accuracy: 0.5236\n",
      "Epoch 31/60\n",
      "38/38 - 0s - loss: 0.6967 - accuracy: 0.4913\n",
      "Epoch 32/60\n",
      "38/38 - 0s - loss: 0.6950 - accuracy: 0.5021\n",
      "Epoch 33/60\n",
      "38/38 - 0s - loss: 0.6965 - accuracy: 0.4880\n",
      "Epoch 34/60\n",
      "38/38 - 0s - loss: 0.6963 - accuracy: 0.4905\n",
      "Epoch 35/60\n",
      "38/38 - 0s - loss: 0.6949 - accuracy: 0.4946\n",
      "Epoch 36/60\n",
      "38/38 - 0s - loss: 0.6947 - accuracy: 0.4880\n",
      "Epoch 37/60\n",
      "38/38 - 0s - loss: 0.6953 - accuracy: 0.4938\n",
      "Epoch 38/60\n",
      "38/38 - 0s - loss: 0.6947 - accuracy: 0.4921\n",
      "Epoch 39/60\n",
      "38/38 - 0s - loss: 0.6943 - accuracy: 0.4979\n",
      "Epoch 40/60\n",
      "38/38 - 0s - loss: 0.6942 - accuracy: 0.5012\n",
      "Epoch 41/60\n",
      "38/38 - 0s - loss: 0.6955 - accuracy: 0.4996\n",
      "Epoch 42/60\n",
      "38/38 - 0s - loss: 0.6952 - accuracy: 0.5186\n",
      "Epoch 43/60\n",
      "38/38 - 0s - loss: 0.6969 - accuracy: 0.4905\n",
      "Epoch 44/60\n",
      "38/38 - 0s - loss: 0.6939 - accuracy: 0.5021\n",
      "Epoch 45/60\n",
      "38/38 - 0s - loss: 0.6950 - accuracy: 0.4905\n",
      "Epoch 46/60\n",
      "38/38 - 0s - loss: 0.6953 - accuracy: 0.5012\n",
      "Epoch 47/60\n",
      "38/38 - 0s - loss: 0.6958 - accuracy: 0.4955\n",
      "Epoch 48/60\n",
      "38/38 - 0s - loss: 0.6951 - accuracy: 0.5037\n",
      "Epoch 49/60\n",
      "38/38 - 0s - loss: 0.6973 - accuracy: 0.4855\n",
      "Epoch 50/60\n",
      "38/38 - 0s - loss: 0.6962 - accuracy: 0.5029\n",
      "Epoch 51/60\n",
      "38/38 - 0s - loss: 0.6957 - accuracy: 0.4963\n",
      "Epoch 52/60\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.5037\n",
      "Epoch 53/60\n",
      "38/38 - 0s - loss: 0.6950 - accuracy: 0.4930\n",
      "Epoch 54/60\n",
      "38/38 - 0s - loss: 0.6921 - accuracy: 0.5368\n",
      "Epoch 55/60\n",
      "38/38 - 0s - loss: 0.7019 - accuracy: 0.4955\n",
      "Epoch 56/60\n",
      "38/38 - 0s - loss: 0.6942 - accuracy: 0.4888\n",
      "Epoch 57/60\n",
      "38/38 - 0s - loss: 0.6926 - accuracy: 0.5045\n",
      "Epoch 58/60\n",
      "38/38 - 0s - loss: 0.6965 - accuracy: 0.5029\n",
      "Epoch 59/60\n",
      "38/38 - 0s - loss: 0.6943 - accuracy: 0.5045\n",
      "Epoch 60/60\n",
      "38/38 - 0s - loss: 0.6947 - accuracy: 0.5087\n",
      "13/13 - 0s - loss: 0.6943 - accuracy: 0.4888\n",
      "----------------------------------------------------------------------------------------\n",
      "TSLA: Normal Neural Network - Loss: 0.6942630410194397, Accuracy: 0.4888337552547455\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C17424048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT'\n",
      " 'TAKE MONEY OUT']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "38/38 - 0s - loss: 0.7082 - accuracy: 0.4913\n",
      "Epoch 2/60\n",
      "38/38 - 0s - loss: 0.6975 - accuracy: 0.4955\n",
      "Epoch 3/60\n",
      "38/38 - 0s - loss: 0.6998 - accuracy: 0.5103\n",
      "Epoch 4/60\n",
      "38/38 - 0s - loss: 0.6957 - accuracy: 0.4913\n",
      "Epoch 5/60\n",
      "38/38 - 0s - loss: 0.6957 - accuracy: 0.4938\n",
      "Epoch 6/60\n",
      "38/38 - 0s - loss: 0.6976 - accuracy: 0.5087\n",
      "Epoch 7/60\n",
      "38/38 - 0s - loss: 0.6956 - accuracy: 0.4905\n",
      "Epoch 8/60\n",
      "38/38 - 0s - loss: 0.6964 - accuracy: 0.4988\n",
      "Epoch 9/60\n",
      "38/38 - 0s - loss: 0.6963 - accuracy: 0.5161\n",
      "Epoch 10/60\n",
      "38/38 - 0s - loss: 0.6960 - accuracy: 0.5103\n",
      "Epoch 11/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.4864\n",
      "Epoch 12/60\n",
      "38/38 - 0s - loss: 0.6968 - accuracy: 0.4930\n",
      "Epoch 13/60\n",
      "38/38 - 0s - loss: 0.6973 - accuracy: 0.5219\n",
      "Epoch 14/60\n",
      "38/38 - 0s - loss: 0.6968 - accuracy: 0.5004\n",
      "Epoch 15/60\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.4979\n",
      "Epoch 16/60\n",
      "38/38 - 0s - loss: 0.6936 - accuracy: 0.5087\n",
      "Epoch 17/60\n",
      "38/38 - 0s - loss: 0.6939 - accuracy: 0.5070\n",
      "Epoch 18/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.5045\n",
      "Epoch 19/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.5153\n",
      "Epoch 20/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.4988\n",
      "Epoch 21/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.5153\n",
      "Epoch 22/60\n",
      "38/38 - 0s - loss: 0.6944 - accuracy: 0.4822\n",
      "Epoch 23/60\n",
      "38/38 - 0s - loss: 0.6942 - accuracy: 0.5145\n",
      "Epoch 24/60\n",
      "38/38 - 0s - loss: 0.6940 - accuracy: 0.5021\n",
      "Epoch 25/60\n",
      "38/38 - 0s - loss: 0.6937 - accuracy: 0.5236\n",
      "Epoch 26/60\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.4963\n",
      "Epoch 27/60\n",
      "38/38 - 0s - loss: 0.6947 - accuracy: 0.5029\n",
      "Epoch 28/60\n",
      "38/38 - 0s - loss: 0.6940 - accuracy: 0.5070\n",
      "Epoch 29/60\n",
      "38/38 - 0s - loss: 0.6947 - accuracy: 0.4839\n",
      "Epoch 30/60\n",
      "38/38 - 0s - loss: 0.6940 - accuracy: 0.4955\n",
      "Epoch 31/60\n",
      "38/38 - 0s - loss: 0.6933 - accuracy: 0.5128\n",
      "Epoch 32/60\n",
      "38/38 - 0s - loss: 0.6932 - accuracy: 0.5062\n",
      "Epoch 33/60\n",
      "38/38 - 0s - loss: 0.6934 - accuracy: 0.5079\n",
      "Epoch 34/60\n",
      "38/38 - 0s - loss: 0.6931 - accuracy: 0.5128\n",
      "Epoch 35/60\n",
      "38/38 - 0s - loss: 0.6934 - accuracy: 0.5103\n",
      "Epoch 36/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.5120\n",
      "Epoch 37/60\n",
      "38/38 - 0s - loss: 0.6936 - accuracy: 0.5136\n",
      "Epoch 38/60\n",
      "38/38 - 0s - loss: 0.6933 - accuracy: 0.5095\n",
      "Epoch 39/60\n",
      "38/38 - 0s - loss: 0.6933 - accuracy: 0.5103\n",
      "Epoch 40/60\n",
      "38/38 - 0s - loss: 0.6934 - accuracy: 0.5136\n",
      "Epoch 41/60\n",
      "38/38 - 0s - loss: 0.6945 - accuracy: 0.4921\n",
      "Epoch 42/60\n",
      "38/38 - 0s - loss: 0.6963 - accuracy: 0.5021\n",
      "Epoch 43/60\n",
      "38/38 - 0s - loss: 0.6942 - accuracy: 0.5062\n",
      "Epoch 44/60\n",
      "38/38 - 0s - loss: 0.6943 - accuracy: 0.4921\n",
      "Epoch 45/60\n",
      "38/38 - 0s - loss: 0.6938 - accuracy: 0.4971\n",
      "Epoch 46/60\n",
      "38/38 - 0s - loss: 0.6946 - accuracy: 0.5095\n",
      "Epoch 47/60\n",
      "38/38 - 0s - loss: 0.6948 - accuracy: 0.5004\n",
      "Epoch 48/60\n",
      "38/38 - 0s - loss: 0.6933 - accuracy: 0.5103\n",
      "Epoch 49/60\n",
      "38/38 - 0s - loss: 0.6929 - accuracy: 0.5203\n",
      "Epoch 50/60\n",
      "38/38 - 0s - loss: 0.6928 - accuracy: 0.5128\n",
      "Epoch 51/60\n",
      "38/38 - 0s - loss: 0.6928 - accuracy: 0.5269\n",
      "Epoch 52/60\n",
      "38/38 - 0s - loss: 0.6933 - accuracy: 0.4971\n",
      "Epoch 53/60\n",
      "38/38 - 0s - loss: 0.6946 - accuracy: 0.4996\n",
      "Epoch 54/60\n",
      "38/38 - 0s - loss: 0.6949 - accuracy: 0.5136\n",
      "Epoch 55/60\n",
      "38/38 - 0s - loss: 0.6935 - accuracy: 0.5070\n",
      "Epoch 56/60\n",
      "38/38 - 0s - loss: 0.6937 - accuracy: 0.5079\n",
      "Epoch 57/60\n",
      "38/38 - 0s - loss: 0.6936 - accuracy: 0.5045\n",
      "Epoch 58/60\n",
      "38/38 - 0s - loss: 0.6934 - accuracy: 0.5087\n",
      "Epoch 59/60\n",
      "38/38 - 0s - loss: 0.6932 - accuracy: 0.5136\n",
      "Epoch 60/60\n",
      "38/38 - 0s - loss: 0.6931 - accuracy: 0.4971\n",
      "13/13 - 0s - loss: 0.6959 - accuracy: 0.5012\n",
      "----------------------------------------------------------------------------------------\n",
      "TSLA: Normal Neural Network - Loss: 0.6958532333374023, Accuracy: 0.5012406706809998\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C174CD950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "38/38 - 0s - loss: 0.6982 - accuracy: 0.5112\n",
      "Epoch 2/60\n",
      "38/38 - 0s - loss: 0.6942 - accuracy: 0.5079\n",
      "Epoch 3/60\n",
      "38/38 - 0s - loss: 0.6927 - accuracy: 0.5211\n",
      "Epoch 4/60\n",
      "38/38 - 0s - loss: 0.6921 - accuracy: 0.5128\n",
      "Epoch 5/60\n",
      "38/38 - 0s - loss: 0.6943 - accuracy: 0.5128\n",
      "Epoch 6/60\n",
      "38/38 - 0s - loss: 0.6947 - accuracy: 0.5128\n",
      "Epoch 7/60\n",
      "38/38 - 0s - loss: 0.6917 - accuracy: 0.5269\n",
      "Epoch 8/60\n",
      "38/38 - 0s - loss: 0.6906 - accuracy: 0.5302\n",
      "Epoch 9/60\n",
      "38/38 - 0s - loss: 0.6914 - accuracy: 0.5186\n",
      "Epoch 10/60\n",
      "38/38 - 0s - loss: 0.6893 - accuracy: 0.5327\n",
      "Epoch 11/60\n",
      "38/38 - 0s - loss: 0.6901 - accuracy: 0.5360\n",
      "Epoch 12/60\n",
      "38/38 - 0s - loss: 0.6916 - accuracy: 0.5161\n",
      "Epoch 13/60\n",
      "38/38 - 0s - loss: 0.6899 - accuracy: 0.5360\n",
      "Epoch 14/60\n",
      "38/38 - 0s - loss: 0.6893 - accuracy: 0.5426\n",
      "Epoch 15/60\n",
      "38/38 - 0s - loss: 0.6890 - accuracy: 0.5360\n",
      "Epoch 16/60\n",
      "38/38 - 0s - loss: 0.6884 - accuracy: 0.5500\n",
      "Epoch 17/60\n",
      "38/38 - 0s - loss: 0.6913 - accuracy: 0.5310\n",
      "Epoch 18/60\n",
      "38/38 - 0s - loss: 0.6883 - accuracy: 0.5451\n",
      "Epoch 19/60\n",
      "38/38 - 0s - loss: 0.6873 - accuracy: 0.5393\n",
      "Epoch 20/60\n",
      "38/38 - 0s - loss: 0.6884 - accuracy: 0.5401\n",
      "Epoch 21/60\n",
      "38/38 - 0s - loss: 0.6881 - accuracy: 0.5368\n",
      "Epoch 22/60\n",
      "38/38 - 0s - loss: 0.6879 - accuracy: 0.5443\n",
      "Epoch 23/60\n",
      "38/38 - 0s - loss: 0.6872 - accuracy: 0.5376\n",
      "Epoch 24/60\n",
      "38/38 - 0s - loss: 0.6864 - accuracy: 0.5426\n",
      "Epoch 25/60\n",
      "38/38 - 0s - loss: 0.6863 - accuracy: 0.5459\n",
      "Epoch 26/60\n",
      "38/38 - 0s - loss: 0.6845 - accuracy: 0.5608\n",
      "Epoch 27/60\n",
      "38/38 - 0s - loss: 0.6852 - accuracy: 0.5533\n",
      "Epoch 28/60\n",
      "38/38 - 0s - loss: 0.6840 - accuracy: 0.5600\n",
      "Epoch 29/60\n",
      "38/38 - 0s - loss: 0.6868 - accuracy: 0.5434\n",
      "Epoch 30/60\n",
      "38/38 - 0s - loss: 0.6860 - accuracy: 0.5467\n",
      "Epoch 31/60\n",
      "38/38 - 0s - loss: 0.6847 - accuracy: 0.5600\n",
      "Epoch 32/60\n",
      "38/38 - 0s - loss: 0.6844 - accuracy: 0.5343\n",
      "Epoch 33/60\n",
      "38/38 - 0s - loss: 0.6836 - accuracy: 0.5459\n",
      "Epoch 34/60\n",
      "38/38 - 0s - loss: 0.6803 - accuracy: 0.5691\n",
      "Epoch 35/60\n",
      "38/38 - 0s - loss: 0.6804 - accuracy: 0.5616\n",
      "Epoch 36/60\n",
      "38/38 - 0s - loss: 0.6811 - accuracy: 0.5558\n",
      "Epoch 37/60\n",
      "38/38 - 0s - loss: 0.6784 - accuracy: 0.5691\n",
      "Epoch 38/60\n",
      "38/38 - 0s - loss: 0.6790 - accuracy: 0.5666\n",
      "Epoch 39/60\n",
      "38/38 - 0s - loss: 0.6795 - accuracy: 0.5558\n",
      "Epoch 40/60\n",
      "38/38 - 0s - loss: 0.6788 - accuracy: 0.5633\n",
      "Epoch 41/60\n",
      "38/38 - 0s - loss: 0.6764 - accuracy: 0.5649\n",
      "Epoch 42/60\n",
      "38/38 - 0s - loss: 0.6772 - accuracy: 0.5674\n",
      "Epoch 43/60\n",
      "38/38 - 0s - loss: 0.6774 - accuracy: 0.5658\n",
      "Epoch 44/60\n",
      "38/38 - 0s - loss: 0.6774 - accuracy: 0.5641\n",
      "Epoch 45/60\n",
      "38/38 - 0s - loss: 0.6781 - accuracy: 0.5757\n",
      "Epoch 46/60\n",
      "38/38 - 0s - loss: 0.6742 - accuracy: 0.5699\n",
      "Epoch 47/60\n",
      "38/38 - 0s - loss: 0.6740 - accuracy: 0.5666\n",
      "Epoch 48/60\n",
      "38/38 - 0s - loss: 0.6750 - accuracy: 0.5724\n",
      "Epoch 49/60\n",
      "38/38 - 0s - loss: 0.6737 - accuracy: 0.5658\n",
      "Epoch 50/60\n",
      "38/38 - 0s - loss: 0.6758 - accuracy: 0.5682\n",
      "Epoch 51/60\n",
      "38/38 - 0s - loss: 0.6753 - accuracy: 0.5724\n",
      "Epoch 52/60\n",
      "38/38 - 0s - loss: 0.6736 - accuracy: 0.5666\n",
      "Epoch 53/60\n",
      "38/38 - 0s - loss: 0.6746 - accuracy: 0.5608\n",
      "Epoch 54/60\n",
      "38/38 - 0s - loss: 0.6709 - accuracy: 0.5682\n",
      "Epoch 55/60\n",
      "38/38 - 0s - loss: 0.6708 - accuracy: 0.5773\n",
      "Epoch 56/60\n",
      "38/38 - 0s - loss: 0.6702 - accuracy: 0.5749\n",
      "Epoch 57/60\n",
      "38/38 - 0s - loss: 0.6693 - accuracy: 0.5773\n",
      "Epoch 58/60\n",
      "38/38 - 0s - loss: 0.6706 - accuracy: 0.5715\n",
      "Epoch 59/60\n",
      "38/38 - 0s - loss: 0.6699 - accuracy: 0.5707\n",
      "Epoch 60/60\n",
      "38/38 - 0s - loss: 0.6677 - accuracy: 0.5773\n",
      "13/13 - 0s - loss: 0.7248 - accuracy: 0.4615\n",
      "----------------------------------------------------------------------------------------\n",
      "TSLA: Normal Neural Network - Loss: 0.7248135805130005, Accuracy: 0.4615384638309479\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C143AD730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(947, 4) (947,)\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.7181 - accuracy: 0.4930\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.7265 - accuracy: 0.5042\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.6990 - accuracy: 0.5268\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.7012 - accuracy: 0.4972\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.6941 - accuracy: 0.5014\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.6938 - accuracy: 0.5366\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.6961 - accuracy: 0.5268\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.6950 - accuracy: 0.5141\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.6930 - accuracy: 0.5408\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5437\n",
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.6929 - accuracy: 0.5437\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.6929 - accuracy: 0.5408\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.6931 - accuracy: 0.5070\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.6945 - accuracy: 0.5493\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.6937 - accuracy: 0.5423\n",
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.6931 - accuracy: 0.5535\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5394\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.6937 - accuracy: 0.5310\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.6918 - accuracy: 0.5451\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5394\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5380\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.6926 - accuracy: 0.5394\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.6907 - accuracy: 0.5211\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.6920 - accuracy: 0.5211\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.6933 - accuracy: 0.5465\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.6926 - accuracy: 0.5408\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5408\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.6919 - accuracy: 0.5408\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.6915 - accuracy: 0.5535\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.6914 - accuracy: 0.5366\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.6912 - accuracy: 0.5507\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.6913 - accuracy: 0.5437\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5493\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5366\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.6912 - accuracy: 0.5493\n",
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.6907 - accuracy: 0.5437\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.6907 - accuracy: 0.5394\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5437\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.6900 - accuracy: 0.5437\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.6905 - accuracy: 0.5423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5437\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.6911 - accuracy: 0.5437\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5437\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5437\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.6926 - accuracy: 0.5437\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.6909 - accuracy: 0.5423\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.6907 - accuracy: 0.5465\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.6905 - accuracy: 0.5423\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.6913 - accuracy: 0.5014\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.6908 - accuracy: 0.5408\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5099\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.6917 - accuracy: 0.5338\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.6912 - accuracy: 0.5437\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5310\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.6908 - accuracy: 0.5437\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5408\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.6904 - accuracy: 0.5451\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.6924 - accuracy: 0.5423\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.6958 - accuracy: 0.5254\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.6970 - accuracy: 0.5211\n",
      "8/8 - 0s - loss: 0.6946 - accuracy: 0.4937\n",
      "----------------------------------------------------------------------------------------\n",
      "AMZN: Normal Neural Network - Loss: 0.6945662498474121, Accuracy: 0.49367088079452515\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1E862048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.7145 - accuracy: 0.5099\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.7032 - accuracy: 0.4887\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.6954 - accuracy: 0.4761\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.6953 - accuracy: 0.5000\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.6966 - accuracy: 0.4972\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.7010 - accuracy: 0.4690\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.6950 - accuracy: 0.4944\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.6993 - accuracy: 0.4887\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.6977 - accuracy: 0.5014\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.6993 - accuracy: 0.4761\n",
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.6958 - accuracy: 0.4873\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.6973 - accuracy: 0.5338\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.6971 - accuracy: 0.5197\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.6991 - accuracy: 0.5056\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.6922 - accuracy: 0.5056\n",
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.6934 - accuracy: 0.5169\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5338\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.6900 - accuracy: 0.5239\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.6944 - accuracy: 0.5211\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.6996 - accuracy: 0.4930\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.6948 - accuracy: 0.5056\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.6947 - accuracy: 0.5000\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.6922 - accuracy: 0.5183\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.6959 - accuracy: 0.5028\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.6926 - accuracy: 0.5183\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.6939 - accuracy: 0.5282\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.6917 - accuracy: 0.5282\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.6928 - accuracy: 0.5211\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5268\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.6963 - accuracy: 0.5000\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.6911 - accuracy: 0.5296\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5155\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.6926 - accuracy: 0.5268\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.6985 - accuracy: 0.4887\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.6930 - accuracy: 0.5183\n",
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.7005 - accuracy: 0.5042\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.6970 - accuracy: 0.5155\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.6927 - accuracy: 0.5225\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5465\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5380\n",
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.6905 - accuracy: 0.5437\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5535\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.6889 - accuracy: 0.5451\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.6950 - accuracy: 0.5225\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.6942 - accuracy: 0.5014\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5352\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.6907 - accuracy: 0.5169\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.6935 - accuracy: 0.5254\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5254\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.6924 - accuracy: 0.5310\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.6922 - accuracy: 0.5324\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5254\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.6944 - accuracy: 0.5211\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5197\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5211\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.6950 - accuracy: 0.5056\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.6943 - accuracy: 0.5169\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.6945 - accuracy: 0.5197\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.6919 - accuracy: 0.5338\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.6946 - accuracy: 0.5225\n",
      "8/8 - 0s - loss: 0.6989 - accuracy: 0.5148\n",
      "----------------------------------------------------------------------------------------\n",
      "AMZN: Normal Neural Network - Loss: 0.6988663077354431, Accuracy: 0.5147679448127747\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C19F14400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.6982 - accuracy: 0.5254\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.6999 - accuracy: 0.5056\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.6993 - accuracy: 0.5169\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.6973 - accuracy: 0.5225\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.6915 - accuracy: 0.5465\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.6919 - accuracy: 0.5352\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5254\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.6976 - accuracy: 0.5000\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5394\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5465\n",
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.6904 - accuracy: 0.5437\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.6885 - accuracy: 0.5437\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.6917 - accuracy: 0.5479\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.6925 - accuracy: 0.5394\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.6930 - accuracy: 0.5366\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.6890 - accuracy: 0.5507\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.6893 - accuracy: 0.5535\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.6948 - accuracy: 0.5380\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.6952 - accuracy: 0.5000\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.6919 - accuracy: 0.5394\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.6904 - accuracy: 0.5479\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.6885 - accuracy: 0.5507\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.6912 - accuracy: 0.5521\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5408\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.6889 - accuracy: 0.5521\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.6893 - accuracy: 0.5521\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.6917 - accuracy: 0.5437\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.6888 - accuracy: 0.5479\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.6886 - accuracy: 0.5549\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.6911 - accuracy: 0.5535\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.6913 - accuracy: 0.5423\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.6885 - accuracy: 0.5493\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.6883 - accuracy: 0.5423\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.6882 - accuracy: 0.5535\n",
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.6913 - accuracy: 0.5408\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.6896 - accuracy: 0.5394\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.6889 - accuracy: 0.5451\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.6914 - accuracy: 0.5423\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.6896 - accuracy: 0.5465\n",
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.6882 - accuracy: 0.5437\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.6938 - accuracy: 0.5437\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.6929 - accuracy: 0.5183\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.6933 - accuracy: 0.5423\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.6892 - accuracy: 0.5465\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.6892 - accuracy: 0.5437\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.6882 - accuracy: 0.5549\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5479\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5465\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5521\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.6884 - accuracy: 0.5493\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.6885 - accuracy: 0.5451\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.6893 - accuracy: 0.5549\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.6893 - accuracy: 0.5535\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.6880 - accuracy: 0.5535\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5535\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.6905 - accuracy: 0.5521\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.6892 - accuracy: 0.5521\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.6921 - accuracy: 0.5310\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.6919 - accuracy: 0.5451\n",
      "8/8 - 0s - loss: 0.6929 - accuracy: 0.5274\n",
      "----------------------------------------------------------------------------------------\n",
      "AMZN: Normal Neural Network - Loss: 0.6928824782371521, Accuracy: 0.5274261832237244\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C13DC9840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.6975 - accuracy: 0.5169\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5352\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.6914 - accuracy: 0.5380\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.6883 - accuracy: 0.5549\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.6889 - accuracy: 0.5535\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.6869 - accuracy: 0.5549\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.6888 - accuracy: 0.5465\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.6891 - accuracy: 0.5465\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.6886 - accuracy: 0.5521\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.6871 - accuracy: 0.5563\n",
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.6881 - accuracy: 0.5549\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.6879 - accuracy: 0.5493\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.6872 - accuracy: 0.5577\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.6877 - accuracy: 0.5493\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.6881 - accuracy: 0.5577\n",
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.6871 - accuracy: 0.5606\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.6864 - accuracy: 0.5563\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.6863 - accuracy: 0.5606\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.6861 - accuracy: 0.5549\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.6879 - accuracy: 0.5634\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.6859 - accuracy: 0.5634\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.6862 - accuracy: 0.5704\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.6870 - accuracy: 0.5634\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.6869 - accuracy: 0.5634\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.6876 - accuracy: 0.5423\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.6867 - accuracy: 0.5606\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.6860 - accuracy: 0.5718\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.6858 - accuracy: 0.5676\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.6867 - accuracy: 0.5592\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.6862 - accuracy: 0.5620\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.6848 - accuracy: 0.5662\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.6843 - accuracy: 0.5634\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.6827 - accuracy: 0.5718\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.6831 - accuracy: 0.5690\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.6814 - accuracy: 0.5704\n",
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.6821 - accuracy: 0.5732\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.6819 - accuracy: 0.5718\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.6825 - accuracy: 0.5620\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.6817 - accuracy: 0.5732\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.6834 - accuracy: 0.5690\n",
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.6802 - accuracy: 0.5789\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.6825 - accuracy: 0.5676\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.6794 - accuracy: 0.5817\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.6805 - accuracy: 0.5676\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.6847 - accuracy: 0.5549\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.6799 - accuracy: 0.5746\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.6779 - accuracy: 0.5803\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.6772 - accuracy: 0.5775\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.6793 - accuracy: 0.5704\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.6793 - accuracy: 0.5690\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.6788 - accuracy: 0.5789\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.6771 - accuracy: 0.5803\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.6778 - accuracy: 0.5718\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.6776 - accuracy: 0.5789\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.6791 - accuracy: 0.5746\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.6759 - accuracy: 0.5775\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.6735 - accuracy: 0.5831\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.6733 - accuracy: 0.5831\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.6713 - accuracy: 0.5817\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.6717 - accuracy: 0.5831\n",
      "8/8 - 0s - loss: 0.7041 - accuracy: 0.5274\n",
      "----------------------------------------------------------------------------------------\n",
      "AMZN: Normal Neural Network - Loss: 0.7040994763374329, Accuracy: 0.5274261832237244\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C14CB86A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(187, 4) (187,)\n",
      "Epoch 1/60\n",
      "5/5 - 0s - loss: 0.7507 - accuracy: 0.5500\n",
      "Epoch 2/60\n",
      "5/5 - 0s - loss: 0.7224 - accuracy: 0.5929\n",
      "Epoch 3/60\n",
      "5/5 - 0s - loss: 0.6838 - accuracy: 0.5857\n",
      "Epoch 4/60\n",
      "5/5 - 0s - loss: 0.6837 - accuracy: 0.5929\n",
      "Epoch 5/60\n",
      "5/5 - 0s - loss: 0.6797 - accuracy: 0.6071\n",
      "Epoch 6/60\n",
      "5/5 - 0s - loss: 0.6805 - accuracy: 0.6000\n",
      "Epoch 7/60\n",
      "5/5 - 0s - loss: 0.6692 - accuracy: 0.6143\n",
      "Epoch 8/60\n",
      "5/5 - 0s - loss: 0.6669 - accuracy: 0.5571\n",
      "Epoch 9/60\n",
      "5/5 - 0s - loss: 0.6754 - accuracy: 0.5500\n",
      "Epoch 10/60\n",
      "5/5 - 0s - loss: 0.6771 - accuracy: 0.5643\n",
      "Epoch 11/60\n",
      "5/5 - 0s - loss: 0.6737 - accuracy: 0.5714\n",
      "Epoch 12/60\n",
      "5/5 - 0s - loss: 0.6775 - accuracy: 0.5929\n",
      "Epoch 13/60\n",
      "5/5 - 0s - loss: 0.6738 - accuracy: 0.6000\n",
      "Epoch 14/60\n",
      "5/5 - 0s - loss: 0.6660 - accuracy: 0.6071\n",
      "Epoch 15/60\n",
      "5/5 - 0s - loss: 0.6660 - accuracy: 0.6000\n",
      "Epoch 16/60\n",
      "5/5 - 0s - loss: 0.6655 - accuracy: 0.6071\n",
      "Epoch 17/60\n",
      "5/5 - 0s - loss: 0.6666 - accuracy: 0.6143\n",
      "Epoch 18/60\n",
      "5/5 - 0s - loss: 0.6666 - accuracy: 0.6071\n",
      "Epoch 19/60\n",
      "5/5 - 0s - loss: 0.6664 - accuracy: 0.6071\n",
      "Epoch 20/60\n",
      "5/5 - 0s - loss: 0.6668 - accuracy: 0.6000\n",
      "Epoch 21/60\n",
      "5/5 - 0s - loss: 0.6655 - accuracy: 0.6000\n",
      "Epoch 22/60\n",
      "5/5 - 0s - loss: 0.6652 - accuracy: 0.6000\n",
      "Epoch 23/60\n",
      "5/5 - 0s - loss: 0.6653 - accuracy: 0.6071\n",
      "Epoch 24/60\n",
      "5/5 - 0s - loss: 0.6705 - accuracy: 0.6071\n",
      "Epoch 25/60\n",
      "5/5 - 0s - loss: 0.6658 - accuracy: 0.6000\n",
      "Epoch 26/60\n",
      "5/5 - 0s - loss: 0.6638 - accuracy: 0.6000\n",
      "Epoch 27/60\n",
      "5/5 - 0s - loss: 0.6666 - accuracy: 0.6071\n",
      "Epoch 28/60\n",
      "5/5 - 0s - loss: 0.6670 - accuracy: 0.6000\n",
      "Epoch 29/60\n",
      "5/5 - 0s - loss: 0.6652 - accuracy: 0.5786\n",
      "Epoch 30/60\n",
      "5/5 - 0s - loss: 0.6663 - accuracy: 0.6071\n",
      "Epoch 31/60\n",
      "5/5 - 0s - loss: 0.6625 - accuracy: 0.6071\n",
      "Epoch 32/60\n",
      "5/5 - 0s - loss: 0.6645 - accuracy: 0.6000\n",
      "Epoch 33/60\n",
      "5/5 - 0s - loss: 0.6639 - accuracy: 0.6000\n",
      "Epoch 34/60\n",
      "5/5 - 0s - loss: 0.6641 - accuracy: 0.6000\n",
      "Epoch 35/60\n",
      "5/5 - 0s - loss: 0.6635 - accuracy: 0.6000\n",
      "Epoch 36/60\n",
      "5/5 - 0s - loss: 0.6636 - accuracy: 0.6071\n",
      "Epoch 37/60\n",
      "5/5 - 0s - loss: 0.6651 - accuracy: 0.6071\n",
      "Epoch 38/60\n",
      "5/5 - 0s - loss: 0.6643 - accuracy: 0.6000\n",
      "Epoch 39/60\n",
      "5/5 - 0s - loss: 0.6636 - accuracy: 0.6071\n",
      "Epoch 40/60\n",
      "5/5 - 0s - loss: 0.6636 - accuracy: 0.5929\n",
      "Epoch 41/60\n",
      "5/5 - 0s - loss: 0.6679 - accuracy: 0.6000\n",
      "Epoch 42/60\n",
      "5/5 - 0s - loss: 0.6676 - accuracy: 0.6000\n",
      "Epoch 43/60\n",
      "5/5 - 0s - loss: 0.6673 - accuracy: 0.5929\n",
      "Epoch 44/60\n",
      "5/5 - 0s - loss: 0.6631 - accuracy: 0.6000\n",
      "Epoch 45/60\n",
      "5/5 - 0s - loss: 0.6628 - accuracy: 0.6071\n",
      "Epoch 46/60\n",
      "5/5 - 0s - loss: 0.6636 - accuracy: 0.6071\n",
      "Epoch 47/60\n",
      "5/5 - 0s - loss: 0.6656 - accuracy: 0.6071\n",
      "Epoch 48/60\n",
      "5/5 - 0s - loss: 0.6646 - accuracy: 0.6071\n",
      "Epoch 49/60\n",
      "5/5 - 0s - loss: 0.6627 - accuracy: 0.6071\n",
      "Epoch 50/60\n",
      "5/5 - 0s - loss: 0.6667 - accuracy: 0.6000\n",
      "Epoch 51/60\n",
      "5/5 - 0s - loss: 0.6662 - accuracy: 0.6071\n",
      "Epoch 52/60\n",
      "5/5 - 0s - loss: 0.6673 - accuracy: 0.6143\n",
      "Epoch 53/60\n",
      "5/5 - 0s - loss: 0.6642 - accuracy: 0.6000\n",
      "Epoch 54/60\n",
      "5/5 - 0s - loss: 0.6647 - accuracy: 0.6071\n",
      "Epoch 55/60\n",
      "5/5 - 0s - loss: 0.6631 - accuracy: 0.6071\n",
      "Epoch 56/60\n",
      "5/5 - 0s - loss: 0.6642 - accuracy: 0.6071\n",
      "Epoch 57/60\n",
      "5/5 - 0s - loss: 0.6636 - accuracy: 0.6071\n",
      "Epoch 58/60\n",
      "5/5 - 0s - loss: 0.6657 - accuracy: 0.6000\n",
      "Epoch 59/60\n",
      "5/5 - 0s - loss: 0.6650 - accuracy: 0.5929\n",
      "Epoch 60/60\n",
      "5/5 - 0s - loss: 0.6642 - accuracy: 0.5929\n",
      "2/2 - 0s - loss: 0.6965 - accuracy: 0.5957\n",
      "----------------------------------------------------------------------------------------\n",
      "SPY: Normal Neural Network - Loss: 0.696489691734314, Accuracy: 0.5957446694374084\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1734FE18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "5/5 - 0s - loss: 0.7632 - accuracy: 0.4643\n",
      "Epoch 2/60\n",
      "5/5 - 0s - loss: 0.7083 - accuracy: 0.6071\n",
      "Epoch 3/60\n",
      "5/5 - 0s - loss: 0.7002 - accuracy: 0.6071\n",
      "Epoch 4/60\n",
      "5/5 - 0s - loss: 0.6826 - accuracy: 0.6071\n",
      "Epoch 5/60\n",
      "5/5 - 0s - loss: 0.6835 - accuracy: 0.6071\n",
      "Epoch 6/60\n",
      "5/5 - 0s - loss: 0.6770 - accuracy: 0.6071\n",
      "Epoch 7/60\n",
      "5/5 - 0s - loss: 0.6765 - accuracy: 0.6071\n",
      "Epoch 8/60\n",
      "5/5 - 0s - loss: 0.6751 - accuracy: 0.6071\n",
      "Epoch 9/60\n",
      "5/5 - 0s - loss: 0.6738 - accuracy: 0.6071\n",
      "Epoch 10/60\n",
      "5/5 - 0s - loss: 0.6718 - accuracy: 0.6071\n",
      "Epoch 11/60\n",
      "5/5 - 0s - loss: 0.6731 - accuracy: 0.6071\n",
      "Epoch 12/60\n",
      "5/5 - 0s - loss: 0.6701 - accuracy: 0.6071\n",
      "Epoch 13/60\n",
      "5/5 - 0s - loss: 0.6723 - accuracy: 0.6071\n",
      "Epoch 14/60\n",
      "5/5 - 0s - loss: 0.6728 - accuracy: 0.6071\n",
      "Epoch 15/60\n",
      "5/5 - 0s - loss: 0.6698 - accuracy: 0.6071\n",
      "Epoch 16/60\n",
      "5/5 - 0s - loss: 0.6705 - accuracy: 0.6071\n",
      "Epoch 17/60\n",
      "5/5 - 0s - loss: 0.6718 - accuracy: 0.6071\n",
      "Epoch 18/60\n",
      "5/5 - 0s - loss: 0.6709 - accuracy: 0.6071\n",
      "Epoch 19/60\n",
      "5/5 - 0s - loss: 0.6701 - accuracy: 0.6071\n",
      "Epoch 20/60\n",
      "5/5 - 0s - loss: 0.6687 - accuracy: 0.6071\n",
      "Epoch 21/60\n",
      "5/5 - 0s - loss: 0.6722 - accuracy: 0.6071\n",
      "Epoch 22/60\n",
      "5/5 - 0s - loss: 0.6687 - accuracy: 0.6071\n",
      "Epoch 23/60\n",
      "5/5 - 0s - loss: 0.6704 - accuracy: 0.6071\n",
      "Epoch 24/60\n",
      "5/5 - 0s - loss: 0.6717 - accuracy: 0.6071\n",
      "Epoch 25/60\n",
      "5/5 - 0s - loss: 0.6696 - accuracy: 0.6071\n",
      "Epoch 26/60\n",
      "5/5 - 0s - loss: 0.6732 - accuracy: 0.6071\n",
      "Epoch 27/60\n",
      "5/5 - 0s - loss: 0.6696 - accuracy: 0.6071\n",
      "Epoch 28/60\n",
      "5/5 - 0s - loss: 0.6790 - accuracy: 0.6071\n",
      "Epoch 29/60\n",
      "5/5 - 0s - loss: 0.6695 - accuracy: 0.6071\n",
      "Epoch 30/60\n",
      "5/5 - 0s - loss: 0.6741 - accuracy: 0.6071\n",
      "Epoch 31/60\n",
      "5/5 - 0s - loss: 0.6705 - accuracy: 0.6071\n",
      "Epoch 32/60\n",
      "5/5 - 0s - loss: 0.6706 - accuracy: 0.6071\n",
      "Epoch 33/60\n",
      "5/5 - 0s - loss: 0.6735 - accuracy: 0.6071\n",
      "Epoch 34/60\n",
      "5/5 - 0s - loss: 0.6694 - accuracy: 0.6071\n",
      "Epoch 35/60\n",
      "5/5 - 0s - loss: 0.6820 - accuracy: 0.6071\n",
      "Epoch 36/60\n",
      "5/5 - 0s - loss: 0.6742 - accuracy: 0.6071\n",
      "Epoch 37/60\n",
      "5/5 - 0s - loss: 0.6695 - accuracy: 0.6071\n",
      "Epoch 38/60\n",
      "5/5 - 0s - loss: 0.6745 - accuracy: 0.6071\n",
      "Epoch 39/60\n",
      "5/5 - 0s - loss: 0.6707 - accuracy: 0.6071\n",
      "Epoch 40/60\n",
      "5/5 - 0s - loss: 0.6727 - accuracy: 0.6071\n",
      "Epoch 41/60\n",
      "5/5 - 0s - loss: 0.6694 - accuracy: 0.6071\n",
      "Epoch 42/60\n",
      "5/5 - 0s - loss: 0.6711 - accuracy: 0.6071\n",
      "Epoch 43/60\n",
      "5/5 - 0s - loss: 0.6771 - accuracy: 0.6071\n",
      "Epoch 44/60\n",
      "5/5 - 0s - loss: 0.6682 - accuracy: 0.6071\n",
      "Epoch 45/60\n",
      "5/5 - 0s - loss: 0.6686 - accuracy: 0.6071\n",
      "Epoch 46/60\n",
      "5/5 - 0s - loss: 0.6724 - accuracy: 0.6071\n",
      "Epoch 47/60\n",
      "5/5 - 0s - loss: 0.6706 - accuracy: 0.6071\n",
      "Epoch 48/60\n",
      "5/5 - 0s - loss: 0.6709 - accuracy: 0.6071\n",
      "Epoch 49/60\n",
      "5/5 - 0s - loss: 0.6713 - accuracy: 0.6071\n",
      "Epoch 50/60\n",
      "5/5 - 0s - loss: 0.6688 - accuracy: 0.6071\n",
      "Epoch 51/60\n",
      "5/5 - 0s - loss: 0.6746 - accuracy: 0.6071\n",
      "Epoch 52/60\n",
      "5/5 - 0s - loss: 0.6691 - accuracy: 0.6071\n",
      "Epoch 53/60\n",
      "5/5 - 0s - loss: 0.6706 - accuracy: 0.6071\n",
      "Epoch 54/60\n",
      "5/5 - 0s - loss: 0.6683 - accuracy: 0.6071\n",
      "Epoch 55/60\n",
      "5/5 - 0s - loss: 0.6780 - accuracy: 0.6071\n",
      "Epoch 56/60\n",
      "5/5 - 0s - loss: 0.6813 - accuracy: 0.6071\n",
      "Epoch 57/60\n",
      "5/5 - 0s - loss: 0.6692 - accuracy: 0.6071\n",
      "Epoch 58/60\n",
      "5/5 - 0s - loss: 0.6667 - accuracy: 0.6071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60\n",
      "5/5 - 0s - loss: 0.6724 - accuracy: 0.6071\n",
      "Epoch 60/60\n",
      "5/5 - 0s - loss: 0.6641 - accuracy: 0.6071\n",
      "2/2 - 0s - loss: 0.6779 - accuracy: 0.5957\n",
      "----------------------------------------------------------------------------------------\n",
      "SPY: Normal Neural Network - Loss: 0.677935004234314, Accuracy: 0.5957446694374084\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C187C98C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "5/5 - 0s - loss: 0.7101 - accuracy: 0.5571\n",
      "Epoch 2/60\n",
      "5/5 - 0s - loss: 0.7012 - accuracy: 0.5929\n",
      "Epoch 3/60\n",
      "5/5 - 0s - loss: 0.6566 - accuracy: 0.6143\n",
      "Epoch 4/60\n",
      "5/5 - 0s - loss: 0.6860 - accuracy: 0.6000\n",
      "Epoch 5/60\n",
      "5/5 - 0s - loss: 0.6656 - accuracy: 0.6000\n",
      "Epoch 6/60\n",
      "5/5 - 0s - loss: 0.6562 - accuracy: 0.6214\n",
      "Epoch 7/60\n",
      "5/5 - 0s - loss: 0.6578 - accuracy: 0.6071\n",
      "Epoch 8/60\n",
      "5/5 - 0s - loss: 0.6545 - accuracy: 0.5857\n",
      "Epoch 9/60\n",
      "5/5 - 0s - loss: 0.6504 - accuracy: 0.6357\n",
      "Epoch 10/60\n",
      "5/5 - 0s - loss: 0.6494 - accuracy: 0.6286\n",
      "Epoch 11/60\n",
      "5/5 - 0s - loss: 0.6487 - accuracy: 0.5929\n",
      "Epoch 12/60\n",
      "5/5 - 0s - loss: 0.6506 - accuracy: 0.5857\n",
      "Epoch 13/60\n",
      "5/5 - 0s - loss: 0.6480 - accuracy: 0.5929\n",
      "Epoch 14/60\n",
      "5/5 - 0s - loss: 0.6489 - accuracy: 0.6214\n",
      "Epoch 15/60\n",
      "5/5 - 0s - loss: 0.6498 - accuracy: 0.5929\n",
      "Epoch 16/60\n",
      "5/5 - 0s - loss: 0.6499 - accuracy: 0.6000\n",
      "Epoch 17/60\n",
      "5/5 - 0s - loss: 0.6485 - accuracy: 0.6357\n",
      "Epoch 18/60\n",
      "5/5 - 0s - loss: 0.6479 - accuracy: 0.6143\n",
      "Epoch 19/60\n",
      "5/5 - 0s - loss: 0.6629 - accuracy: 0.5786\n",
      "Epoch 20/60\n",
      "5/5 - 0s - loss: 0.6512 - accuracy: 0.6143\n",
      "Epoch 21/60\n",
      "5/5 - 0s - loss: 0.6503 - accuracy: 0.6286\n",
      "Epoch 22/60\n",
      "5/5 - 0s - loss: 0.6470 - accuracy: 0.6429\n",
      "Epoch 23/60\n",
      "5/5 - 0s - loss: 0.6469 - accuracy: 0.5857\n",
      "Epoch 24/60\n",
      "5/5 - 0s - loss: 0.6501 - accuracy: 0.5714\n",
      "Epoch 25/60\n",
      "5/5 - 0s - loss: 0.6469 - accuracy: 0.5786\n",
      "Epoch 26/60\n",
      "5/5 - 0s - loss: 0.6503 - accuracy: 0.6000\n",
      "Epoch 27/60\n",
      "5/5 - 0s - loss: 0.6515 - accuracy: 0.6071\n",
      "Epoch 28/60\n",
      "5/5 - 0s - loss: 0.6471 - accuracy: 0.5929\n",
      "Epoch 29/60\n",
      "5/5 - 0s - loss: 0.6455 - accuracy: 0.6000\n",
      "Epoch 30/60\n",
      "5/5 - 0s - loss: 0.6546 - accuracy: 0.5643\n",
      "Epoch 31/60\n",
      "5/5 - 0s - loss: 0.6502 - accuracy: 0.5857\n",
      "Epoch 32/60\n",
      "5/5 - 0s - loss: 0.6564 - accuracy: 0.6143\n",
      "Epoch 33/60\n",
      "5/5 - 0s - loss: 0.6447 - accuracy: 0.6214\n",
      "Epoch 34/60\n",
      "5/5 - 0s - loss: 0.6551 - accuracy: 0.5929\n",
      "Epoch 35/60\n",
      "5/5 - 0s - loss: 0.6515 - accuracy: 0.5857\n",
      "Epoch 36/60\n",
      "5/5 - 0s - loss: 0.6563 - accuracy: 0.6214\n",
      "Epoch 37/60\n",
      "5/5 - 0s - loss: 0.6490 - accuracy: 0.6214\n",
      "Epoch 38/60\n",
      "5/5 - 0s - loss: 0.6454 - accuracy: 0.6214\n",
      "Epoch 39/60\n",
      "5/5 - 0s - loss: 0.6474 - accuracy: 0.6000\n",
      "Epoch 40/60\n",
      "5/5 - 0s - loss: 0.6443 - accuracy: 0.6286\n",
      "Epoch 41/60\n",
      "5/5 - 0s - loss: 0.6430 - accuracy: 0.6071\n",
      "Epoch 42/60\n",
      "5/5 - 0s - loss: 0.6438 - accuracy: 0.6143\n",
      "Epoch 43/60\n",
      "5/5 - 0s - loss: 0.6426 - accuracy: 0.6286\n",
      "Epoch 44/60\n",
      "5/5 - 0s - loss: 0.6438 - accuracy: 0.6286\n",
      "Epoch 45/60\n",
      "5/5 - 0s - loss: 0.6485 - accuracy: 0.6143\n",
      "Epoch 46/60\n",
      "5/5 - 0s - loss: 0.6443 - accuracy: 0.6214\n",
      "Epoch 47/60\n",
      "5/5 - 0s - loss: 0.6448 - accuracy: 0.6000\n",
      "Epoch 48/60\n",
      "5/5 - 0s - loss: 0.6419 - accuracy: 0.6286\n",
      "Epoch 49/60\n",
      "5/5 - 0s - loss: 0.6450 - accuracy: 0.6000\n",
      "Epoch 50/60\n",
      "5/5 - 0s - loss: 0.6416 - accuracy: 0.5857\n",
      "Epoch 51/60\n",
      "5/5 - 0s - loss: 0.6422 - accuracy: 0.6000\n",
      "Epoch 52/60\n",
      "5/5 - 0s - loss: 0.6415 - accuracy: 0.6071\n",
      "Epoch 53/60\n",
      "5/5 - 0s - loss: 0.6423 - accuracy: 0.5857\n",
      "Epoch 54/60\n",
      "5/5 - 0s - loss: 0.6463 - accuracy: 0.6000\n",
      "Epoch 55/60\n",
      "5/5 - 0s - loss: 0.6423 - accuracy: 0.6071\n",
      "Epoch 56/60\n",
      "5/5 - 0s - loss: 0.6429 - accuracy: 0.6214\n",
      "Epoch 57/60\n",
      "5/5 - 0s - loss: 0.6393 - accuracy: 0.6429\n",
      "Epoch 58/60\n",
      "5/5 - 0s - loss: 0.6458 - accuracy: 0.6286\n",
      "Epoch 59/60\n",
      "5/5 - 0s - loss: 0.6432 - accuracy: 0.5929\n",
      "Epoch 60/60\n",
      "5/5 - 0s - loss: 0.6462 - accuracy: 0.6143\n",
      "2/2 - 0s - loss: 0.6714 - accuracy: 0.6170\n",
      "----------------------------------------------------------------------------------------\n",
      "SPY: Normal Neural Network - Loss: 0.6714247465133667, Accuracy: 0.6170212626457214\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1405A1E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "5/5 - 0s - loss: 0.7010 - accuracy: 0.5000\n",
      "Epoch 2/60\n",
      "5/5 - 0s - loss: 0.6711 - accuracy: 0.6071\n",
      "Epoch 3/60\n",
      "5/5 - 0s - loss: 0.6670 - accuracy: 0.6071\n",
      "Epoch 4/60\n",
      "5/5 - 0s - loss: 0.6584 - accuracy: 0.6071\n",
      "Epoch 5/60\n",
      "5/5 - 0s - loss: 0.6523 - accuracy: 0.6071\n",
      "Epoch 6/60\n",
      "5/5 - 0s - loss: 0.6486 - accuracy: 0.6286\n",
      "Epoch 7/60\n",
      "5/5 - 0s - loss: 0.6431 - accuracy: 0.6286\n",
      "Epoch 8/60\n",
      "5/5 - 0s - loss: 0.6389 - accuracy: 0.6357\n",
      "Epoch 9/60\n",
      "5/5 - 0s - loss: 0.6375 - accuracy: 0.6357\n",
      "Epoch 10/60\n",
      "5/5 - 0s - loss: 0.6338 - accuracy: 0.6286\n",
      "Epoch 11/60\n",
      "5/5 - 0s - loss: 0.6318 - accuracy: 0.6286\n",
      "Epoch 12/60\n",
      "5/5 - 0s - loss: 0.6306 - accuracy: 0.6357\n",
      "Epoch 13/60\n",
      "5/5 - 0s - loss: 0.6263 - accuracy: 0.6286\n",
      "Epoch 14/60\n",
      "5/5 - 0s - loss: 0.6249 - accuracy: 0.6286\n",
      "Epoch 15/60\n",
      "5/5 - 0s - loss: 0.6248 - accuracy: 0.6286\n",
      "Epoch 16/60\n",
      "5/5 - 0s - loss: 0.6267 - accuracy: 0.6286\n",
      "Epoch 17/60\n",
      "5/5 - 0s - loss: 0.6188 - accuracy: 0.6357\n",
      "Epoch 18/60\n",
      "5/5 - 0s - loss: 0.6246 - accuracy: 0.6357\n",
      "Epoch 19/60\n",
      "5/5 - 0s - loss: 0.6190 - accuracy: 0.6500\n",
      "Epoch 20/60\n",
      "5/5 - 0s - loss: 0.6173 - accuracy: 0.6429\n",
      "Epoch 21/60\n",
      "5/5 - 0s - loss: 0.6122 - accuracy: 0.6500\n",
      "Epoch 22/60\n",
      "5/5 - 0s - loss: 0.6086 - accuracy: 0.6500\n",
      "Epoch 23/60\n",
      "5/5 - 0s - loss: 0.6093 - accuracy: 0.6643\n",
      "Epoch 24/60\n",
      "5/5 - 0s - loss: 0.6080 - accuracy: 0.6571\n",
      "Epoch 25/60\n",
      "5/5 - 0s - loss: 0.6069 - accuracy: 0.6857\n",
      "Epoch 26/60\n",
      "5/5 - 0s - loss: 0.6051 - accuracy: 0.6929\n",
      "Epoch 27/60\n",
      "5/5 - 0s - loss: 0.6041 - accuracy: 0.6857\n",
      "Epoch 28/60\n",
      "5/5 - 0s - loss: 0.5994 - accuracy: 0.6714\n",
      "Epoch 29/60\n",
      "5/5 - 0s - loss: 0.5995 - accuracy: 0.6714\n",
      "Epoch 30/60\n",
      "5/5 - 0s - loss: 0.5988 - accuracy: 0.7071\n",
      "Epoch 31/60\n",
      "5/5 - 0s - loss: 0.5930 - accuracy: 0.6786\n",
      "Epoch 32/60\n",
      "5/5 - 0s - loss: 0.5948 - accuracy: 0.6786\n",
      "Epoch 33/60\n",
      "5/5 - 0s - loss: 0.5922 - accuracy: 0.6857\n",
      "Epoch 34/60\n",
      "5/5 - 0s - loss: 0.5877 - accuracy: 0.7071\n",
      "Epoch 35/60\n",
      "5/5 - 0s - loss: 0.5891 - accuracy: 0.7000\n",
      "Epoch 36/60\n",
      "5/5 - 0s - loss: 0.5816 - accuracy: 0.7000\n",
      "Epoch 37/60\n",
      "5/5 - 0s - loss: 0.5804 - accuracy: 0.6786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/60\n",
      "5/5 - 0s - loss: 0.5765 - accuracy: 0.6857\n",
      "Epoch 39/60\n",
      "5/5 - 0s - loss: 0.5758 - accuracy: 0.7000\n",
      "Epoch 40/60\n",
      "5/5 - 0s - loss: 0.5780 - accuracy: 0.6929\n",
      "Epoch 41/60\n",
      "5/5 - 0s - loss: 0.5757 - accuracy: 0.7071\n",
      "Epoch 42/60\n",
      "5/5 - 0s - loss: 0.5689 - accuracy: 0.7000\n",
      "Epoch 43/60\n",
      "5/5 - 0s - loss: 0.5636 - accuracy: 0.6929\n",
      "Epoch 44/60\n",
      "5/5 - 0s - loss: 0.5670 - accuracy: 0.7000\n",
      "Epoch 45/60\n",
      "5/5 - 0s - loss: 0.5676 - accuracy: 0.7071\n",
      "Epoch 46/60\n",
      "5/5 - 0s - loss: 0.5583 - accuracy: 0.7214\n",
      "Epoch 47/60\n",
      "5/5 - 0s - loss: 0.5650 - accuracy: 0.6929\n",
      "Epoch 48/60\n",
      "5/5 - 0s - loss: 0.5586 - accuracy: 0.7000\n",
      "Epoch 49/60\n",
      "5/5 - 0s - loss: 0.5568 - accuracy: 0.7357\n",
      "Epoch 50/60\n",
      "5/5 - 0s - loss: 0.5570 - accuracy: 0.7214\n",
      "Epoch 51/60\n",
      "5/5 - 0s - loss: 0.5487 - accuracy: 0.7071\n",
      "Epoch 52/60\n",
      "5/5 - 0s - loss: 0.5509 - accuracy: 0.7143\n",
      "Epoch 53/60\n",
      "5/5 - 0s - loss: 0.5450 - accuracy: 0.7214\n",
      "Epoch 54/60\n",
      "5/5 - 0s - loss: 0.5561 - accuracy: 0.7071\n",
      "Epoch 55/60\n",
      "5/5 - 0s - loss: 0.5803 - accuracy: 0.7000\n",
      "Epoch 56/60\n",
      "5/5 - 0s - loss: 0.5657 - accuracy: 0.7214\n",
      "Epoch 57/60\n",
      "5/5 - 0s - loss: 0.5468 - accuracy: 0.7071\n",
      "Epoch 58/60\n",
      "5/5 - 0s - loss: 0.5387 - accuracy: 0.7286\n",
      "Epoch 59/60\n",
      "5/5 - 0s - loss: 0.5458 - accuracy: 0.7357\n",
      "Epoch 60/60\n",
      "5/5 - 0s - loss: 0.5379 - accuracy: 0.7357\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_test_function.<locals>.test_function at 0x0000023C1437CC80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 - 0s - loss: 0.8683 - accuracy: 0.4681\n",
      "----------------------------------------------------------------------------------------\n",
      "SPY: Normal Neural Network - Loss: 0.8683403730392456, Accuracy: 0.4680851101875305\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C13EDD950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT'\n",
      " 'TAKE MONEY OUT']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(68, 4) (68,)\n",
      "Epoch 1/60\n",
      "2/2 - 0s - loss: 0.7552 - accuracy: 0.4314\n",
      "Epoch 2/60\n",
      "2/2 - 0s - loss: 0.6801 - accuracy: 0.5882\n",
      "Epoch 3/60\n",
      "2/2 - 0s - loss: 0.6960 - accuracy: 0.5686\n",
      "Epoch 4/60\n",
      "2/2 - 0s - loss: 0.6848 - accuracy: 0.5490\n",
      "Epoch 5/60\n",
      "2/2 - 0s - loss: 0.6828 - accuracy: 0.5490\n",
      "Epoch 6/60\n",
      "2/2 - 0s - loss: 0.6769 - accuracy: 0.5686\n",
      "Epoch 7/60\n",
      "2/2 - 0s - loss: 0.6638 - accuracy: 0.5686\n",
      "Epoch 8/60\n",
      "2/2 - 0s - loss: 0.6611 - accuracy: 0.5882\n",
      "Epoch 9/60\n",
      "2/2 - 0s - loss: 0.6748 - accuracy: 0.5882\n",
      "Epoch 10/60\n",
      "2/2 - 0s - loss: 0.6750 - accuracy: 0.5882\n",
      "Epoch 11/60\n",
      "2/2 - 0s - loss: 0.6648 - accuracy: 0.5882\n",
      "Epoch 12/60\n",
      "2/2 - 0s - loss: 0.6625 - accuracy: 0.5686\n",
      "Epoch 13/60\n",
      "2/2 - 0s - loss: 0.6623 - accuracy: 0.5686\n",
      "Epoch 14/60\n",
      "2/2 - 0s - loss: 0.6635 - accuracy: 0.5882\n",
      "Epoch 15/60\n",
      "2/2 - 0s - loss: 0.6629 - accuracy: 0.6078\n",
      "Epoch 16/60\n",
      "2/2 - 0s - loss: 0.6603 - accuracy: 0.5490\n",
      "Epoch 17/60\n",
      "2/2 - 0s - loss: 0.6613 - accuracy: 0.5490\n",
      "Epoch 18/60\n",
      "2/2 - 0s - loss: 0.6616 - accuracy: 0.5490\n",
      "Epoch 19/60\n",
      "2/2 - 0s - loss: 0.6611 - accuracy: 0.5686\n",
      "Epoch 20/60\n",
      "2/2 - 0s - loss: 0.6609 - accuracy: 0.5882\n",
      "Epoch 21/60\n",
      "2/2 - 0s - loss: 0.6619 - accuracy: 0.5882\n",
      "Epoch 22/60\n",
      "2/2 - 0s - loss: 0.6603 - accuracy: 0.5882\n",
      "Epoch 23/60\n",
      "2/2 - 0s - loss: 0.6606 - accuracy: 0.5882\n",
      "Epoch 24/60\n",
      "2/2 - 0s - loss: 0.6646 - accuracy: 0.6078\n",
      "Epoch 25/60\n",
      "2/2 - 0s - loss: 0.6648 - accuracy: 0.5882\n",
      "Epoch 26/60\n",
      "2/2 - 0s - loss: 0.6607 - accuracy: 0.6078\n",
      "Epoch 27/60\n",
      "2/2 - 0s - loss: 0.6592 - accuracy: 0.5882\n",
      "Epoch 28/60\n",
      "2/2 - 0s - loss: 0.6627 - accuracy: 0.5882\n",
      "Epoch 29/60\n",
      "2/2 - 0s - loss: 0.6607 - accuracy: 0.5686\n",
      "Epoch 30/60\n",
      "2/2 - 0s - loss: 0.6598 - accuracy: 0.5882\n",
      "Epoch 31/60\n",
      "2/2 - 0s - loss: 0.6617 - accuracy: 0.5882\n",
      "Epoch 32/60\n",
      "2/2 - 0s - loss: 0.6643 - accuracy: 0.5882\n",
      "Epoch 33/60\n",
      "2/2 - 0s - loss: 0.6623 - accuracy: 0.5686\n",
      "Epoch 34/60\n",
      "2/2 - 0s - loss: 0.6586 - accuracy: 0.5882\n",
      "Epoch 35/60\n",
      "2/2 - 0s - loss: 0.6618 - accuracy: 0.6078\n",
      "Epoch 36/60\n",
      "2/2 - 0s - loss: 0.6670 - accuracy: 0.6078\n",
      "Epoch 37/60\n",
      "2/2 - 0s - loss: 0.6652 - accuracy: 0.6078\n",
      "Epoch 38/60\n",
      "2/2 - 0s - loss: 0.6610 - accuracy: 0.5882\n",
      "Epoch 39/60\n",
      "2/2 - 0s - loss: 0.6610 - accuracy: 0.5882\n",
      "Epoch 40/60\n",
      "2/2 - 0s - loss: 0.6645 - accuracy: 0.5686\n",
      "Epoch 41/60\n",
      "2/2 - 0s - loss: 0.6646 - accuracy: 0.5686\n",
      "Epoch 42/60\n",
      "2/2 - 0s - loss: 0.6638 - accuracy: 0.5686\n",
      "Epoch 43/60\n",
      "2/2 - 0s - loss: 0.6635 - accuracy: 0.5882\n",
      "Epoch 44/60\n",
      "2/2 - 0s - loss: 0.6602 - accuracy: 0.5882\n",
      "Epoch 45/60\n",
      "2/2 - 0s - loss: 0.6603 - accuracy: 0.5686\n",
      "Epoch 46/60\n",
      "2/2 - 0s - loss: 0.6637 - accuracy: 0.5686\n",
      "Epoch 47/60\n",
      "2/2 - 0s - loss: 0.6613 - accuracy: 0.5882\n",
      "Epoch 48/60\n",
      "2/2 - 0s - loss: 0.6593 - accuracy: 0.5882\n",
      "Epoch 49/60\n",
      "2/2 - 0s - loss: 0.6626 - accuracy: 0.5882\n",
      "Epoch 50/60\n",
      "2/2 - 0s - loss: 0.6685 - accuracy: 0.5490\n",
      "Epoch 51/60\n",
      "2/2 - 0s - loss: 0.6642 - accuracy: 0.5490\n",
      "Epoch 52/60\n",
      "2/2 - 0s - loss: 0.6620 - accuracy: 0.5882\n",
      "Epoch 53/60\n",
      "2/2 - 0s - loss: 0.6658 - accuracy: 0.5882\n",
      "Epoch 54/60\n",
      "2/2 - 0s - loss: 0.6672 - accuracy: 0.5686\n",
      "Epoch 55/60\n",
      "2/2 - 0s - loss: 0.6711 - accuracy: 0.5686\n",
      "Epoch 56/60\n",
      "2/2 - 0s - loss: 0.6636 - accuracy: 0.5686\n",
      "Epoch 57/60\n",
      "2/2 - 0s - loss: 0.6589 - accuracy: 0.5686\n",
      "Epoch 58/60\n",
      "2/2 - 0s - loss: 0.6660 - accuracy: 0.5686\n",
      "Epoch 59/60\n",
      "2/2 - 0s - loss: 0.6627 - accuracy: 0.5686\n",
      "Epoch 60/60\n",
      "2/2 - 0s - loss: 0.6630 - accuracy: 0.5686\n",
      "WARNING:tensorflow:6 out of the last 17 calls to <function Model.make_test_function.<locals>.test_function at 0x0000023C148F5B70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 - 0s - loss: 0.6779 - accuracy: 0.5882\n",
      "----------------------------------------------------------------------------------------\n",
      "DOCU: Normal Neural Network - Loss: 0.6778608560562134, Accuracy: 0.5882353186607361\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C14523378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT'\n",
      " 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "2/2 - 0s - loss: 0.6883 - accuracy: 0.5490\n",
      "Epoch 2/60\n",
      "2/2 - 0s - loss: 0.7127 - accuracy: 0.4706\n",
      "Epoch 3/60\n",
      "2/2 - 0s - loss: 0.6889 - accuracy: 0.6275\n",
      "Epoch 4/60\n",
      "2/2 - 0s - loss: 0.7073 - accuracy: 0.5294\n",
      "Epoch 5/60\n",
      "2/2 - 0s - loss: 0.7054 - accuracy: 0.5294\n",
      "Epoch 6/60\n",
      "2/2 - 0s - loss: 0.6898 - accuracy: 0.5294\n",
      "Epoch 7/60\n",
      "2/2 - 0s - loss: 0.6931 - accuracy: 0.5098\n",
      "Epoch 8/60\n",
      "2/2 - 0s - loss: 0.7019 - accuracy: 0.4706\n",
      "Epoch 9/60\n",
      "2/2 - 0s - loss: 0.7047 - accuracy: 0.4706\n",
      "Epoch 10/60\n",
      "2/2 - 0s - loss: 0.6996 - accuracy: 0.4706\n",
      "Epoch 11/60\n",
      "2/2 - 0s - loss: 0.7012 - accuracy: 0.3725\n",
      "Epoch 12/60\n",
      "2/2 - 0s - loss: 0.6926 - accuracy: 0.5294\n",
      "Epoch 13/60\n",
      "2/2 - 0s - loss: 0.6940 - accuracy: 0.5294\n",
      "Epoch 14/60\n",
      "2/2 - 0s - loss: 0.6934 - accuracy: 0.5294\n",
      "Epoch 15/60\n",
      "2/2 - 0s - loss: 0.6926 - accuracy: 0.5294\n",
      "Epoch 16/60\n",
      "2/2 - 0s - loss: 0.6899 - accuracy: 0.5294\n",
      "Epoch 17/60\n",
      "2/2 - 0s - loss: 0.6907 - accuracy: 0.5294\n",
      "Epoch 18/60\n",
      "2/2 - 0s - loss: 0.6917 - accuracy: 0.5294\n",
      "Epoch 19/60\n",
      "2/2 - 0s - loss: 0.6919 - accuracy: 0.6078\n",
      "Epoch 20/60\n",
      "2/2 - 0s - loss: 0.6909 - accuracy: 0.5294\n",
      "Epoch 21/60\n",
      "2/2 - 0s - loss: 0.6901 - accuracy: 0.5294\n",
      "Epoch 22/60\n",
      "2/2 - 0s - loss: 0.6903 - accuracy: 0.5294\n",
      "Epoch 23/60\n",
      "2/2 - 0s - loss: 0.6906 - accuracy: 0.5294\n",
      "Epoch 24/60\n",
      "2/2 - 0s - loss: 0.6905 - accuracy: 0.5294\n",
      "Epoch 25/60\n",
      "2/2 - 0s - loss: 0.6898 - accuracy: 0.5294\n",
      "Epoch 26/60\n",
      "2/2 - 0s - loss: 0.6893 - accuracy: 0.5294\n",
      "Epoch 27/60\n",
      "2/2 - 0s - loss: 0.6907 - accuracy: 0.5294\n",
      "Epoch 28/60\n",
      "2/2 - 0s - loss: 0.6917 - accuracy: 0.5294\n",
      "Epoch 29/60\n",
      "2/2 - 0s - loss: 0.6909 - accuracy: 0.5294\n",
      "Epoch 30/60\n",
      "2/2 - 0s - loss: 0.6894 - accuracy: 0.5294\n",
      "Epoch 31/60\n",
      "2/2 - 0s - loss: 0.6890 - accuracy: 0.5294\n",
      "Epoch 32/60\n",
      "2/2 - 0s - loss: 0.6905 - accuracy: 0.5294\n",
      "Epoch 33/60\n",
      "2/2 - 0s - loss: 0.6891 - accuracy: 0.5294\n",
      "Epoch 34/60\n",
      "2/2 - 0s - loss: 0.6904 - accuracy: 0.5294\n",
      "Epoch 35/60\n",
      "2/2 - 0s - loss: 0.6901 - accuracy: 0.5294\n",
      "Epoch 36/60\n",
      "2/2 - 0s - loss: 0.6899 - accuracy: 0.5294\n",
      "Epoch 37/60\n",
      "2/2 - 0s - loss: 0.6888 - accuracy: 0.5294\n",
      "Epoch 38/60\n",
      "2/2 - 0s - loss: 0.6883 - accuracy: 0.5294\n",
      "Epoch 39/60\n",
      "2/2 - 0s - loss: 0.6879 - accuracy: 0.5294\n",
      "Epoch 40/60\n",
      "2/2 - 0s - loss: 0.6894 - accuracy: 0.5490\n",
      "Epoch 41/60\n",
      "2/2 - 0s - loss: 0.6905 - accuracy: 0.5882\n",
      "Epoch 42/60\n",
      "2/2 - 0s - loss: 0.6899 - accuracy: 0.6078\n",
      "Epoch 43/60\n",
      "2/2 - 0s - loss: 0.6870 - accuracy: 0.6078\n",
      "Epoch 44/60\n",
      "2/2 - 0s - loss: 0.6882 - accuracy: 0.5294\n",
      "Epoch 45/60\n",
      "2/2 - 0s - loss: 0.6879 - accuracy: 0.5294\n",
      "Epoch 46/60\n",
      "2/2 - 0s - loss: 0.6881 - accuracy: 0.5294\n",
      "Epoch 47/60\n",
      "2/2 - 0s - loss: 0.6874 - accuracy: 0.5294\n",
      "Epoch 48/60\n",
      "2/2 - 0s - loss: 0.6850 - accuracy: 0.5294\n",
      "Epoch 49/60\n",
      "2/2 - 0s - loss: 0.6840 - accuracy: 0.5294\n",
      "Epoch 50/60\n",
      "2/2 - 0s - loss: 0.6855 - accuracy: 0.5686\n",
      "Epoch 51/60\n",
      "2/2 - 0s - loss: 0.6872 - accuracy: 0.5490\n",
      "Epoch 52/60\n",
      "2/2 - 0s - loss: 0.6873 - accuracy: 0.5294\n",
      "Epoch 53/60\n",
      "2/2 - 0s - loss: 0.6892 - accuracy: 0.4902\n",
      "Epoch 54/60\n",
      "2/2 - 0s - loss: 0.6859 - accuracy: 0.5490\n",
      "Epoch 55/60\n",
      "2/2 - 0s - loss: 0.6855 - accuracy: 0.5490\n",
      "Epoch 56/60\n",
      "2/2 - 0s - loss: 0.6816 - accuracy: 0.5294\n",
      "Epoch 57/60\n",
      "2/2 - 0s - loss: 0.6818 - accuracy: 0.5294\n",
      "Epoch 58/60\n",
      "2/2 - 0s - loss: 0.6807 - accuracy: 0.5294\n",
      "Epoch 59/60\n",
      "2/2 - 0s - loss: 0.6801 - accuracy: 0.5294\n",
      "Epoch 60/60\n",
      "2/2 - 0s - loss: 0.6792 - accuracy: 0.4902\n",
      "WARNING:tensorflow:7 out of the last 18 calls to <function Model.make_test_function.<locals>.test_function at 0x0000023C1E823488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 - 0s - loss: 0.6864 - accuracy: 0.5294\n",
      "----------------------------------------------------------------------------------------\n",
      "DOCU: Normal Neural Network - Loss: 0.686362087726593, Accuracy: 0.529411792755127\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C13CAAB70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT'\n",
      " 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "2/2 - 0s - loss: 0.7366 - accuracy: 0.4902\n",
      "Epoch 2/60\n",
      "2/2 - 0s - loss: 0.6650 - accuracy: 0.5490\n",
      "Epoch 3/60\n",
      "2/2 - 0s - loss: 0.6788 - accuracy: 0.5490\n",
      "Epoch 4/60\n",
      "2/2 - 0s - loss: 0.6928 - accuracy: 0.5686\n",
      "Epoch 5/60\n",
      "2/2 - 0s - loss: 0.6862 - accuracy: 0.5490\n",
      "Epoch 6/60\n",
      "2/2 - 0s - loss: 0.6695 - accuracy: 0.5882\n",
      "Epoch 7/60\n",
      "2/2 - 0s - loss: 0.6660 - accuracy: 0.5882\n",
      "Epoch 8/60\n",
      "2/2 - 0s - loss: 0.6686 - accuracy: 0.5490\n",
      "Epoch 9/60\n",
      "2/2 - 0s - loss: 0.6663 - accuracy: 0.5490\n",
      "Epoch 10/60\n",
      "2/2 - 0s - loss: 0.6641 - accuracy: 0.5490\n",
      "Epoch 11/60\n",
      "2/2 - 0s - loss: 0.6610 - accuracy: 0.5490\n",
      "Epoch 12/60\n",
      "2/2 - 0s - loss: 0.6599 - accuracy: 0.5686\n",
      "Epoch 13/60\n",
      "2/2 - 0s - loss: 0.6596 - accuracy: 0.5882\n",
      "Epoch 14/60\n",
      "2/2 - 0s - loss: 0.6590 - accuracy: 0.5686\n",
      "Epoch 15/60\n",
      "2/2 - 0s - loss: 0.6594 - accuracy: 0.5686\n",
      "Epoch 16/60\n",
      "2/2 - 0s - loss: 0.6560 - accuracy: 0.5686\n",
      "Epoch 17/60\n",
      "2/2 - 0s - loss: 0.6592 - accuracy: 0.6078\n",
      "Epoch 18/60\n",
      "2/2 - 0s - loss: 0.6571 - accuracy: 0.5882\n",
      "Epoch 19/60\n",
      "2/2 - 0s - loss: 0.6517 - accuracy: 0.5686\n",
      "Epoch 20/60\n",
      "2/2 - 0s - loss: 0.6529 - accuracy: 0.5686\n",
      "Epoch 21/60\n",
      "2/2 - 0s - loss: 0.6514 - accuracy: 0.5686\n",
      "Epoch 22/60\n",
      "2/2 - 0s - loss: 0.6559 - accuracy: 0.5882\n",
      "Epoch 23/60\n",
      "2/2 - 0s - loss: 0.6493 - accuracy: 0.5882\n",
      "Epoch 24/60\n",
      "2/2 - 0s - loss: 0.6471 - accuracy: 0.5882\n",
      "Epoch 25/60\n",
      "2/2 - 0s - loss: 0.6474 - accuracy: 0.5686\n",
      "Epoch 26/60\n",
      "2/2 - 0s - loss: 0.6507 - accuracy: 0.5686\n",
      "Epoch 27/60\n",
      "2/2 - 0s - loss: 0.6496 - accuracy: 0.5686\n",
      "Epoch 28/60\n",
      "2/2 - 0s - loss: 0.6477 - accuracy: 0.5686\n",
      "Epoch 29/60\n",
      "2/2 - 0s - loss: 0.6380 - accuracy: 0.5686\n",
      "Epoch 30/60\n",
      "2/2 - 0s - loss: 0.6509 - accuracy: 0.5490\n",
      "Epoch 31/60\n",
      "2/2 - 0s - loss: 0.6518 - accuracy: 0.5882\n",
      "Epoch 32/60\n",
      "2/2 - 0s - loss: 0.6493 - accuracy: 0.5882\n",
      "Epoch 33/60\n",
      "2/2 - 0s - loss: 0.6380 - accuracy: 0.5882\n",
      "Epoch 34/60\n",
      "2/2 - 0s - loss: 0.6352 - accuracy: 0.5882\n",
      "Epoch 35/60\n",
      "2/2 - 0s - loss: 0.6357 - accuracy: 0.5882\n",
      "Epoch 36/60\n",
      "2/2 - 0s - loss: 0.6417 - accuracy: 0.5294\n",
      "Epoch 37/60\n",
      "2/2 - 0s - loss: 0.6351 - accuracy: 0.5686\n",
      "Epoch 38/60\n",
      "2/2 - 0s - loss: 0.6346 - accuracy: 0.6078\n",
      "Epoch 39/60\n",
      "2/2 - 0s - loss: 0.6289 - accuracy: 0.6275\n",
      "Epoch 40/60\n",
      "2/2 - 0s - loss: 0.6314 - accuracy: 0.6275\n",
      "Epoch 41/60\n",
      "2/2 - 0s - loss: 0.6307 - accuracy: 0.6471\n",
      "Epoch 42/60\n",
      "2/2 - 0s - loss: 0.6229 - accuracy: 0.6667\n",
      "Epoch 43/60\n",
      "2/2 - 0s - loss: 0.6211 - accuracy: 0.6471\n",
      "Epoch 44/60\n",
      "2/2 - 0s - loss: 0.6244 - accuracy: 0.6275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/60\n",
      "2/2 - 0s - loss: 0.6196 - accuracy: 0.6471\n",
      "Epoch 46/60\n",
      "2/2 - 0s - loss: 0.6234 - accuracy: 0.6471\n",
      "Epoch 47/60\n",
      "2/2 - 0s - loss: 0.6189 - accuracy: 0.6471\n",
      "Epoch 48/60\n",
      "2/2 - 0s - loss: 0.6148 - accuracy: 0.6471\n",
      "Epoch 49/60\n",
      "2/2 - 0s - loss: 0.6141 - accuracy: 0.6275\n",
      "Epoch 50/60\n",
      "2/2 - 0s - loss: 0.6171 - accuracy: 0.6471\n",
      "Epoch 51/60\n",
      "2/2 - 0s - loss: 0.6166 - accuracy: 0.6667\n",
      "Epoch 52/60\n",
      "2/2 - 0s - loss: 0.6079 - accuracy: 0.6667\n",
      "Epoch 53/60\n",
      "2/2 - 0s - loss: 0.6091 - accuracy: 0.6078\n",
      "Epoch 54/60\n",
      "2/2 - 0s - loss: 0.6131 - accuracy: 0.5882\n",
      "Epoch 55/60\n",
      "2/2 - 0s - loss: 0.6094 - accuracy: 0.5686\n",
      "Epoch 56/60\n",
      "2/2 - 0s - loss: 0.6003 - accuracy: 0.6275\n",
      "Epoch 57/60\n",
      "2/2 - 0s - loss: 0.6010 - accuracy: 0.6471\n",
      "Epoch 58/60\n",
      "2/2 - 0s - loss: 0.6052 - accuracy: 0.6863\n",
      "Epoch 59/60\n",
      "2/2 - 0s - loss: 0.6115 - accuracy: 0.6667\n",
      "Epoch 60/60\n",
      "2/2 - 0s - loss: 0.5984 - accuracy: 0.6667\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000023C19A70840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 - 0s - loss: 0.8660 - accuracy: 0.4118\n",
      "----------------------------------------------------------------------------------------\n",
      "DOCU: Normal Neural Network - Loss: 0.865954577922821, Accuracy: 0.4117647111415863\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1C31A488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT'\n",
      " 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "2/2 - 0s - loss: 0.6939 - accuracy: 0.4706\n",
      "Epoch 2/60\n",
      "2/2 - 0s - loss: 0.6788 - accuracy: 0.5882\n",
      "Epoch 3/60\n",
      "2/2 - 0s - loss: 0.6700 - accuracy: 0.5882\n",
      "Epoch 4/60\n",
      "2/2 - 0s - loss: 0.6632 - accuracy: 0.5686\n",
      "Epoch 5/60\n",
      "2/2 - 0s - loss: 0.6572 - accuracy: 0.6078\n",
      "Epoch 6/60\n",
      "2/2 - 0s - loss: 0.6517 - accuracy: 0.5882\n",
      "Epoch 7/60\n",
      "2/2 - 0s - loss: 0.6475 - accuracy: 0.5882\n",
      "Epoch 8/60\n",
      "2/2 - 0s - loss: 0.6444 - accuracy: 0.5882\n",
      "Epoch 9/60\n",
      "2/2 - 0s - loss: 0.6410 - accuracy: 0.5882\n",
      "Epoch 10/60\n",
      "2/2 - 0s - loss: 0.6369 - accuracy: 0.6078\n",
      "Epoch 11/60\n",
      "2/2 - 0s - loss: 0.6334 - accuracy: 0.5882\n",
      "Epoch 12/60\n",
      "2/2 - 0s - loss: 0.6316 - accuracy: 0.5882\n",
      "Epoch 13/60\n",
      "2/2 - 0s - loss: 0.6263 - accuracy: 0.5882\n",
      "Epoch 14/60\n",
      "2/2 - 0s - loss: 0.6232 - accuracy: 0.5686\n",
      "Epoch 15/60\n",
      "2/2 - 0s - loss: 0.6237 - accuracy: 0.6471\n",
      "Epoch 16/60\n",
      "2/2 - 0s - loss: 0.6186 - accuracy: 0.6078\n",
      "Epoch 17/60\n",
      "2/2 - 0s - loss: 0.6143 - accuracy: 0.5882\n",
      "Epoch 18/60\n",
      "2/2 - 0s - loss: 0.6087 - accuracy: 0.6275\n",
      "Epoch 19/60\n",
      "2/2 - 0s - loss: 0.6081 - accuracy: 0.6471\n",
      "Epoch 20/60\n",
      "2/2 - 0s - loss: 0.6036 - accuracy: 0.6863\n",
      "Epoch 21/60\n",
      "2/2 - 0s - loss: 0.6043 - accuracy: 0.6863\n",
      "Epoch 22/60\n",
      "2/2 - 0s - loss: 0.5989 - accuracy: 0.6667\n",
      "Epoch 23/60\n",
      "2/2 - 0s - loss: 0.5956 - accuracy: 0.6471\n",
      "Epoch 24/60\n",
      "2/2 - 0s - loss: 0.5910 - accuracy: 0.6471\n",
      "Epoch 25/60\n",
      "2/2 - 0s - loss: 0.5882 - accuracy: 0.6667\n",
      "Epoch 26/60\n",
      "2/2 - 0s - loss: 0.5851 - accuracy: 0.6471\n",
      "Epoch 27/60\n",
      "2/2 - 0s - loss: 0.5839 - accuracy: 0.6667\n",
      "Epoch 28/60\n",
      "2/2 - 0s - loss: 0.5817 - accuracy: 0.6471\n",
      "Epoch 29/60\n",
      "2/2 - 0s - loss: 0.5762 - accuracy: 0.6471\n",
      "Epoch 30/60\n",
      "2/2 - 0s - loss: 0.5747 - accuracy: 0.6275\n",
      "Epoch 31/60\n",
      "2/2 - 0s - loss: 0.5784 - accuracy: 0.5882\n",
      "Epoch 32/60\n",
      "2/2 - 0s - loss: 0.5754 - accuracy: 0.5882\n",
      "Epoch 33/60\n",
      "2/2 - 0s - loss: 0.5659 - accuracy: 0.6275\n",
      "Epoch 34/60\n",
      "2/2 - 0s - loss: 0.5682 - accuracy: 0.6667\n",
      "Epoch 35/60\n",
      "2/2 - 0s - loss: 0.5700 - accuracy: 0.6667\n",
      "Epoch 36/60\n",
      "2/2 - 0s - loss: 0.5670 - accuracy: 0.6667\n",
      "Epoch 37/60\n",
      "2/2 - 0s - loss: 0.5598 - accuracy: 0.6667\n",
      "Epoch 38/60\n",
      "2/2 - 0s - loss: 0.5591 - accuracy: 0.6667\n",
      "Epoch 39/60\n",
      "2/2 - 0s - loss: 0.5585 - accuracy: 0.6667\n",
      "Epoch 40/60\n",
      "2/2 - 0s - loss: 0.5571 - accuracy: 0.6275\n",
      "Epoch 41/60\n",
      "2/2 - 0s - loss: 0.5561 - accuracy: 0.6863\n",
      "Epoch 42/60\n",
      "2/2 - 0s - loss: 0.5495 - accuracy: 0.6863\n",
      "Epoch 43/60\n",
      "2/2 - 0s - loss: 0.5482 - accuracy: 0.6667\n",
      "Epoch 44/60\n",
      "2/2 - 0s - loss: 0.5443 - accuracy: 0.6275\n",
      "Epoch 45/60\n",
      "2/2 - 0s - loss: 0.5524 - accuracy: 0.6275\n",
      "Epoch 46/60\n",
      "2/2 - 0s - loss: 0.5477 - accuracy: 0.6078\n",
      "Epoch 47/60\n",
      "2/2 - 0s - loss: 0.5379 - accuracy: 0.6471\n",
      "Epoch 48/60\n",
      "2/2 - 0s - loss: 0.5405 - accuracy: 0.6863\n",
      "Epoch 49/60\n",
      "2/2 - 0s - loss: 0.5420 - accuracy: 0.6863\n",
      "Epoch 50/60\n",
      "2/2 - 0s - loss: 0.5361 - accuracy: 0.6667\n",
      "Epoch 51/60\n",
      "2/2 - 0s - loss: 0.5368 - accuracy: 0.6471\n",
      "Epoch 52/60\n",
      "2/2 - 0s - loss: 0.5326 - accuracy: 0.6275\n",
      "Epoch 53/60\n",
      "2/2 - 0s - loss: 0.5279 - accuracy: 0.7843\n",
      "Epoch 54/60\n",
      "2/2 - 0s - loss: 0.5419 - accuracy: 0.6667\n",
      "Epoch 55/60\n",
      "2/2 - 0s - loss: 0.5419 - accuracy: 0.6667\n",
      "Epoch 56/60\n",
      "2/2 - 0s - loss: 0.5321 - accuracy: 0.6667\n",
      "Epoch 57/60\n",
      "2/2 - 0s - loss: 0.5254 - accuracy: 0.6471\n",
      "Epoch 58/60\n",
      "2/2 - 0s - loss: 0.5292 - accuracy: 0.6471\n",
      "Epoch 59/60\n",
      "2/2 - 0s - loss: 0.5257 - accuracy: 0.6471\n",
      "Epoch 60/60\n",
      "2/2 - 0s - loss: 0.5225 - accuracy: 0.6667\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x0000023C19A35510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 - 0s - loss: 1.2751 - accuracy: 0.3529\n",
      "----------------------------------------------------------------------------------------\n",
      "DOCU: Normal Neural Network - Loss: 1.2750557661056519, Accuracy: 0.3529411852359772\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1D5BDC80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT'\n",
      " 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1981, 4) (1981,)\n",
      "Epoch 1/60\n",
      "47/47 - 0s - loss: 0.7237 - accuracy: 0.4599\n",
      "Epoch 2/60\n",
      "47/47 - 0s - loss: 0.7013 - accuracy: 0.5017\n",
      "Epoch 3/60\n",
      "47/47 - 0s - loss: 0.6993 - accuracy: 0.4970\n",
      "Epoch 4/60\n",
      "47/47 - 0s - loss: 0.6957 - accuracy: 0.4754\n",
      "Epoch 5/60\n",
      "47/47 - 0s - loss: 0.6967 - accuracy: 0.4997\n",
      "Epoch 6/60\n",
      "47/47 - 0s - loss: 0.6960 - accuracy: 0.4976\n",
      "Epoch 7/60\n",
      "47/47 - 0s - loss: 0.6961 - accuracy: 0.4923\n",
      "Epoch 8/60\n",
      "47/47 - 0s - loss: 0.6951 - accuracy: 0.4815\n",
      "Epoch 9/60\n",
      "47/47 - 0s - loss: 0.6948 - accuracy: 0.4835\n",
      "Epoch 10/60\n",
      "47/47 - 0s - loss: 0.6943 - accuracy: 0.5077\n",
      "Epoch 11/60\n",
      "47/47 - 0s - loss: 0.6954 - accuracy: 0.4855\n",
      "Epoch 12/60\n",
      "47/47 - 0s - loss: 0.6943 - accuracy: 0.4970\n",
      "Epoch 13/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4889\n",
      "Epoch 14/60\n",
      "47/47 - 0s - loss: 0.6948 - accuracy: 0.4949\n",
      "Epoch 15/60\n",
      "47/47 - 0s - loss: 0.6947 - accuracy: 0.4909\n",
      "Epoch 16/60\n",
      "47/47 - 0s - loss: 0.6960 - accuracy: 0.4923\n",
      "Epoch 17/60\n",
      "47/47 - 0s - loss: 0.6950 - accuracy: 0.4956\n",
      "Epoch 18/60\n",
      "47/47 - 0s - loss: 0.6947 - accuracy: 0.5037\n",
      "Epoch 19/60\n",
      "47/47 - 0s - loss: 0.6950 - accuracy: 0.4774\n",
      "Epoch 20/60\n",
      "47/47 - 0s - loss: 0.6945 - accuracy: 0.4983\n",
      "Epoch 21/60\n",
      "47/47 - 0s - loss: 0.6950 - accuracy: 0.5030\n",
      "Epoch 22/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.5064\n",
      "Epoch 23/60\n",
      "47/47 - 0s - loss: 0.6973 - accuracy: 0.4936\n",
      "Epoch 24/60\n",
      "47/47 - 0s - loss: 0.6949 - accuracy: 0.5051\n",
      "Epoch 25/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4842\n",
      "Epoch 26/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.4997\n",
      "Epoch 27/60\n",
      "47/47 - 0s - loss: 0.6950 - accuracy: 0.4990\n",
      "Epoch 28/60\n",
      "47/47 - 0s - loss: 0.6955 - accuracy: 0.4687\n",
      "Epoch 29/60\n",
      "47/47 - 0s - loss: 0.6953 - accuracy: 0.4970\n",
      "Epoch 30/60\n",
      "47/47 - 0s - loss: 0.6944 - accuracy: 0.4943\n",
      "Epoch 31/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.5091\n",
      "Epoch 32/60\n",
      "47/47 - 0s - loss: 0.6943 - accuracy: 0.4956\n",
      "Epoch 33/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.4923\n",
      "Epoch 34/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4687\n",
      "Epoch 35/60\n",
      "47/47 - 0s - loss: 0.6945 - accuracy: 0.4835\n",
      "Epoch 36/60\n",
      "47/47 - 0s - loss: 0.6937 - accuracy: 0.4916\n",
      "Epoch 37/60\n",
      "47/47 - 0s - loss: 0.6945 - accuracy: 0.5003\n",
      "Epoch 38/60\n",
      "47/47 - 0s - loss: 0.6933 - accuracy: 0.4983\n",
      "Epoch 39/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.5030\n",
      "Epoch 40/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.4923\n",
      "Epoch 41/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.4909\n",
      "Epoch 42/60\n",
      "47/47 - 0s - loss: 0.6932 - accuracy: 0.4916\n",
      "Epoch 43/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.5057\n",
      "Epoch 44/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.4963\n",
      "Epoch 45/60\n",
      "47/47 - 0s - loss: 0.6932 - accuracy: 0.5057\n",
      "Epoch 46/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.4842\n",
      "Epoch 47/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.4929\n",
      "Epoch 48/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.4970\n",
      "Epoch 49/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.4808\n",
      "Epoch 50/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.4929\n",
      "Epoch 51/60\n",
      "47/47 - 0s - loss: 0.6941 - accuracy: 0.4909\n",
      "Epoch 52/60\n",
      "47/47 - 0s - loss: 0.6941 - accuracy: 0.4956\n",
      "Epoch 53/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.5010\n",
      "Epoch 54/60\n",
      "47/47 - 0s - loss: 0.6946 - accuracy: 0.4970\n",
      "Epoch 55/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4882\n",
      "Epoch 56/60\n",
      "47/47 - 0s - loss: 0.6940 - accuracy: 0.4929\n",
      "Epoch 57/60\n",
      "47/47 - 0s - loss: 0.6953 - accuracy: 0.4842\n",
      "Epoch 58/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4916\n",
      "Epoch 59/60\n",
      "47/47 - 0s - loss: 0.6940 - accuracy: 0.4997\n",
      "Epoch 60/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.5003\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000023C1D77BEA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "16/16 - 0s - loss: 0.6935 - accuracy: 0.5141\n",
      "----------------------------------------------------------------------------------------\n",
      "NFLX: Normal Neural Network - Loss: 0.6935026049613953, Accuracy: 0.5141128897666931\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1D6EB840> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'TAKE MONEY OUT' 'TAKE MONEY OUT']\n",
      "Actual Labels: ['ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "47/47 - 0s - loss: 0.6962 - accuracy: 0.5158\n",
      "Epoch 2/60\n",
      "47/47 - 0s - loss: 0.6998 - accuracy: 0.4990\n",
      "Epoch 3/60\n",
      "47/47 - 0s - loss: 0.6971 - accuracy: 0.5104\n",
      "Epoch 4/60\n",
      "47/47 - 0s - loss: 0.6974 - accuracy: 0.4909\n",
      "Epoch 5/60\n",
      "47/47 - 0s - loss: 0.6965 - accuracy: 0.5077\n",
      "Epoch 6/60\n",
      "47/47 - 0s - loss: 0.6977 - accuracy: 0.4875\n",
      "Epoch 7/60\n",
      "47/47 - 0s - loss: 0.7049 - accuracy: 0.5172\n",
      "Epoch 8/60\n",
      "47/47 - 0s - loss: 0.6943 - accuracy: 0.5172\n",
      "Epoch 9/60\n",
      "47/47 - 0s - loss: 0.6964 - accuracy: 0.4997\n",
      "Epoch 10/60\n",
      "47/47 - 0s - loss: 0.6975 - accuracy: 0.5010\n",
      "Epoch 11/60\n",
      "47/47 - 0s - loss: 0.6967 - accuracy: 0.5212\n",
      "Epoch 12/60\n",
      "47/47 - 0s - loss: 0.6951 - accuracy: 0.5064\n",
      "Epoch 13/60\n",
      "47/47 - 0s - loss: 0.6972 - accuracy: 0.5010\n",
      "Epoch 14/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.5030\n",
      "Epoch 15/60\n",
      "47/47 - 0s - loss: 0.6977 - accuracy: 0.4997\n",
      "Epoch 16/60\n",
      "47/47 - 0s - loss: 0.6965 - accuracy: 0.4747\n",
      "Epoch 17/60\n",
      "47/47 - 0s - loss: 0.6956 - accuracy: 0.4970\n",
      "Epoch 18/60\n",
      "47/47 - 0s - loss: 0.6991 - accuracy: 0.4949\n",
      "Epoch 19/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.4956\n",
      "Epoch 20/60\n",
      "47/47 - 0s - loss: 0.6977 - accuracy: 0.4943\n",
      "Epoch 21/60\n",
      "47/47 - 0s - loss: 0.6963 - accuracy: 0.5279\n",
      "Epoch 22/60\n",
      "47/47 - 0s - loss: 0.7014 - accuracy: 0.5051\n",
      "Epoch 23/60\n",
      "47/47 - 0s - loss: 0.6933 - accuracy: 0.5057\n",
      "Epoch 24/60\n",
      "47/47 - 0s - loss: 0.6987 - accuracy: 0.4761\n",
      "Epoch 25/60\n",
      "47/47 - 0s - loss: 0.6951 - accuracy: 0.4949\n",
      "Epoch 26/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.5064\n",
      "Epoch 27/60\n",
      "47/47 - 0s - loss: 0.6937 - accuracy: 0.5098\n",
      "Epoch 28/60\n",
      "47/47 - 0s - loss: 0.6950 - accuracy: 0.5010\n",
      "Epoch 29/60\n",
      "47/47 - 0s - loss: 0.6992 - accuracy: 0.4822\n",
      "Epoch 30/60\n",
      "47/47 - 0s - loss: 0.6945 - accuracy: 0.5104\n",
      "Epoch 31/60\n",
      "47/47 - 0s - loss: 0.6962 - accuracy: 0.4855\n",
      "Epoch 32/60\n",
      "47/47 - 0s - loss: 0.6953 - accuracy: 0.5145\n",
      "Epoch 33/60\n",
      "47/47 - 0s - loss: 0.6961 - accuracy: 0.5111\n",
      "Epoch 34/60\n",
      "47/47 - 0s - loss: 0.6945 - accuracy: 0.4875\n",
      "Epoch 35/60\n",
      "47/47 - 0s - loss: 0.6969 - accuracy: 0.4781\n",
      "Epoch 36/60\n",
      "47/47 - 0s - loss: 0.6957 - accuracy: 0.4889\n",
      "Epoch 37/60\n",
      "47/47 - 0s - loss: 0.6953 - accuracy: 0.4976\n",
      "Epoch 38/60\n",
      "47/47 - 0s - loss: 0.7018 - accuracy: 0.5091\n",
      "Epoch 39/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.4956\n",
      "Epoch 40/60\n",
      "47/47 - 0s - loss: 0.6964 - accuracy: 0.4997\n",
      "Epoch 41/60\n",
      "47/47 - 0s - loss: 0.6949 - accuracy: 0.5104\n",
      "Epoch 42/60\n",
      "47/47 - 0s - loss: 0.6969 - accuracy: 0.4990\n",
      "Epoch 43/60\n",
      "47/47 - 0s - loss: 0.6945 - accuracy: 0.5064\n",
      "Epoch 44/60\n",
      "47/47 - 0s - loss: 0.6954 - accuracy: 0.5037\n",
      "Epoch 45/60\n",
      "47/47 - 0s - loss: 0.6995 - accuracy: 0.4714\n",
      "Epoch 46/60\n",
      "47/47 - 0s - loss: 0.6966 - accuracy: 0.4855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/60\n",
      "47/47 - 0s - loss: 0.6947 - accuracy: 0.4848\n",
      "Epoch 48/60\n",
      "47/47 - 0s - loss: 0.6943 - accuracy: 0.4909\n",
      "Epoch 49/60\n",
      "47/47 - 0s - loss: 0.6933 - accuracy: 0.5172\n",
      "Epoch 50/60\n",
      "47/47 - 0s - loss: 0.6953 - accuracy: 0.5091\n",
      "Epoch 51/60\n",
      "47/47 - 0s - loss: 0.6921 - accuracy: 0.5259\n",
      "Epoch 52/60\n",
      "47/47 - 0s - loss: 0.6967 - accuracy: 0.5104\n",
      "Epoch 53/60\n",
      "47/47 - 0s - loss: 0.6968 - accuracy: 0.4943\n",
      "Epoch 54/60\n",
      "47/47 - 0s - loss: 0.6948 - accuracy: 0.5057\n",
      "Epoch 55/60\n",
      "47/47 - 0s - loss: 0.6946 - accuracy: 0.5152\n",
      "Epoch 56/60\n",
      "47/47 - 0s - loss: 0.6962 - accuracy: 0.4835\n",
      "Epoch 57/60\n",
      "47/47 - 0s - loss: 0.6968 - accuracy: 0.4909\n",
      "Epoch 58/60\n",
      "47/47 - 0s - loss: 0.6959 - accuracy: 0.4983\n",
      "Epoch 59/60\n",
      "47/47 - 0s - loss: 0.6971 - accuracy: 0.5003\n",
      "Epoch 60/60\n",
      "47/47 - 0s - loss: 0.6952 - accuracy: 0.5077\n",
      "16/16 - 0s - loss: 0.6938 - accuracy: 0.4940\n",
      "----------------------------------------------------------------------------------------\n",
      "NFLX: Normal Neural Network - Loss: 0.6937991976737976, Accuracy: 0.49395161867141724\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1EAA81E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT'\n",
      " 'TAKE MONEY OUT']\n",
      "Actual Labels: ['ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "47/47 - 0s - loss: 0.7028 - accuracy: 0.5172\n",
      "Epoch 2/60\n",
      "47/47 - 0s - loss: 0.7014 - accuracy: 0.4734\n",
      "Epoch 3/60\n",
      "47/47 - 0s - loss: 0.6979 - accuracy: 0.4774\n",
      "Epoch 4/60\n",
      "47/47 - 0s - loss: 0.6966 - accuracy: 0.4734\n",
      "Epoch 5/60\n",
      "47/47 - 0s - loss: 0.6966 - accuracy: 0.4788\n",
      "Epoch 6/60\n",
      "47/47 - 0s - loss: 0.6947 - accuracy: 0.4970\n",
      "Epoch 7/60\n",
      "47/47 - 0s - loss: 0.6947 - accuracy: 0.5010\n",
      "Epoch 8/60\n",
      "47/47 - 0s - loss: 0.6953 - accuracy: 0.4848\n",
      "Epoch 9/60\n",
      "47/47 - 0s - loss: 0.6951 - accuracy: 0.4640\n",
      "Epoch 10/60\n",
      "47/47 - 0s - loss: 0.6944 - accuracy: 0.4902\n",
      "Epoch 11/60\n",
      "47/47 - 0s - loss: 0.6953 - accuracy: 0.4956\n",
      "Epoch 12/60\n",
      "47/47 - 0s - loss: 0.6991 - accuracy: 0.5010\n",
      "Epoch 13/60\n",
      "47/47 - 0s - loss: 0.6948 - accuracy: 0.4896\n",
      "Epoch 14/60\n",
      "47/47 - 0s - loss: 0.6946 - accuracy: 0.4902\n",
      "Epoch 15/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.5017\n",
      "Epoch 16/60\n",
      "47/47 - 0s - loss: 0.6957 - accuracy: 0.4956\n",
      "Epoch 17/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4936\n",
      "Epoch 18/60\n",
      "47/47 - 0s - loss: 0.6944 - accuracy: 0.4815\n",
      "Epoch 19/60\n",
      "47/47 - 0s - loss: 0.6937 - accuracy: 0.5024\n",
      "Epoch 20/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.4855\n",
      "Epoch 21/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.4896\n",
      "Epoch 22/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.5084\n",
      "Epoch 23/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4936\n",
      "Epoch 24/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.4990\n",
      "Epoch 25/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.4902\n",
      "Epoch 26/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.4976\n",
      "Epoch 27/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.4929\n",
      "Epoch 28/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.4896\n",
      "Epoch 29/60\n",
      "47/47 - 0s - loss: 0.6964 - accuracy: 0.4956\n",
      "Epoch 30/60\n",
      "47/47 - 0s - loss: 0.6942 - accuracy: 0.4808\n",
      "Epoch 31/60\n",
      "47/47 - 0s - loss: 0.6943 - accuracy: 0.4943\n",
      "Epoch 32/60\n",
      "47/47 - 0s - loss: 0.6948 - accuracy: 0.4970\n",
      "Epoch 33/60\n",
      "47/47 - 0s - loss: 0.6941 - accuracy: 0.5024\n",
      "Epoch 34/60\n",
      "47/47 - 0s - loss: 0.6944 - accuracy: 0.4875\n",
      "Epoch 35/60\n",
      "47/47 - 0s - loss: 0.6941 - accuracy: 0.4963\n",
      "Epoch 36/60\n",
      "47/47 - 0s - loss: 0.6948 - accuracy: 0.4875\n",
      "Epoch 37/60\n",
      "47/47 - 0s - loss: 0.6952 - accuracy: 0.4875\n",
      "Epoch 38/60\n",
      "47/47 - 0s - loss: 0.6940 - accuracy: 0.4976\n",
      "Epoch 39/60\n",
      "47/47 - 0s - loss: 0.6950 - accuracy: 0.4788\n",
      "Epoch 40/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.5030\n",
      "Epoch 41/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.5030\n",
      "Epoch 42/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.4997\n",
      "Epoch 43/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.5044\n",
      "Epoch 44/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.4943\n",
      "Epoch 45/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.4842\n",
      "Epoch 46/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.4828\n",
      "Epoch 47/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.4990\n",
      "Epoch 48/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4896\n",
      "Epoch 49/60\n",
      "47/47 - 0s - loss: 0.6940 - accuracy: 0.4923\n",
      "Epoch 50/60\n",
      "47/47 - 0s - loss: 0.6932 - accuracy: 0.5077\n",
      "Epoch 51/60\n",
      "47/47 - 0s - loss: 0.6960 - accuracy: 0.4889\n",
      "Epoch 52/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.5017\n",
      "Epoch 53/60\n",
      "47/47 - 0s - loss: 0.6939 - accuracy: 0.4747\n",
      "Epoch 54/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.4761\n",
      "Epoch 55/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.5024\n",
      "Epoch 56/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.5064\n",
      "Epoch 57/60\n",
      "47/47 - 0s - loss: 0.6933 - accuracy: 0.5057\n",
      "Epoch 58/60\n",
      "47/47 - 0s - loss: 0.6935 - accuracy: 0.4795\n",
      "Epoch 59/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.4943\n",
      "Epoch 60/60\n",
      "47/47 - 0s - loss: 0.6933 - accuracy: 0.4970\n",
      "16/16 - 0s - loss: 0.6932 - accuracy: 0.5060\n",
      "----------------------------------------------------------------------------------------\n",
      "NFLX: Normal Neural Network - Loss: 0.693164587020874, Accuracy: 0.5060483813285828\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1EB99E18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "47/47 - 0s - loss: 0.6989 - accuracy: 0.4862\n",
      "Epoch 2/60\n",
      "47/47 - 0s - loss: 0.6949 - accuracy: 0.4835\n",
      "Epoch 3/60\n",
      "47/47 - 0s - loss: 0.6942 - accuracy: 0.4963\n",
      "Epoch 4/60\n",
      "47/47 - 0s - loss: 0.6943 - accuracy: 0.5104\n",
      "Epoch 5/60\n",
      "47/47 - 0s - loss: 0.6936 - accuracy: 0.4943\n",
      "Epoch 6/60\n",
      "47/47 - 0s - loss: 0.6932 - accuracy: 0.5138\n",
      "Epoch 7/60\n",
      "47/47 - 0s - loss: 0.6942 - accuracy: 0.4970\n",
      "Epoch 8/60\n",
      "47/47 - 0s - loss: 0.6931 - accuracy: 0.4970\n",
      "Epoch 9/60\n",
      "47/47 - 0s - loss: 0.6932 - accuracy: 0.5037\n",
      "Epoch 10/60\n",
      "47/47 - 0s - loss: 0.6938 - accuracy: 0.5024\n",
      "Epoch 11/60\n",
      "47/47 - 0s - loss: 0.6937 - accuracy: 0.4896\n",
      "Epoch 12/60\n",
      "47/47 - 0s - loss: 0.6930 - accuracy: 0.5091\n",
      "Epoch 13/60\n",
      "47/47 - 0s - loss: 0.6927 - accuracy: 0.5145\n",
      "Epoch 14/60\n",
      "47/47 - 0s - loss: 0.6930 - accuracy: 0.5118\n",
      "Epoch 15/60\n",
      "47/47 - 0s - loss: 0.6923 - accuracy: 0.5024\n",
      "Epoch 16/60\n",
      "47/47 - 0s - loss: 0.6921 - accuracy: 0.5219\n",
      "Epoch 17/60\n",
      "47/47 - 0s - loss: 0.6916 - accuracy: 0.5259\n",
      "Epoch 18/60\n",
      "47/47 - 0s - loss: 0.6924 - accuracy: 0.5131\n",
      "Epoch 19/60\n",
      "47/47 - 0s - loss: 0.6934 - accuracy: 0.5064\n",
      "Epoch 20/60\n",
      "47/47 - 0s - loss: 0.6919 - accuracy: 0.5057\n",
      "Epoch 21/60\n",
      "47/47 - 0s - loss: 0.6919 - accuracy: 0.5253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/60\n",
      "47/47 - 0s - loss: 0.6923 - accuracy: 0.5212\n",
      "Epoch 23/60\n",
      "47/47 - 0s - loss: 0.6924 - accuracy: 0.5212\n",
      "Epoch 24/60\n",
      "47/47 - 0s - loss: 0.6914 - accuracy: 0.5266\n",
      "Epoch 25/60\n",
      "47/47 - 0s - loss: 0.6913 - accuracy: 0.5232\n",
      "Epoch 26/60\n",
      "47/47 - 0s - loss: 0.6918 - accuracy: 0.5279\n",
      "Epoch 27/60\n",
      "47/47 - 0s - loss: 0.6912 - accuracy: 0.5354\n",
      "Epoch 28/60\n",
      "47/47 - 0s - loss: 0.6914 - accuracy: 0.5239\n",
      "Epoch 29/60\n",
      "47/47 - 0s - loss: 0.6921 - accuracy: 0.5279\n",
      "Epoch 30/60\n",
      "47/47 - 0s - loss: 0.6912 - accuracy: 0.5347\n",
      "Epoch 31/60\n",
      "47/47 - 0s - loss: 0.6902 - accuracy: 0.5360\n",
      "Epoch 32/60\n",
      "47/47 - 0s - loss: 0.6915 - accuracy: 0.5327\n",
      "Epoch 33/60\n",
      "47/47 - 0s - loss: 0.6919 - accuracy: 0.5158\n",
      "Epoch 34/60\n",
      "47/47 - 0s - loss: 0.6908 - accuracy: 0.5327\n",
      "Epoch 35/60\n",
      "47/47 - 0s - loss: 0.6911 - accuracy: 0.5367\n",
      "Epoch 36/60\n",
      "47/47 - 0s - loss: 0.6909 - accuracy: 0.5347\n",
      "Epoch 37/60\n",
      "47/47 - 0s - loss: 0.6909 - accuracy: 0.5380\n",
      "Epoch 38/60\n",
      "47/47 - 0s - loss: 0.6900 - accuracy: 0.5347\n",
      "Epoch 39/60\n",
      "47/47 - 0s - loss: 0.6909 - accuracy: 0.5360\n",
      "Epoch 40/60\n",
      "47/47 - 0s - loss: 0.6897 - accuracy: 0.5421\n",
      "Epoch 41/60\n",
      "47/47 - 0s - loss: 0.6895 - accuracy: 0.5380\n",
      "Epoch 42/60\n",
      "47/47 - 0s - loss: 0.6897 - accuracy: 0.5367\n",
      "Epoch 43/60\n",
      "47/47 - 0s - loss: 0.6902 - accuracy: 0.5320\n",
      "Epoch 44/60\n",
      "47/47 - 0s - loss: 0.6891 - accuracy: 0.5333\n",
      "Epoch 45/60\n",
      "47/47 - 0s - loss: 0.6906 - accuracy: 0.5367\n",
      "Epoch 46/60\n",
      "47/47 - 0s - loss: 0.6897 - accuracy: 0.5428\n",
      "Epoch 47/60\n",
      "47/47 - 0s - loss: 0.6892 - accuracy: 0.5347\n",
      "Epoch 48/60\n",
      "47/47 - 0s - loss: 0.6885 - accuracy: 0.5394\n",
      "Epoch 49/60\n",
      "47/47 - 0s - loss: 0.6877 - accuracy: 0.5434\n",
      "Epoch 50/60\n",
      "47/47 - 0s - loss: 0.6880 - accuracy: 0.5414\n",
      "Epoch 51/60\n",
      "47/47 - 0s - loss: 0.6888 - accuracy: 0.5461\n",
      "Epoch 52/60\n",
      "47/47 - 0s - loss: 0.6873 - accuracy: 0.5508\n",
      "Epoch 53/60\n",
      "47/47 - 0s - loss: 0.6870 - accuracy: 0.5522\n",
      "Epoch 54/60\n",
      "47/47 - 0s - loss: 0.6867 - accuracy: 0.5455\n",
      "Epoch 55/60\n",
      "47/47 - 0s - loss: 0.6871 - accuracy: 0.5374\n",
      "Epoch 56/60\n",
      "47/47 - 0s - loss: 0.6873 - accuracy: 0.5468\n",
      "Epoch 57/60\n",
      "47/47 - 0s - loss: 0.6885 - accuracy: 0.5455\n",
      "Epoch 58/60\n",
      "47/47 - 0s - loss: 0.6878 - accuracy: 0.5407\n",
      "Epoch 59/60\n",
      "47/47 - 0s - loss: 0.6879 - accuracy: 0.5428\n",
      "Epoch 60/60\n",
      "47/47 - 0s - loss: 0.6860 - accuracy: 0.5495\n",
      "16/16 - 0s - loss: 0.7052 - accuracy: 0.4657\n",
      "----------------------------------------------------------------------------------------\n",
      "NFLX: Normal Neural Network - Loss: 0.7052357196807861, Accuracy: 0.4657258093357086\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1FD94488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT' 'TAKE MONEY OUT'\n",
      " 'TAKE MONEY OUT']\n",
      "Actual Labels: ['ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(1145, 4) (1145,)\n",
      "Epoch 1/60\n",
      "27/27 - 0s - loss: 0.7133 - accuracy: 0.5117\n",
      "Epoch 2/60\n",
      "27/27 - 0s - loss: 0.7132 - accuracy: 0.5152\n",
      "Epoch 3/60\n",
      "27/27 - 0s - loss: 0.6995 - accuracy: 0.5198\n",
      "Epoch 4/60\n",
      "27/27 - 0s - loss: 0.6963 - accuracy: 0.5000\n",
      "Epoch 5/60\n",
      "27/27 - 0s - loss: 0.6898 - accuracy: 0.5315\n",
      "Epoch 6/60\n",
      "27/27 - 0s - loss: 0.6928 - accuracy: 0.5256\n",
      "Epoch 7/60\n",
      "27/27 - 0s - loss: 0.6906 - accuracy: 0.5210\n",
      "Epoch 8/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5221\n",
      "Epoch 9/60\n",
      "27/27 - 0s - loss: 0.6891 - accuracy: 0.5233\n",
      "Epoch 10/60\n",
      "27/27 - 0s - loss: 0.6901 - accuracy: 0.5012\n",
      "Epoch 11/60\n",
      "27/27 - 0s - loss: 0.6891 - accuracy: 0.5186\n",
      "Epoch 12/60\n",
      "27/27 - 0s - loss: 0.6898 - accuracy: 0.5233\n",
      "Epoch 13/60\n",
      "27/27 - 0s - loss: 0.6890 - accuracy: 0.5186\n",
      "Epoch 14/60\n",
      "27/27 - 0s - loss: 0.6890 - accuracy: 0.5186\n",
      "Epoch 15/60\n",
      "27/27 - 0s - loss: 0.6901 - accuracy: 0.5256\n",
      "Epoch 16/60\n",
      "27/27 - 0s - loss: 0.6920 - accuracy: 0.5035\n",
      "Epoch 17/60\n",
      "27/27 - 0s - loss: 0.6899 - accuracy: 0.5256\n",
      "Epoch 18/60\n",
      "27/27 - 0s - loss: 0.6885 - accuracy: 0.5082\n",
      "Epoch 19/60\n",
      "27/27 - 0s - loss: 0.6885 - accuracy: 0.5047\n",
      "Epoch 20/60\n",
      "27/27 - 0s - loss: 0.6880 - accuracy: 0.5280\n",
      "Epoch 21/60\n",
      "27/27 - 0s - loss: 0.6884 - accuracy: 0.5198\n",
      "Epoch 22/60\n",
      "27/27 - 0s - loss: 0.6887 - accuracy: 0.5291\n",
      "Epoch 23/60\n",
      "27/27 - 0s - loss: 0.6885 - accuracy: 0.5245\n",
      "Epoch 24/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5326\n",
      "Epoch 25/60\n",
      "27/27 - 0s - loss: 0.6896 - accuracy: 0.5140\n",
      "Epoch 26/60\n",
      "27/27 - 0s - loss: 0.6882 - accuracy: 0.5326\n",
      "Epoch 27/60\n",
      "27/27 - 0s - loss: 0.6939 - accuracy: 0.5128\n",
      "Epoch 28/60\n",
      "27/27 - 0s - loss: 0.6961 - accuracy: 0.5128\n",
      "Epoch 29/60\n",
      "27/27 - 0s - loss: 0.6923 - accuracy: 0.5093\n",
      "Epoch 30/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5233\n",
      "Epoch 31/60\n",
      "27/27 - 0s - loss: 0.6900 - accuracy: 0.5023\n",
      "Epoch 32/60\n",
      "27/27 - 0s - loss: 0.6894 - accuracy: 0.5280\n",
      "Epoch 33/60\n",
      "27/27 - 0s - loss: 0.6884 - accuracy: 0.5245\n",
      "Epoch 34/60\n",
      "27/27 - 0s - loss: 0.6899 - accuracy: 0.5210\n",
      "Epoch 35/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5326\n",
      "Epoch 36/60\n",
      "27/27 - 0s - loss: 0.6886 - accuracy: 0.5117\n",
      "Epoch 37/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5198\n",
      "Epoch 38/60\n",
      "27/27 - 0s - loss: 0.6888 - accuracy: 0.5221\n",
      "Epoch 39/60\n",
      "27/27 - 0s - loss: 0.6893 - accuracy: 0.5373\n",
      "Epoch 40/60\n",
      "27/27 - 0s - loss: 0.6896 - accuracy: 0.5012\n",
      "Epoch 41/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5035\n",
      "Epoch 42/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5280\n",
      "Epoch 43/60\n",
      "27/27 - 0s - loss: 0.6895 - accuracy: 0.5152\n",
      "Epoch 44/60\n",
      "27/27 - 0s - loss: 0.6903 - accuracy: 0.5221\n",
      "Epoch 45/60\n",
      "27/27 - 0s - loss: 0.6888 - accuracy: 0.5326\n",
      "Epoch 46/60\n",
      "27/27 - 0s - loss: 0.6891 - accuracy: 0.5128\n",
      "Epoch 47/60\n",
      "27/27 - 0s - loss: 0.6898 - accuracy: 0.5256\n",
      "Epoch 48/60\n",
      "27/27 - 0s - loss: 0.6895 - accuracy: 0.5186\n",
      "Epoch 49/60\n",
      "27/27 - 0s - loss: 0.6890 - accuracy: 0.5361\n",
      "Epoch 50/60\n",
      "27/27 - 0s - loss: 0.6882 - accuracy: 0.5338\n",
      "Epoch 51/60\n",
      "27/27 - 0s - loss: 0.6889 - accuracy: 0.5210\n",
      "Epoch 52/60\n",
      "27/27 - 0s - loss: 0.6890 - accuracy: 0.5233\n",
      "Epoch 53/60\n",
      "27/27 - 0s - loss: 0.6890 - accuracy: 0.5117\n",
      "Epoch 54/60\n",
      "27/27 - 0s - loss: 0.6891 - accuracy: 0.5198\n",
      "Epoch 55/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5303\n",
      "Epoch 56/60\n",
      "27/27 - 0s - loss: 0.6896 - accuracy: 0.5082\n",
      "Epoch 57/60\n",
      "27/27 - 0s - loss: 0.6885 - accuracy: 0.4918\n",
      "Epoch 58/60\n",
      "27/27 - 0s - loss: 0.6887 - accuracy: 0.5280\n",
      "Epoch 59/60\n",
      "27/27 - 0s - loss: 0.6902 - accuracy: 0.5058\n",
      "Epoch 60/60\n",
      "27/27 - 0s - loss: 0.6905 - accuracy: 0.5233\n",
      "9/9 - 0s - loss: 0.7046 - accuracy: 0.4634\n",
      "----------------------------------------------------------------------------------------\n",
      "NKE: Normal Neural Network - Loss: 0.7046113610267639, Accuracy: 0.46341463923454285\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C13C38D08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT'\n",
      " 'ADD MONEY']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "27/27 - 0s - loss: 0.6948 - accuracy: 0.5070\n",
      "Epoch 2/60\n",
      "27/27 - 0s - loss: 0.6999 - accuracy: 0.5175\n",
      "Epoch 3/60\n",
      "27/27 - 0s - loss: 0.6933 - accuracy: 0.5420\n",
      "Epoch 4/60\n",
      "27/27 - 0s - loss: 0.6971 - accuracy: 0.5070\n",
      "Epoch 5/60\n",
      "27/27 - 0s - loss: 0.6922 - accuracy: 0.5373\n",
      "Epoch 6/60\n",
      "27/27 - 0s - loss: 0.6946 - accuracy: 0.5140\n",
      "Epoch 7/60\n",
      "27/27 - 0s - loss: 0.6937 - accuracy: 0.5163\n",
      "Epoch 8/60\n",
      "27/27 - 0s - loss: 0.6939 - accuracy: 0.5163\n",
      "Epoch 9/60\n",
      "27/27 - 0s - loss: 0.6950 - accuracy: 0.5140\n",
      "Epoch 10/60\n",
      "27/27 - 0s - loss: 0.6944 - accuracy: 0.5350\n",
      "Epoch 11/60\n",
      "27/27 - 0s - loss: 0.6929 - accuracy: 0.5163\n",
      "Epoch 12/60\n",
      "27/27 - 0s - loss: 0.6911 - accuracy: 0.5408\n",
      "Epoch 13/60\n",
      "27/27 - 0s - loss: 0.6965 - accuracy: 0.5105\n",
      "Epoch 14/60\n",
      "27/27 - 0s - loss: 0.6922 - accuracy: 0.5361\n",
      "Epoch 15/60\n",
      "27/27 - 0s - loss: 0.6901 - accuracy: 0.5315\n",
      "Epoch 16/60\n",
      "27/27 - 0s - loss: 0.7041 - accuracy: 0.5105\n",
      "Epoch 17/60\n",
      "27/27 - 0s - loss: 0.6919 - accuracy: 0.5175\n",
      "Epoch 18/60\n",
      "27/27 - 0s - loss: 0.6903 - accuracy: 0.5443\n",
      "Epoch 19/60\n",
      "27/27 - 0s - loss: 0.6903 - accuracy: 0.5513\n",
      "Epoch 20/60\n",
      "27/27 - 0s - loss: 0.6900 - accuracy: 0.5152\n",
      "Epoch 21/60\n",
      "27/27 - 0s - loss: 0.6889 - accuracy: 0.5361\n",
      "Epoch 22/60\n",
      "27/27 - 0s - loss: 0.6916 - accuracy: 0.5245\n",
      "Epoch 23/60\n",
      "27/27 - 0s - loss: 0.6885 - accuracy: 0.5175\n",
      "Epoch 24/60\n",
      "27/27 - 0s - loss: 0.6976 - accuracy: 0.5117\n",
      "Epoch 25/60\n",
      "27/27 - 0s - loss: 0.6946 - accuracy: 0.5035\n",
      "Epoch 26/60\n",
      "27/27 - 0s - loss: 0.6931 - accuracy: 0.5245\n",
      "Epoch 27/60\n",
      "27/27 - 0s - loss: 0.7024 - accuracy: 0.5058\n",
      "Epoch 28/60\n",
      "27/27 - 0s - loss: 0.6925 - accuracy: 0.5047\n",
      "Epoch 29/60\n",
      "27/27 - 0s - loss: 0.6938 - accuracy: 0.5210\n",
      "Epoch 30/60\n",
      "27/27 - 0s - loss: 0.6887 - accuracy: 0.5385\n",
      "Epoch 31/60\n",
      "27/27 - 0s - loss: 0.6894 - accuracy: 0.5186\n",
      "Epoch 32/60\n",
      "27/27 - 0s - loss: 0.6951 - accuracy: 0.5303\n",
      "Epoch 33/60\n",
      "27/27 - 0s - loss: 0.6875 - accuracy: 0.5350\n",
      "Epoch 34/60\n",
      "27/27 - 0s - loss: 0.6899 - accuracy: 0.5385\n",
      "Epoch 35/60\n",
      "27/27 - 0s - loss: 0.6908 - accuracy: 0.5256\n",
      "Epoch 36/60\n",
      "27/27 - 0s - loss: 0.6926 - accuracy: 0.5233\n",
      "Epoch 37/60\n",
      "27/27 - 0s - loss: 0.6924 - accuracy: 0.5280\n",
      "Epoch 38/60\n",
      "27/27 - 0s - loss: 0.6889 - accuracy: 0.5466\n",
      "Epoch 39/60\n",
      "27/27 - 0s - loss: 0.6888 - accuracy: 0.5396\n",
      "Epoch 40/60\n",
      "27/27 - 0s - loss: 0.6873 - accuracy: 0.5385\n",
      "Epoch 41/60\n",
      "27/27 - 0s - loss: 0.6894 - accuracy: 0.5175\n",
      "Epoch 42/60\n",
      "27/27 - 0s - loss: 0.6890 - accuracy: 0.5233\n",
      "Epoch 43/60\n",
      "27/27 - 0s - loss: 0.6854 - accuracy: 0.5513\n",
      "Epoch 44/60\n",
      "27/27 - 0s - loss: 0.6941 - accuracy: 0.5105\n",
      "Epoch 45/60\n",
      "27/27 - 0s - loss: 0.6893 - accuracy: 0.5186\n",
      "Epoch 46/60\n",
      "27/27 - 0s - loss: 0.6903 - accuracy: 0.5245\n",
      "Epoch 47/60\n",
      "27/27 - 0s - loss: 0.6916 - accuracy: 0.5105\n",
      "Epoch 48/60\n",
      "27/27 - 0s - loss: 0.6874 - accuracy: 0.5245\n",
      "Epoch 49/60\n",
      "27/27 - 0s - loss: 0.6918 - accuracy: 0.5175\n",
      "Epoch 50/60\n",
      "27/27 - 0s - loss: 0.6914 - accuracy: 0.5373\n",
      "Epoch 51/60\n",
      "27/27 - 0s - loss: 0.6900 - accuracy: 0.5361\n",
      "Epoch 52/60\n",
      "27/27 - 0s - loss: 0.6886 - accuracy: 0.5280\n",
      "Epoch 53/60\n",
      "27/27 - 0s - loss: 0.6894 - accuracy: 0.5140\n",
      "Epoch 54/60\n",
      "27/27 - 0s - loss: 0.6914 - accuracy: 0.5128\n",
      "Epoch 55/60\n",
      "27/27 - 0s - loss: 0.6918 - accuracy: 0.5326\n",
      "Epoch 56/60\n",
      "27/27 - 0s - loss: 0.6879 - accuracy: 0.5221\n",
      "Epoch 57/60\n",
      "27/27 - 0s - loss: 0.6884 - accuracy: 0.5152\n",
      "Epoch 58/60\n",
      "27/27 - 0s - loss: 0.6865 - accuracy: 0.5361\n",
      "Epoch 59/60\n",
      "27/27 - 0s - loss: 0.6933 - accuracy: 0.5268\n",
      "Epoch 60/60\n",
      "27/27 - 0s - loss: 0.6914 - accuracy: 0.5338\n",
      "9/9 - 0s - loss: 0.7012 - accuracy: 0.5331\n",
      "----------------------------------------------------------------------------------------\n",
      "NKE: Normal Neural Network - Loss: 0.7012232542037964, Accuracy: 0.5331010222434998\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C145EFEA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "27/27 - 0s - loss: 0.7197 - accuracy: 0.5128\n",
      "Epoch 2/60\n",
      "27/27 - 0s - loss: 0.7083 - accuracy: 0.5047\n",
      "Epoch 3/60\n",
      "27/27 - 0s - loss: 0.6975 - accuracy: 0.4872\n",
      "Epoch 4/60\n",
      "27/27 - 0s - loss: 0.6956 - accuracy: 0.5058\n",
      "Epoch 5/60\n",
      "27/27 - 0s - loss: 0.6920 - accuracy: 0.5117\n",
      "Epoch 6/60\n",
      "27/27 - 0s - loss: 0.6913 - accuracy: 0.5128\n",
      "Epoch 7/60\n",
      "27/27 - 0s - loss: 0.6916 - accuracy: 0.5175\n",
      "Epoch 8/60\n",
      "27/27 - 0s - loss: 0.6895 - accuracy: 0.5338\n",
      "Epoch 9/60\n",
      "27/27 - 0s - loss: 0.6936 - accuracy: 0.5198\n",
      "Epoch 10/60\n",
      "27/27 - 0s - loss: 0.6904 - accuracy: 0.5280\n",
      "Epoch 11/60\n",
      "27/27 - 0s - loss: 0.6903 - accuracy: 0.5303\n",
      "Epoch 12/60\n",
      "27/27 - 0s - loss: 0.6915 - accuracy: 0.5198\n",
      "Epoch 13/60\n",
      "27/27 - 0s - loss: 0.6906 - accuracy: 0.5291\n",
      "Epoch 14/60\n",
      "27/27 - 0s - loss: 0.6900 - accuracy: 0.5128\n",
      "Epoch 15/60\n",
      "27/27 - 0s - loss: 0.6897 - accuracy: 0.5245\n",
      "Epoch 16/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.5256\n",
      "Epoch 17/60\n",
      "27/27 - 0s - loss: 0.6885 - accuracy: 0.5326\n",
      "Epoch 18/60\n",
      "27/27 - 0s - loss: 0.6880 - accuracy: 0.5210\n",
      "Epoch 19/60\n",
      "27/27 - 0s - loss: 0.6882 - accuracy: 0.5303\n",
      "Epoch 20/60\n",
      "27/27 - 0s - loss: 0.6890 - accuracy: 0.5221\n",
      "Epoch 21/60\n",
      "27/27 - 0s - loss: 0.6880 - accuracy: 0.5315\n",
      "Epoch 22/60\n",
      "27/27 - 0s - loss: 0.6882 - accuracy: 0.5361\n",
      "Epoch 23/60\n",
      "27/27 - 0s - loss: 0.6880 - accuracy: 0.5210\n",
      "Epoch 24/60\n",
      "27/27 - 0s - loss: 0.6894 - accuracy: 0.5058\n",
      "Epoch 25/60\n",
      "27/27 - 0s - loss: 0.6894 - accuracy: 0.5152\n",
      "Epoch 26/60\n",
      "27/27 - 0s - loss: 0.6944 - accuracy: 0.5070\n",
      "Epoch 27/60\n",
      "27/27 - 0s - loss: 0.6932 - accuracy: 0.5373\n",
      "Epoch 28/60\n",
      "27/27 - 0s - loss: 0.6888 - accuracy: 0.5315\n",
      "Epoch 29/60\n",
      "27/27 - 0s - loss: 0.6903 - accuracy: 0.5175\n",
      "Epoch 30/60\n",
      "27/27 - 0s - loss: 0.6889 - accuracy: 0.5198\n",
      "Epoch 31/60\n",
      "27/27 - 0s - loss: 0.6888 - accuracy: 0.5303\n",
      "Epoch 32/60\n",
      "27/27 - 0s - loss: 0.6873 - accuracy: 0.5326\n",
      "Epoch 33/60\n",
      "27/27 - 0s - loss: 0.6886 - accuracy: 0.5315\n",
      "Epoch 34/60\n",
      "27/27 - 0s - loss: 0.6900 - accuracy: 0.5303\n",
      "Epoch 35/60\n",
      "27/27 - 0s - loss: 0.6875 - accuracy: 0.5140\n",
      "Epoch 36/60\n",
      "27/27 - 0s - loss: 0.6881 - accuracy: 0.5175\n",
      "Epoch 37/60\n",
      "27/27 - 0s - loss: 0.6877 - accuracy: 0.5350\n",
      "Epoch 38/60\n",
      "27/27 - 0s - loss: 0.6868 - accuracy: 0.5385\n",
      "Epoch 39/60\n",
      "27/27 - 0s - loss: 0.6890 - accuracy: 0.5221\n",
      "Epoch 40/60\n",
      "27/27 - 0s - loss: 0.6895 - accuracy: 0.5361\n",
      "Epoch 41/60\n",
      "27/27 - 0s - loss: 0.6892 - accuracy: 0.4825\n",
      "Epoch 42/60\n",
      "27/27 - 0s - loss: 0.6905 - accuracy: 0.5186\n",
      "Epoch 43/60\n",
      "27/27 - 0s - loss: 0.6879 - accuracy: 0.5338\n",
      "Epoch 44/60\n",
      "27/27 - 0s - loss: 0.6880 - accuracy: 0.5175\n",
      "Epoch 45/60\n",
      "27/27 - 0s - loss: 0.6880 - accuracy: 0.5175\n",
      "Epoch 46/60\n",
      "27/27 - 0s - loss: 0.6879 - accuracy: 0.5105\n",
      "Epoch 47/60\n",
      "27/27 - 0s - loss: 0.6875 - accuracy: 0.5175\n",
      "Epoch 48/60\n",
      "27/27 - 0s - loss: 0.6899 - accuracy: 0.5303\n",
      "Epoch 49/60\n",
      "27/27 - 0s - loss: 0.6908 - accuracy: 0.5408\n",
      "Epoch 50/60\n",
      "27/27 - 0s - loss: 0.6886 - accuracy: 0.5385\n",
      "Epoch 51/60\n",
      "27/27 - 0s - loss: 0.6883 - accuracy: 0.5186\n",
      "Epoch 52/60\n",
      "27/27 - 0s - loss: 0.6873 - accuracy: 0.5186\n",
      "Epoch 53/60\n",
      "27/27 - 0s - loss: 0.6878 - accuracy: 0.5280\n",
      "Epoch 54/60\n",
      "27/27 - 0s - loss: 0.6882 - accuracy: 0.5186\n",
      "Epoch 55/60\n",
      "27/27 - 0s - loss: 0.6871 - accuracy: 0.5326\n",
      "Epoch 56/60\n",
      "27/27 - 0s - loss: 0.6873 - accuracy: 0.5140\n",
      "Epoch 57/60\n",
      "27/27 - 0s - loss: 0.6873 - accuracy: 0.5291\n",
      "Epoch 58/60\n",
      "27/27 - 0s - loss: 0.6884 - accuracy: 0.5186\n",
      "Epoch 59/60\n",
      "27/27 - 0s - loss: 0.6880 - accuracy: 0.5163\n",
      "Epoch 60/60\n",
      "27/27 - 0s - loss: 0.6872 - accuracy: 0.5361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 0s - loss: 0.6987 - accuracy: 0.5331\n",
      "----------------------------------------------------------------------------------------\n",
      "NKE: Normal Neural Network - Loss: 0.6987430453300476, Accuracy: 0.5331010222434998\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C14302AE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "27/27 - 0s - loss: 0.6939 - accuracy: 0.5233\n",
      "Epoch 2/60\n",
      "27/27 - 0s - loss: 0.6871 - accuracy: 0.5361\n",
      "Epoch 3/60\n",
      "27/27 - 0s - loss: 0.6875 - accuracy: 0.5396\n",
      "Epoch 4/60\n",
      "27/27 - 0s - loss: 0.6866 - accuracy: 0.5466\n",
      "Epoch 5/60\n",
      "27/27 - 0s - loss: 0.6859 - accuracy: 0.5221\n",
      "Epoch 6/60\n",
      "27/27 - 0s - loss: 0.6857 - accuracy: 0.5466\n",
      "Epoch 7/60\n",
      "27/27 - 0s - loss: 0.6857 - accuracy: 0.5280\n",
      "Epoch 8/60\n",
      "27/27 - 0s - loss: 0.6833 - accuracy: 0.5396\n",
      "Epoch 9/60\n",
      "27/27 - 0s - loss: 0.6836 - accuracy: 0.5455\n",
      "Epoch 10/60\n",
      "27/27 - 0s - loss: 0.6822 - accuracy: 0.5350\n",
      "Epoch 11/60\n",
      "27/27 - 0s - loss: 0.6826 - accuracy: 0.5490\n",
      "Epoch 12/60\n",
      "27/27 - 0s - loss: 0.6819 - accuracy: 0.5303\n",
      "Epoch 13/60\n",
      "27/27 - 0s - loss: 0.6823 - accuracy: 0.5513\n",
      "Epoch 14/60\n",
      "27/27 - 0s - loss: 0.6828 - accuracy: 0.5385\n",
      "Epoch 15/60\n",
      "27/27 - 0s - loss: 0.6827 - accuracy: 0.5490\n",
      "Epoch 16/60\n",
      "27/27 - 0s - loss: 0.6799 - accuracy: 0.5455\n",
      "Epoch 17/60\n",
      "27/27 - 0s - loss: 0.6804 - accuracy: 0.5559\n",
      "Epoch 18/60\n",
      "27/27 - 0s - loss: 0.6806 - accuracy: 0.5490\n",
      "Epoch 19/60\n",
      "27/27 - 0s - loss: 0.6829 - accuracy: 0.5326\n",
      "Epoch 20/60\n",
      "27/27 - 0s - loss: 0.6821 - accuracy: 0.5606\n",
      "Epoch 21/60\n",
      "27/27 - 0s - loss: 0.6819 - accuracy: 0.5385\n",
      "Epoch 22/60\n",
      "27/27 - 0s - loss: 0.6800 - accuracy: 0.5524\n",
      "Epoch 23/60\n",
      "27/27 - 0s - loss: 0.6797 - accuracy: 0.5501\n",
      "Epoch 24/60\n",
      "27/27 - 0s - loss: 0.6819 - accuracy: 0.5280\n",
      "Epoch 25/60\n",
      "27/27 - 0s - loss: 0.6783 - accuracy: 0.5490\n",
      "Epoch 26/60\n",
      "27/27 - 0s - loss: 0.6792 - accuracy: 0.5373\n",
      "Epoch 27/60\n",
      "27/27 - 0s - loss: 0.6806 - accuracy: 0.5641\n",
      "Epoch 28/60\n",
      "27/27 - 0s - loss: 0.6784 - accuracy: 0.5385\n",
      "Epoch 29/60\n",
      "27/27 - 0s - loss: 0.6778 - accuracy: 0.5559\n",
      "Epoch 30/60\n",
      "27/27 - 0s - loss: 0.6783 - accuracy: 0.5396\n",
      "Epoch 31/60\n",
      "27/27 - 0s - loss: 0.6772 - accuracy: 0.5536\n",
      "Epoch 32/60\n",
      "27/27 - 0s - loss: 0.6760 - accuracy: 0.5571\n",
      "Epoch 33/60\n",
      "27/27 - 0s - loss: 0.6770 - accuracy: 0.5606\n",
      "Epoch 34/60\n",
      "27/27 - 0s - loss: 0.6771 - accuracy: 0.5524\n",
      "Epoch 35/60\n",
      "27/27 - 0s - loss: 0.6763 - accuracy: 0.5606\n",
      "Epoch 36/60\n",
      "27/27 - 0s - loss: 0.6768 - accuracy: 0.5641\n",
      "Epoch 37/60\n",
      "27/27 - 0s - loss: 0.6744 - accuracy: 0.5664\n",
      "Epoch 38/60\n",
      "27/27 - 0s - loss: 0.6746 - accuracy: 0.5583\n",
      "Epoch 39/60\n",
      "27/27 - 0s - loss: 0.6768 - accuracy: 0.5664\n",
      "Epoch 40/60\n",
      "27/27 - 0s - loss: 0.6744 - accuracy: 0.5653\n",
      "Epoch 41/60\n",
      "27/27 - 0s - loss: 0.6743 - accuracy: 0.5594\n",
      "Epoch 42/60\n",
      "27/27 - 0s - loss: 0.6738 - accuracy: 0.5583\n",
      "Epoch 43/60\n",
      "27/27 - 0s - loss: 0.6725 - accuracy: 0.5618\n",
      "Epoch 44/60\n",
      "27/27 - 0s - loss: 0.6737 - accuracy: 0.5478\n",
      "Epoch 45/60\n",
      "27/27 - 0s - loss: 0.6718 - accuracy: 0.5583\n",
      "Epoch 46/60\n",
      "27/27 - 0s - loss: 0.6713 - accuracy: 0.5629\n",
      "Epoch 47/60\n",
      "27/27 - 0s - loss: 0.6728 - accuracy: 0.5571\n",
      "Epoch 48/60\n",
      "27/27 - 0s - loss: 0.6717 - accuracy: 0.5536\n",
      "Epoch 49/60\n",
      "27/27 - 0s - loss: 0.6706 - accuracy: 0.5781\n",
      "Epoch 50/60\n",
      "27/27 - 0s - loss: 0.6748 - accuracy: 0.5583\n",
      "Epoch 51/60\n",
      "27/27 - 0s - loss: 0.6731 - accuracy: 0.5548\n",
      "Epoch 52/60\n",
      "27/27 - 0s - loss: 0.6714 - accuracy: 0.5711\n",
      "Epoch 53/60\n",
      "27/27 - 0s - loss: 0.6684 - accuracy: 0.5583\n",
      "Epoch 54/60\n",
      "27/27 - 0s - loss: 0.6742 - accuracy: 0.5583\n",
      "Epoch 55/60\n",
      "27/27 - 0s - loss: 0.6690 - accuracy: 0.5758\n",
      "Epoch 56/60\n",
      "27/27 - 0s - loss: 0.6701 - accuracy: 0.5734\n",
      "Epoch 57/60\n",
      "27/27 - 0s - loss: 0.6677 - accuracy: 0.5664\n",
      "Epoch 58/60\n",
      "27/27 - 0s - loss: 0.6672 - accuracy: 0.5828\n",
      "Epoch 59/60\n",
      "27/27 - 0s - loss: 0.6706 - accuracy: 0.5455\n",
      "Epoch 60/60\n",
      "27/27 - 0s - loss: 0.6660 - accuracy: 0.5816\n",
      "9/9 - 0s - loss: 0.7494 - accuracy: 0.4948\n",
      "----------------------------------------------------------------------------------------\n",
      "NKE: Normal Neural Network - Loss: 0.7494076490402222, Accuracy: 0.4947735071182251\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C18A0F158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT' 'TAKE MONEY OUT'\n",
      " 'TAKE MONEY OUT']\n",
      "Actual Labels: ['TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY', 'ADD MONEY', 'TAKE MONEY OUT']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "(947, 4) (947,)\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.7169 - accuracy: 0.5197\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.6980 - accuracy: 0.5394\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.7070 - accuracy: 0.4944\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.6936 - accuracy: 0.5338\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.6944 - accuracy: 0.5254\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5423\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.6946 - accuracy: 0.5113\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.6922 - accuracy: 0.5254\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.6938 - accuracy: 0.5394\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.6918 - accuracy: 0.5423\n",
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.6919 - accuracy: 0.5197\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.6933 - accuracy: 0.5394\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.6956 - accuracy: 0.5183\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5268\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.6944 - accuracy: 0.5239\n",
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.6900 - accuracy: 0.5380\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5408\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5437\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.6929 - accuracy: 0.5169\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.6908 - accuracy: 0.5437\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.6911 - accuracy: 0.5408\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.6922 - accuracy: 0.5141\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.6950 - accuracy: 0.5183\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.6926 - accuracy: 0.5254\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.6935 - accuracy: 0.5408\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5408\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5437\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.6905 - accuracy: 0.5423\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5408\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.6911 - accuracy: 0.5437\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.6912 - accuracy: 0.5437\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5465\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.6920 - accuracy: 0.5394\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.6928 - accuracy: 0.5465\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.6933 - accuracy: 0.5141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5521\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.6913 - accuracy: 0.5423\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5408\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5408\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.6904 - accuracy: 0.5437\n",
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5437\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.6912 - accuracy: 0.5268\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.6909 - accuracy: 0.5451\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.6951 - accuracy: 0.5380\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.6922 - accuracy: 0.4972\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.6917 - accuracy: 0.5437\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.6929 - accuracy: 0.5141\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.6944 - accuracy: 0.5268\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5239\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.6936 - accuracy: 0.5296\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.6918 - accuracy: 0.5324\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5408\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5352\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.6935 - accuracy: 0.5408\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5535\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.6917 - accuracy: 0.5423\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.6925 - accuracy: 0.5408\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.6913 - accuracy: 0.5366\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5423\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5437\n",
      "8/8 - 0s - loss: 0.6960 - accuracy: 0.5232\n",
      "----------------------------------------------------------------------------------------\n",
      "PG: Normal Neural Network - Loss: 0.6959879994392395, Accuracy: 0.5232067704200745\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C14E42730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.7458 - accuracy: 0.4662\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.6999 - accuracy: 0.4944\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.6957 - accuracy: 0.4859\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.6947 - accuracy: 0.5141\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.6977 - accuracy: 0.5141\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.6970 - accuracy: 0.4930\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.6926 - accuracy: 0.5169\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.7036 - accuracy: 0.5070\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.6942 - accuracy: 0.5113\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.6941 - accuracy: 0.5197\n",
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.6942 - accuracy: 0.5056\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.6956 - accuracy: 0.5042\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.7131 - accuracy: 0.4577\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.7011 - accuracy: 0.4803\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.6959 - accuracy: 0.4930\n",
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.6949 - accuracy: 0.5155\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.7054 - accuracy: 0.4958\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.6967 - accuracy: 0.4944\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.6975 - accuracy: 0.4915\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.6964 - accuracy: 0.4620\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.6931 - accuracy: 0.5225\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.6920 - accuracy: 0.5127\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.6975 - accuracy: 0.5070\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.6920 - accuracy: 0.5394\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5183\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.6946 - accuracy: 0.4887\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.6913 - accuracy: 0.5155\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.6935 - accuracy: 0.5239\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.6931 - accuracy: 0.5014\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.6965 - accuracy: 0.5085\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.6931 - accuracy: 0.5155\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.6940 - accuracy: 0.5056\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5268\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.6948 - accuracy: 0.4958\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.6921 - accuracy: 0.5352\n",
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.6958 - accuracy: 0.4944\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.6928 - accuracy: 0.5352\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5408\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.6914 - accuracy: 0.5211\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.6942 - accuracy: 0.5338\n",
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5493\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.6929 - accuracy: 0.5324\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.6892 - accuracy: 0.5310\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.6931 - accuracy: 0.5268\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5296\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5592\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.6925 - accuracy: 0.5211\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.6911 - accuracy: 0.5352\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.6932 - accuracy: 0.5437\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.6970 - accuracy: 0.4986\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.6922 - accuracy: 0.5225\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.6922 - accuracy: 0.5183\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.6907 - accuracy: 0.5324\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.6898 - accuracy: 0.5352\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.6929 - accuracy: 0.5056\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.6947 - accuracy: 0.4986\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.6905 - accuracy: 0.5296\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.6914 - accuracy: 0.5113\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.6905 - accuracy: 0.5380\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.6911 - accuracy: 0.5197\n",
      "8/8 - 0s - loss: 0.6992 - accuracy: 0.5148\n",
      "----------------------------------------------------------------------------------------\n",
      "PG: Normal Neural Network - Loss: 0.6992085576057434, Accuracy: 0.5147679448127747\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C1502F488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.7123 - accuracy: 0.4915\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.7061 - accuracy: 0.5042\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.7013 - accuracy: 0.4718\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.6960 - accuracy: 0.5155\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.6990 - accuracy: 0.4986\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.6952 - accuracy: 0.5408\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.6934 - accuracy: 0.5085\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.6926 - accuracy: 0.5366\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.6963 - accuracy: 0.5042\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.6967 - accuracy: 0.5099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.6930 - accuracy: 0.5197\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.6934 - accuracy: 0.5437\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5380\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.6936 - accuracy: 0.5408\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.6919 - accuracy: 0.5549\n",
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.6912 - accuracy: 0.5451\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.6897 - accuracy: 0.5507\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5493\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.6899 - accuracy: 0.5408\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.6900 - accuracy: 0.5451\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5507\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.6904 - accuracy: 0.5394\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.6915 - accuracy: 0.5408\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.6909 - accuracy: 0.5380\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.6895 - accuracy: 0.5437\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5451\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.6904 - accuracy: 0.5437\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.6946 - accuracy: 0.5437\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5437\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5465\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.6885 - accuracy: 0.5451\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.6898 - accuracy: 0.5423\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.6904 - accuracy: 0.5437\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.6900 - accuracy: 0.5479\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5507\n",
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.6910 - accuracy: 0.5423\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.6916 - accuracy: 0.5296\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.6906 - accuracy: 0.5437\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.6899 - accuracy: 0.5437\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.6895 - accuracy: 0.5493\n",
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.6896 - accuracy: 0.5493\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.6903 - accuracy: 0.5549\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.6890 - accuracy: 0.5465\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.6884 - accuracy: 0.5563\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.6887 - accuracy: 0.5437\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.6890 - accuracy: 0.5465\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.6879 - accuracy: 0.5563\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.6895 - accuracy: 0.5521\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.6897 - accuracy: 0.5423\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.6923 - accuracy: 0.5352\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.6917 - accuracy: 0.5423\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.6886 - accuracy: 0.5507\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.6901 - accuracy: 0.5535\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.6886 - accuracy: 0.5394\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.6891 - accuracy: 0.5507\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.6893 - accuracy: 0.5620\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.6886 - accuracy: 0.5451\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.6920 - accuracy: 0.5268\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.6904 - accuracy: 0.5662\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.6891 - accuracy: 0.5451\n",
      "8/8 - 0s - loss: 0.6936 - accuracy: 0.5443\n",
      "----------------------------------------------------------------------------------------\n",
      "PG: Normal Neural Network - Loss: 0.6935630440711975, Accuracy: 0.5443037748336792\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C13E308C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'ADD MONEY' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n",
      "Epoch 1/60\n",
      "23/23 - 0s - loss: 0.6985 - accuracy: 0.5000\n",
      "Epoch 2/60\n",
      "23/23 - 0s - loss: 0.6902 - accuracy: 0.5507\n",
      "Epoch 3/60\n",
      "23/23 - 0s - loss: 0.6896 - accuracy: 0.5423\n",
      "Epoch 4/60\n",
      "23/23 - 0s - loss: 0.6885 - accuracy: 0.5465\n",
      "Epoch 5/60\n",
      "23/23 - 0s - loss: 0.6877 - accuracy: 0.5465\n",
      "Epoch 6/60\n",
      "23/23 - 0s - loss: 0.6863 - accuracy: 0.5620\n",
      "Epoch 7/60\n",
      "23/23 - 0s - loss: 0.6889 - accuracy: 0.5521\n",
      "Epoch 8/60\n",
      "23/23 - 0s - loss: 0.6905 - accuracy: 0.5479\n",
      "Epoch 9/60\n",
      "23/23 - 0s - loss: 0.6879 - accuracy: 0.5577\n",
      "Epoch 10/60\n",
      "23/23 - 0s - loss: 0.6867 - accuracy: 0.5662\n",
      "Epoch 11/60\n",
      "23/23 - 0s - loss: 0.6886 - accuracy: 0.5592\n",
      "Epoch 12/60\n",
      "23/23 - 0s - loss: 0.6883 - accuracy: 0.5634\n",
      "Epoch 13/60\n",
      "23/23 - 0s - loss: 0.6872 - accuracy: 0.5592\n",
      "Epoch 14/60\n",
      "23/23 - 0s - loss: 0.6873 - accuracy: 0.5718\n",
      "Epoch 15/60\n",
      "23/23 - 0s - loss: 0.6879 - accuracy: 0.5620\n",
      "Epoch 16/60\n",
      "23/23 - 0s - loss: 0.6860 - accuracy: 0.5718\n",
      "Epoch 17/60\n",
      "23/23 - 0s - loss: 0.6865 - accuracy: 0.5493\n",
      "Epoch 18/60\n",
      "23/23 - 0s - loss: 0.6877 - accuracy: 0.5648\n",
      "Epoch 19/60\n",
      "23/23 - 0s - loss: 0.6857 - accuracy: 0.5690\n",
      "Epoch 20/60\n",
      "23/23 - 0s - loss: 0.6851 - accuracy: 0.5662\n",
      "Epoch 21/60\n",
      "23/23 - 0s - loss: 0.6854 - accuracy: 0.5718\n",
      "Epoch 22/60\n",
      "23/23 - 0s - loss: 0.6842 - accuracy: 0.5732\n",
      "Epoch 23/60\n",
      "23/23 - 0s - loss: 0.6860 - accuracy: 0.5648\n",
      "Epoch 24/60\n",
      "23/23 - 0s - loss: 0.6829 - accuracy: 0.5704\n",
      "Epoch 25/60\n",
      "23/23 - 0s - loss: 0.6832 - accuracy: 0.5775\n",
      "Epoch 26/60\n",
      "23/23 - 0s - loss: 0.6828 - accuracy: 0.5775\n",
      "Epoch 27/60\n",
      "23/23 - 0s - loss: 0.6837 - accuracy: 0.5789\n",
      "Epoch 28/60\n",
      "23/23 - 0s - loss: 0.6848 - accuracy: 0.5648\n",
      "Epoch 29/60\n",
      "23/23 - 0s - loss: 0.6817 - accuracy: 0.5676\n",
      "Epoch 30/60\n",
      "23/23 - 0s - loss: 0.6816 - accuracy: 0.5803\n",
      "Epoch 31/60\n",
      "23/23 - 0s - loss: 0.6805 - accuracy: 0.5789\n",
      "Epoch 32/60\n",
      "23/23 - 0s - loss: 0.6822 - accuracy: 0.5704\n",
      "Epoch 33/60\n",
      "23/23 - 0s - loss: 0.6818 - accuracy: 0.5761\n",
      "Epoch 34/60\n",
      "23/23 - 0s - loss: 0.6786 - accuracy: 0.5817\n",
      "Epoch 35/60\n",
      "23/23 - 0s - loss: 0.6797 - accuracy: 0.5831\n",
      "Epoch 36/60\n",
      "23/23 - 0s - loss: 0.6804 - accuracy: 0.5775\n",
      "Epoch 37/60\n",
      "23/23 - 0s - loss: 0.6788 - accuracy: 0.5789\n",
      "Epoch 38/60\n",
      "23/23 - 0s - loss: 0.6776 - accuracy: 0.5845\n",
      "Epoch 39/60\n",
      "23/23 - 0s - loss: 0.6789 - accuracy: 0.5789\n",
      "Epoch 40/60\n",
      "23/23 - 0s - loss: 0.6775 - accuracy: 0.5775\n",
      "Epoch 41/60\n",
      "23/23 - 0s - loss: 0.6763 - accuracy: 0.5817\n",
      "Epoch 42/60\n",
      "23/23 - 0s - loss: 0.6748 - accuracy: 0.5859\n",
      "Epoch 43/60\n",
      "23/23 - 0s - loss: 0.6732 - accuracy: 0.5789\n",
      "Epoch 44/60\n",
      "23/23 - 0s - loss: 0.6709 - accuracy: 0.5873\n",
      "Epoch 45/60\n",
      "23/23 - 0s - loss: 0.6709 - accuracy: 0.5901\n",
      "Epoch 46/60\n",
      "23/23 - 0s - loss: 0.6827 - accuracy: 0.5634\n",
      "Epoch 47/60\n",
      "23/23 - 0s - loss: 0.6784 - accuracy: 0.5746\n",
      "Epoch 48/60\n",
      "23/23 - 0s - loss: 0.6766 - accuracy: 0.5845\n",
      "Epoch 49/60\n",
      "23/23 - 0s - loss: 0.6758 - accuracy: 0.5732\n",
      "Epoch 50/60\n",
      "23/23 - 0s - loss: 0.6719 - accuracy: 0.5831\n",
      "Epoch 51/60\n",
      "23/23 - 0s - loss: 0.6725 - accuracy: 0.5803\n",
      "Epoch 52/60\n",
      "23/23 - 0s - loss: 0.6708 - accuracy: 0.5803\n",
      "Epoch 53/60\n",
      "23/23 - 0s - loss: 0.6726 - accuracy: 0.5887\n",
      "Epoch 54/60\n",
      "23/23 - 0s - loss: 0.6687 - accuracy: 0.5944\n",
      "Epoch 55/60\n",
      "23/23 - 0s - loss: 0.6727 - accuracy: 0.5859\n",
      "Epoch 56/60\n",
      "23/23 - 0s - loss: 0.6696 - accuracy: 0.5859\n",
      "Epoch 57/60\n",
      "23/23 - 0s - loss: 0.6671 - accuracy: 0.5845\n",
      "Epoch 58/60\n",
      "23/23 - 0s - loss: 0.6667 - accuracy: 0.5930\n",
      "Epoch 59/60\n",
      "23/23 - 0s - loss: 0.6669 - accuracy: 0.5803\n",
      "Epoch 60/60\n",
      "23/23 - 0s - loss: 0.6648 - accuracy: 0.5831\n",
      "8/8 - 0s - loss: 0.7209 - accuracy: 0.5232\n",
      "----------------------------------------------------------------------------------------\n",
      "PG: Normal Neural Network - Loss: 0.7209182977676392, Accuracy: 0.5232067704200745\n",
      "----------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023C147B42F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \n",
      "######### -----  PREDICTION TIME!!!!! ------ ######### \n",
      "     \n",
      "Predicted classes: ['ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY' 'TAKE MONEY OUT' 'ADD MONEY']\n",
      "Actual Labels: ['ADD MONEY', 'TAKE MONEY OUT', 'TAKE MONEY OUT', 'ADD MONEY', 'ADD MONEY']\n",
      "     \n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "AAPL_Deep = deep_learning('AAPL',aapl_complete)\n",
    "TSLA_Deep = deep_learning('TSLA',tsla_complete)\n",
    "AMZN_Deep = deep_learning('AMZN',amzn_complete)\n",
    "SPY_Deep = deep_learning('SPY',spy_complete)\n",
    "DOCU_Deep = deep_learning('DOCU',docu_complete)\n",
    "NFLX_Deep = deep_learning('NFLX',nflx_complete)\n",
    "NKE_Deep = deep_learning('NKE',nke_complete)\n",
    "PG_Deep = deep_learning('PG',pg_complete)\n",
    "\n",
    "NN_SUMMARY = pd.concat([AAPL_Deep,TSLA_Deep,\n",
    "                        AMZN_Deep,SPY_Deep,\n",
    "                       DOCU_Deep,NFLX_Deep,\n",
    "                       NKE_Deep,PG_Deep]).reset_index(drop=True)\n",
    "\n",
    "NN_SUMMARY = NN_SUMMARY.set_index('TICKER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NN - linear</th>\n",
       "      <th>NN - sigmoid</th>\n",
       "      <th>NN - tanh</th>\n",
       "      <th>NN - relu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TICKER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.541254</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.541254</td>\n",
       "      <td>0.478548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.488834</td>\n",
       "      <td>0.501241</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.514768</td>\n",
       "      <td>0.527426</td>\n",
       "      <td>0.527426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.468085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOCU</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.514113</td>\n",
       "      <td>0.493952</td>\n",
       "      <td>0.506048</td>\n",
       "      <td>0.465726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NKE</th>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.533101</td>\n",
       "      <td>0.533101</td>\n",
       "      <td>0.494774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG</th>\n",
       "      <td>0.523207</td>\n",
       "      <td>0.514768</td>\n",
       "      <td>0.544304</td>\n",
       "      <td>0.523207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NN - linear  NN - sigmoid  NN - tanh  NN - relu\n",
       "TICKER                                                 \n",
       "AAPL       0.541254      0.521452   0.541254   0.478548\n",
       "TSLA       0.538462      0.488834   0.501241   0.461538\n",
       "AMZN       0.493671      0.514768   0.527426   0.527426\n",
       "SPY        0.595745      0.595745   0.617021   0.468085\n",
       "DOCU       0.588235      0.529412   0.411765   0.352941\n",
       "NFLX       0.514113      0.493952   0.506048   0.465726\n",
       "NKE        0.463415      0.533101   0.533101   0.494774\n",
       "PG         0.523207      0.514768   0.544304   0.523207"
      ]
     },
     "execution_count": 746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='7253' style='display: table; margin: 0 auto;'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"e7a62c8a-edff-4d9f-8855-9de7953aeb6f\" data-root-id=\"7253\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"00087ae9-4430-4a15-9f2d-ab61036c9561\":{\"roots\":{\"references\":[{\"attributes\":{\"align\":null,\"below\":[{\"id\":\"7262\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"7265\",\"type\":\"Grid\"},{\"id\":\"7270\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"7266\",\"type\":\"LinearAxis\"}],\"margin\":null,\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"plot_height\":300,\"plot_width\":700,\"renderers\":[{\"id\":\"7290\",\"type\":\"GlyphRenderer\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"7254\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"7276\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"7250\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"7258\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"7251\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"7260\",\"type\":\"LinearScale\"}},\"id\":\"7253\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"data_source\":{\"id\":\"7284\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7287\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"7289\",\"type\":\"VBar\"},\"nonselection_glyph\":{\"id\":\"7288\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"7291\",\"type\":\"CDSView\"}},\"id\":\"7290\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"7293\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"7258\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"7260\",\"type\":\"LinearScale\"},{\"attributes\":{\"text\":\"Neurel Network\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12pt\"}},\"id\":\"7254\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"7271\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7283\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7288\",\"type\":\"VBar\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"7267\",\"type\":\"BasicTicker\"}},\"id\":\"7270\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"7263\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"callback\":null,\"end\":0.6170212626457214,\"reset_end\":0.6170212626457214,\"reset_start\":0.0,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"7251\",\"type\":\"Range1d\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"7290\",\"type\":\"GlyphRenderer\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"TICKER\",\"@{TICKER}\"],[\"Variable\",\"@{Variable}\"],[\"value\",\"@{value}\"]]},\"id\":\"7252\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"7295\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"7285\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"factors\":[[\"AAPL\",\"NN - linear\"],[\"AAPL\",\"NN - relu\"],[\"AAPL\",\"NN - sigmoid\"],[\"AAPL\",\"NN - tanh\"],[\"AMZN\",\"NN - linear\"],[\"AMZN\",\"NN - relu\"],[\"AMZN\",\"NN - sigmoid\"],[\"AMZN\",\"NN - tanh\"],[\"DOCU\",\"NN - linear\"],[\"DOCU\",\"NN - relu\"],[\"DOCU\",\"NN - sigmoid\"],[\"DOCU\",\"NN - tanh\"],[\"NFLX\",\"NN - linear\"],[\"NFLX\",\"NN - relu\"],[\"NFLX\",\"NN - sigmoid\"],[\"NFLX\",\"NN - tanh\"],[\"NKE\",\"NN - linear\"],[\"NKE\",\"NN - relu\"],[\"NKE\",\"NN - sigmoid\"],[\"NKE\",\"NN - tanh\"],[\"PG\",\"NN - linear\"],[\"PG\",\"NN - relu\"],[\"PG\",\"NN - sigmoid\"],[\"PG\",\"NN - tanh\"],[\"SPY\",\"NN - linear\"],[\"SPY\",\"NN - relu\"],[\"SPY\",\"NN - sigmoid\"],[\"SPY\",\"NN - tanh\"],[\"TSLA\",\"NN - linear\"],[\"TSLA\",\"NN - relu\"],[\"TSLA\",\"NN - sigmoid\"],[\"TSLA\",\"NN - tanh\"]],\"tags\":[[[\"TICKER\",\"TICKER\",null],[\"Variable\",\"Variable\",null]]]},\"id\":\"7250\",\"type\":\"FactorRange\"},{\"attributes\":{\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7283\",\"type\":\"CategoricalColorMapper\"}},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7287\",\"type\":\"VBar\"},{\"attributes\":{\"factors\":[\"NN - linear\",\"NN - sigmoid\",\"NN - tanh\",\"NN - relu\"],\"palette\":[\"#1f77b3\",\"#ff7e0e\",\"#2ba02b\",\"#d62628\"]},\"id\":\"7283\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{},\"id\":\"7275\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"7267\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"7303\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7283\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7289\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"7273\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"axis_label\":\"TICKER, Variable\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7293\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7263\",\"type\":\"CategoricalTicker\"}},\"id\":\"7262\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"7272\",\"type\":\"PanTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"7301\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"overlay\":{\"id\":\"7301\",\"type\":\"BoxAnnotation\"}},\"id\":\"7274\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"7263\",\"type\":\"CategoricalTicker\"}},\"id\":\"7265\",\"type\":\"Grid\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"7252\",\"type\":\"HoverTool\"},{\"id\":\"7271\",\"type\":\"SaveTool\"},{\"id\":\"7272\",\"type\":\"PanTool\"},{\"id\":\"7273\",\"type\":\"WheelZoomTool\"},{\"id\":\"7274\",\"type\":\"BoxZoomTool\"},{\"id\":\"7275\",\"type\":\"ResetTool\"}]},\"id\":\"7276\",\"type\":\"Toolbar\"},{\"attributes\":{\"axis_label\":\"\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7295\",\"type\":\"BasicTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7267\",\"type\":\"BasicTicker\"}},\"id\":\"7266\",\"type\":\"LinearAxis\"},{\"attributes\":{\"source\":{\"id\":\"7284\",\"type\":\"ColumnDataSource\"}},\"id\":\"7291\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\",\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\",\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\",\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\"],\"value\":{\"__ndarray__\":\"AAAAIPRR4T8AAADAEzvhPwAAAMBNmN8/AAAAIFcQ4z8AAADg0tLiPwAAAOCcc+A/AAAA4JWo3T8AAAAgHL7gPwAAAGC8r+A/AAAAYA1J3z8AAACg+njgPwAAACBXEOM/AAAAAPHw4D8AAABA55zfPwAAAOApD+E/AAAAoPp44D8AAAAg9FHhPwAAAOApCuA/AAAA4Kzg4D8AAABgo77jPwAAAGBaWto/AAAAYIwx4D8AAADgKQ/hPwAAAMDvauE/AAAAIIeg3j8AAACg2IndPwAAAOCs4OA/AAAAQBv13T8AAACglpbWPwAAAKBzzt0/AAAAgF6q3z8AAAAgHL7gPw==\",\"dtype\":\"float64\",\"shape\":[32]},\"xoffsets\":[[\"AAPL\",\"NN - linear\"],[\"TSLA\",\"NN - linear\"],[\"AMZN\",\"NN - linear\"],[\"SPY\",\"NN - linear\"],[\"DOCU\",\"NN - linear\"],[\"NFLX\",\"NN - linear\"],[\"NKE\",\"NN - linear\"],[\"PG\",\"NN - linear\"],[\"AAPL\",\"NN - sigmoid\"],[\"TSLA\",\"NN - sigmoid\"],[\"AMZN\",\"NN - sigmoid\"],[\"SPY\",\"NN - sigmoid\"],[\"DOCU\",\"NN - sigmoid\"],[\"NFLX\",\"NN - sigmoid\"],[\"NKE\",\"NN - sigmoid\"],[\"PG\",\"NN - sigmoid\"],[\"AAPL\",\"NN - tanh\"],[\"TSLA\",\"NN - tanh\"],[\"AMZN\",\"NN - tanh\"],[\"SPY\",\"NN - tanh\"],[\"DOCU\",\"NN - tanh\"],[\"NFLX\",\"NN - tanh\"],[\"NKE\",\"NN - tanh\"],[\"PG\",\"NN - tanh\"],[\"AAPL\",\"NN - relu\"],[\"TSLA\",\"NN - relu\"],[\"AMZN\",\"NN - relu\"],[\"SPY\",\"NN - relu\"],[\"DOCU\",\"NN - relu\"],[\"NFLX\",\"NN - relu\"],[\"NKE\",\"NN - relu\"],[\"PG\",\"NN - relu\"]]},\"selected\":{\"id\":\"7285\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"7303\",\"type\":\"UnionRenderers\"}},\"id\":\"7284\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"7253\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"00087ae9-4430-4a15-9f2d-ab61036c9561\",\"roots\":{\"7253\":\"e7a62c8a-edff-4d9f-8855-9de7953aeb6f\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":Bars   [TICKER,Variable]   (value)"
      ]
     },
     "execution_count": 747,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "7253"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_SUMMARY.hvplot.bar(label = \"Neurel Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine\n",
    "def svm(tickerName, df):\n",
    "    target = df[\"ACTION\"]\n",
    "    target_names = [\"negative\", \"positive\"]\n",
    "    \n",
    "    test2=df\n",
    "    data = df.drop(columns=[\"ACTION\",'close','predicted pct change','positive','neutral','negative','compound'], axis=1).dropna()\n",
    "    feature_names = data.columns\n",
    "    print(data)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)\n",
    "    \n",
    "    # Support vector machine linear classifier\n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    kernels = ['linear','rbf','poly','sigmoid']\n",
    "\n",
    "    title_sent = {\"TICKER\": tickerName}\n",
    "\n",
    "    df2 = pd.DataFrame(title_sent,index=[0])\n",
    "    \n",
    "    for kernel in kernels:\n",
    "        result = []\n",
    "        model = SVC(kernel=kernel)\n",
    "        model.fit(X_train, y_train)\n",
    "        print('-------- -----------------------------')\n",
    "        # Model Accuracy\n",
    "        print('Test Acc: %.3f' % model.score(X_test, y_test))\n",
    "        print('-------------------------------------')\n",
    "\n",
    "        # Calculate classification report\n",
    "        from sklearn.metrics import classification_report\n",
    "        predictions = model.predict(X_test)\n",
    "        print(classification_report(y_test, predictions,\n",
    "                                target_names=target_names))\n",
    "        print('***')\n",
    "        print(result)\n",
    "        print('***')\n",
    "        print(df2)\n",
    "        result.append(model.score(X_test, y_test))\n",
    "        print(result)\n",
    "        df2[f\"SVM - {kernel}\"] = result\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine - USE ALL THE COLUMNS\n",
    "def svm_all(tickerName, df):\n",
    "    target = df[\"ACTION\"]\n",
    "    target_names = [\"negative\", \"positive\"]\n",
    "    \n",
    "    test2=df\n",
    "    data = df.drop(columns=[\"ACTION\"], axis=1).dropna()\n",
    "    feature_names = data.columns\n",
    "    print(data)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)\n",
    "    \n",
    "    # Support vector machine linear classifier\n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    kernels = ['linear','rbf','sigmoid']\n",
    "\n",
    "    title_sent = {\"TICKER\": tickerName}\n",
    "\n",
    "    df2 = pd.DataFrame(title_sent,index=[0])\n",
    "    \n",
    "    for kernel in kernels:\n",
    "        result = []\n",
    "        model = SVC(kernel=kernel)\n",
    "        model.fit(X_train, y_train)\n",
    "        print('-------- -----------------------------')\n",
    "        # Model Accuracy\n",
    "        print('Test Acc: %.3f' % model.score(X_test, y_test))\n",
    "        print('-------------------------------------')\n",
    "\n",
    "        # Calculate classification report\n",
    "        from sklearn.metrics import classification_report\n",
    "        predictions = model.predict(X_test)\n",
    "        print(classification_report(y_test, predictions,\n",
    "                                target_names=target_names))\n",
    "        print('***')\n",
    "        print(result)\n",
    "        print('***')\n",
    "        print(df2)\n",
    "        result.append(model.score(X_test, y_test))\n",
    "        print(result)\n",
    "        df2[f\"SVM - {kernel}\"] = result\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            compound  positive   neutral  negative     close  \\\n",
      "Date                                                           \n",
      "2020-11-16  0.074000  0.071750  0.928250  0.000000  408.0900   \n",
      "2020-11-13  0.161560  0.132800  0.834000  0.033200  406.7800   \n",
      "2020-11-12  0.171640  0.108800  0.891200  0.000000  411.7300   \n",
      "2020-11-11  0.000000  0.000000  1.000000  0.000000  417.1300   \n",
      "2020-11-10  0.113333  0.055667  0.944333  0.000000  410.4100   \n",
      "2020-11-09 -0.173580  0.000000  0.918800  0.081200  421.3900   \n",
      "2020-11-06 -0.016700  0.043500  0.908000  0.048500  429.9200   \n",
      "2020-11-05 -0.024850  0.070000  0.830833  0.099167  438.0900   \n",
      "2020-11-04  0.134160  0.168600  0.766600  0.064800  420.9100   \n",
      "2020-11-03  0.219300  0.139800  0.860200  0.000000  423.7400   \n",
      "2020-11-02  0.083700  0.046000  0.954000  0.000000  400.5100   \n",
      "2020-10-30 -0.002220  0.058200  0.875000  0.066800  388.0400   \n",
      "2020-10-29 -0.051880  0.035800  0.893600  0.070600  410.6400   \n",
      "2020-10-28  0.000000  0.000000  1.000000  0.000000  406.0000   \n",
      "2020-10-27  0.023033  0.104000  0.813167  0.082833  424.4710   \n",
      "2020-10-26 -0.078000  0.037500  0.881000  0.081500  420.2800   \n",
      "2020-10-23  0.094983  0.066500  0.907667  0.025667  420.6150   \n",
      "2020-10-22  0.097465  0.065118  0.934882  0.000000  425.9000   \n",
      "2020-10-21  0.135636  0.103643  0.872571  0.023786  422.5000   \n",
      "2020-10-20  0.068437  0.091000  0.859000  0.050000  421.9700   \n",
      "2020-10-19  0.008712  0.014375  0.969625  0.016000  430.8300   \n",
      "2020-10-16  0.050575  0.038250  0.961750  0.000000  439.9100   \n",
      "2020-10-15  0.161920  0.085300  0.914700  0.000000  448.8800   \n",
      "2020-10-14  0.112100  0.096500  0.857875  0.045625  461.4700   \n",
      "2020-10-13  0.271725  0.129000  0.871000  0.000000  446.6500   \n",
      "2020-10-12  0.161100  0.103167  0.857167  0.039667  442.0000   \n",
      "2020-10-09  0.382200  0.259000  0.702333  0.038667  433.9600   \n",
      "2020-10-08  0.000000  0.000000  1.000000  0.000000  425.9200   \n",
      "2020-10-07  0.133400  0.129286  0.826000  0.044714  425.1900   \n",
      "2020-10-06  0.000000  0.000000  1.000000  0.000000  413.7001   \n",
      "...              ...       ...       ...       ...       ...   \n",
      "2011-04-05  0.125000  0.111000  0.889000  0.000000   26.7000   \n",
      "2011-03-31  0.020100  0.119667  0.769333  0.111000   27.6900   \n",
      "2011-02-22  0.000000  0.000000  1.000000  0.000000   21.8800   \n",
      "2011-02-16  0.148000  0.134000  0.866000  0.000000   24.7100   \n",
      "2011-02-15 -0.095929  0.093714  0.773857  0.132429   22.8500   \n",
      "2011-02-14  0.000000  0.000000  1.000000  0.000000   23.0600   \n",
      "2011-02-09  0.557400  0.535000  0.465000  0.000000   23.2100   \n",
      "2011-01-27  0.000000  0.000000  1.000000  0.000000   24.9126   \n",
      "2010-12-27 -0.097200  0.125500  0.650000  0.224750   25.5500   \n",
      "2010-12-23  0.148000  0.134000  0.866000  0.000000   30.0900   \n",
      "2010-12-02 -0.571900  0.187000  0.220000  0.593000   32.2500   \n",
      "2010-11-23  0.226300  0.241000  0.759000  0.000000   34.5999   \n",
      "2010-11-10  0.148000  0.134000  0.866000  0.000000   29.3200   \n",
      "2010-11-09 -0.171560  0.077600  0.730800  0.191600   24.6900   \n",
      "2010-11-04  0.000000  0.000000  1.000000  0.000000   24.9000   \n",
      "2010-11-03  0.000000  0.000000  1.000000  0.000000   21.8000   \n",
      "2010-10-13  0.000000  0.000000  1.000000  0.000000   20.4800   \n",
      "2010-10-08  0.000000  0.000000  1.000000  0.000000   20.4000   \n",
      "2010-08-04 -0.107975  0.140000  0.708500  0.151500   21.1710   \n",
      "2010-07-27  0.000000  0.000000  1.000000  0.000000   20.4900   \n",
      "2010-07-21  0.000000  0.000000  1.000000  0.000000   20.1200   \n",
      "2010-07-15  0.000000  0.000000  1.000000  0.000000   19.8400   \n",
      "2010-07-14  0.177900  0.175000  0.825000  0.000000   19.7100   \n",
      "2010-07-08  0.000000  0.000000  1.000000  0.000000   17.4600   \n",
      "2010-07-07  0.000000  0.000000  1.000000  0.000000   15.7400   \n",
      "2010-07-06  0.219067  0.174333  0.825667  0.000000   15.9800   \n",
      "2010-07-02  0.101150  0.132500  0.867500  0.000000   19.2000   \n",
      "2010-07-01  0.025800  0.237000  0.538000  0.226000   21.9000   \n",
      "2010-06-30  0.030320  0.109000  0.837800  0.053200   23.6300   \n",
      "2010-06-29  0.193880  0.173600  0.826400  0.000000   23.9400   \n",
      "\n",
      "            predicted pct change SCORE  \n",
      "Date                                    \n",
      "2020-11-16              0.082041     1  \n",
      "2020-11-13              0.003220     1  \n",
      "2020-11-12             -0.012022     1  \n",
      "2020-11-11             -0.012946     0  \n",
      "2020-11-10              0.016374     1  \n",
      "2020-11-09             -0.026057    -1  \n",
      "2020-11-06             -0.019841     0  \n",
      "2020-11-05             -0.018649     0  \n",
      "2020-11-04              0.040816     1  \n",
      "2020-11-03             -0.006679     1  \n",
      "2020-11-02              0.058001     1  \n",
      "2020-10-30              0.032136     0  \n",
      "2020-10-29             -0.055036    -1  \n",
      "2020-10-28              0.011429     0  \n",
      "2020-10-27             -0.043515     0  \n",
      "2020-10-26              0.009972    -1  \n",
      "2020-10-23             -0.000796     1  \n",
      "2020-10-22             -0.012409     1  \n",
      "2020-10-21              0.008047     1  \n",
      "2020-10-20              0.001256     1  \n",
      "2020-10-19             -0.020565     0  \n",
      "2020-10-16             -0.020641     1  \n",
      "2020-10-15             -0.019983     1  \n",
      "2020-10-14             -0.027282     1  \n",
      "2020-10-13              0.033180     1  \n",
      "2020-10-12              0.010520     1  \n",
      "2020-10-09              0.018527     1  \n",
      "2020-10-08              0.018877     0  \n",
      "2020-10-07              0.001717     1  \n",
      "2020-10-06              0.027774     0  \n",
      "...                          ...   ...  \n",
      "2011-04-05             -0.008614     1  \n",
      "2011-03-31             -0.036475     0  \n",
      "2011-02-22             -0.002285     0  \n",
      "2011-02-16             -0.045326     1  \n",
      "2011-02-15              0.081400    -1  \n",
      "2011-02-14             -0.009107     0  \n",
      "2011-02-09              0.002154     1  \n",
      "2011-01-27             -0.036231     0  \n",
      "2010-12-27              0.034051    -1  \n",
      "2010-12-23             -0.150881     1  \n",
      "2010-12-02             -0.026047    -1  \n",
      "2010-11-23              0.025147     1  \n",
      "2010-11-10             -0.045020     1  \n",
      "2010-11-09              0.187525    -1  \n",
      "2010-11-04             -0.018474     0  \n",
      "2010-11-03              0.142202     0  \n",
      "2010-10-13              0.013184     0  \n",
      "2010-10-08             -0.007843     0  \n",
      "2010-08-04             -0.034528    -1  \n",
      "2010-07-27              0.009761     0  \n",
      "2010-07-21              0.043738     0  \n",
      "2010-07-15              0.042339     0  \n",
      "2010-07-14              0.006596     1  \n",
      "2010-07-08             -0.007446     0  \n",
      "2010-07-07              0.109276     0  \n",
      "2010-07-06             -0.015019     1  \n",
      "2010-07-02             -0.167708     1  \n",
      "2010-07-01             -0.123288     0  \n",
      "2010-06-30             -0.073212     0  \n",
      "2010-06-29             -0.012949     1  \n",
      "\n",
      "[1612 rows x 7 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.856\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.96      0.87       201\n",
      "    positive       0.94      0.76      0.84       202\n",
      "\n",
      "    accuracy                           0.86       403\n",
      "   macro avg       0.87      0.86      0.85       403\n",
      "weighted avg       0.87      0.86      0.85       403\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   TSLA\n",
      "[0.8560794044665012]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.516\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.58      0.55       201\n",
      "    positive       0.52      0.45      0.48       202\n",
      "\n",
      "    accuracy                           0.52       403\n",
      "   macro avg       0.52      0.52      0.51       403\n",
      "weighted avg       0.52      0.52      0.51       403\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   TSLA      0.856079\n",
      "[0.5161290322580645]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.499\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      1.00      0.67       201\n",
      "    positive       0.00      0.00      0.00       202\n",
      "\n",
      "    accuracy                           0.50       403\n",
      "   macro avg       0.25      0.50      0.33       403\n",
      "weighted avg       0.25      0.50      0.33       403\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   TSLA      0.856079   0.516129\n",
      "[0.4987593052109181]\n",
      "            compound  positive   neutral  negative    close  \\\n",
      "Date                                                          \n",
      "2020-11-16  0.128853  0.112172  0.852310  0.035500  362.430   \n",
      "2020-11-13  0.113609  0.106130  0.852093  0.041759  357.330   \n",
      "2020-11-12  0.080411  0.101130  0.837167  0.061685  353.280   \n",
      "2020-11-11  0.103217  0.095655  0.865793  0.038569  356.620   \n",
      "2020-11-10  0.045819  0.078069  0.875569  0.046345  354.070   \n",
      "2020-11-09  0.128045  0.100788  0.865197  0.034030  354.540   \n",
      "2020-11-06  0.161524  0.125794  0.828838  0.045382  350.190   \n",
      "2020-11-05  0.113038  0.094945  0.864932  0.040123  350.210   \n",
      "2020-11-04  0.121262  0.107536  0.819375  0.073071  343.495   \n",
      "2020-11-03  0.076597  0.117220  0.818814  0.064000  335.970   \n",
      "2020-11-02  0.080898  0.088918  0.870429  0.040694  330.210   \n",
      "2020-10-30 -0.034044  0.073852  0.833262  0.092902  326.530   \n",
      "2020-10-29  0.133306  0.109088  0.844225  0.046700  329.990   \n",
      "2020-10-28  0.054591  0.097208  0.841887  0.060906  326.670   \n",
      "2020-10-27  0.065193  0.079194  0.882463  0.038358  338.250   \n",
      "2020-10-26 -0.048530  0.051821  0.876518  0.071643  339.415   \n",
      "2020-10-23  0.006883  0.077043  0.857894  0.065043  345.760   \n",
      "2020-10-22  0.108431  0.118000  0.826155  0.055828  344.630   \n",
      "2020-10-21  0.100582  0.097246  0.859386  0.043351  342.690   \n",
      "2020-10-20  0.116267  0.103789  0.858000  0.038211  343.340   \n",
      "2020-10-19  0.061048  0.099100  0.841360  0.059560  342.020   \n",
      "2020-10-16  0.144528  0.118022  0.857870  0.024130  347.250   \n",
      "2020-10-15  0.020327  0.074677  0.851306  0.073968  347.510   \n",
      "2020-10-14  0.127555  0.114357  0.833452  0.052167  347.970   \n",
      "2020-10-13  0.064456  0.099386  0.835474  0.065158  350.150   \n",
      "2020-10-12  0.152708  0.105590  0.870795  0.023590  352.430   \n",
      "2020-10-09  0.223365  0.141065  0.839978  0.018957  346.840   \n",
      "2020-10-08  0.041202  0.071976  0.871293  0.056732  343.730   \n",
      "2020-10-07  0.132740  0.109448  0.860655  0.029914  340.730   \n",
      "2020-10-06  0.029970  0.068089  0.872000  0.059911  334.940   \n",
      "...              ...       ...       ...       ...      ...   \n",
      "2020-04-02 -0.018401  0.078183  0.834690  0.087127  251.870   \n",
      "2020-04-01 -0.135472  0.047261  0.830304  0.122435  246.120   \n",
      "2020-03-31 -0.023419  0.082633  0.820544  0.096810  257.700   \n",
      "2020-03-30  0.093147  0.107569  0.818086  0.074310  261.680   \n",
      "2020-03-27  0.023839  0.068653  0.854547  0.076787  253.390   \n",
      "2020-03-26  0.076139  0.099576  0.842485  0.057924  261.390   \n",
      "2020-03-25  0.076760  0.107012  0.811432  0.081556  246.770   \n",
      "2020-03-24  0.105773  0.120392  0.816014  0.063608  243.590   \n",
      "2020-03-23  0.039463  0.102362  0.814032  0.083617  222.510   \n",
      "2020-03-20 -0.041146  0.081644  0.826034  0.092322  228.940   \n",
      "2020-03-17  0.648600  0.431000  0.569000  0.000000  254.190   \n",
      "2020-03-16 -0.246950  0.000000  0.894500  0.105500  239.410   \n",
      "2020-03-13  0.180600  0.166500  0.833500  0.000000  270.800   \n",
      "2020-03-11 -0.624900  0.000000  0.661000  0.339000  274.360   \n",
      "2020-03-09  0.000000  0.000000  1.000000  0.000000  274.400   \n",
      "2020-02-26  0.000000  0.000000  1.000000  0.000000  311.550   \n",
      "2020-02-07  0.000000  0.000000  1.000000  0.000000  332.200   \n",
      "2020-01-27 -0.226300  0.000000  0.872000  0.128000  323.520   \n",
      "2020-01-17 -0.296000  0.000000  0.855000  0.145000  332.010   \n",
      "2019-10-10  0.340000  0.260000  0.600000  0.140000  293.240   \n",
      "2019-10-08  0.000000  0.000000  1.000000  0.000000  288.550   \n",
      "2019-10-04  0.000000  0.000000  1.000000  0.000000  294.370   \n",
      "2019-07-18 -0.128000  0.236000  0.488000  0.276000  298.850   \n",
      "2019-06-12  0.000000  0.000000  1.000000  0.000000  288.360   \n",
      "2019-06-03 -0.340000  0.000000  0.821000  0.179000  274.540   \n",
      "2019-05-28  0.000000  0.000000  1.000000  0.000000  280.310   \n",
      "2019-05-15  0.025800  0.184000  0.640000  0.176000  285.060   \n",
      "2019-04-09  0.000000  0.000000  1.000000  0.000000  287.305   \n",
      "2019-02-01  0.025800  0.128000  0.752000  0.120000  270.070   \n",
      "2019-01-28  0.000000  0.000000  1.000000  0.000000  263.635   \n",
      "\n",
      "            predicted pct change SCORE  \n",
      "Date                                    \n",
      "2020-11-16             -0.004939     1  \n",
      "2020-11-13              0.014273     1  \n",
      "2020-11-12              0.011464     1  \n",
      "2020-11-11             -0.009366     1  \n",
      "2020-11-10              0.007202     0  \n",
      "2020-11-09             -0.001326     1  \n",
      "2020-11-06              0.012422     1  \n",
      "2020-11-05             -0.000057     1  \n",
      "2020-11-04              0.019549     1  \n",
      "2020-11-03              0.022398     1  \n",
      "2020-11-02              0.017443     1  \n",
      "2020-10-30              0.011270     0  \n",
      "2020-10-29             -0.010485     1  \n",
      "2020-10-28              0.010163     1  \n",
      "2020-10-27             -0.034235     1  \n",
      "2020-10-26             -0.003432     0  \n",
      "2020-10-23             -0.018351     0  \n",
      "2020-10-22              0.003279     1  \n",
      "2020-10-21              0.005661     1  \n",
      "2020-10-20             -0.001893     1  \n",
      "2020-10-19              0.003859     1  \n",
      "2020-10-16             -0.015061     1  \n",
      "2020-10-15             -0.000748     0  \n",
      "2020-10-14             -0.001322     1  \n",
      "2020-10-13             -0.006226     1  \n",
      "2020-10-12             -0.006469     1  \n",
      "2020-10-09              0.016117     1  \n",
      "2020-10-08              0.009048     0  \n",
      "2020-10-07              0.008805     1  \n",
      "2020-10-06              0.017287     0  \n",
      "...                          ...   ...  \n",
      "2020-04-02             -0.014531     0  \n",
      "2020-04-01              0.023363    -1  \n",
      "2020-03-31             -0.044936     0  \n",
      "2020-03-30             -0.015209     1  \n",
      "2020-03-27              0.032716     0  \n",
      "2020-03-26             -0.030606     1  \n",
      "2020-03-25              0.059245     1  \n",
      "2020-03-24              0.013055     1  \n",
      "2020-03-23              0.094737     0  \n",
      "2020-03-20             -0.028086     0  \n",
      "2020-03-17             -0.051851     1  \n",
      "2020-03-16              0.061735    -1  \n",
      "2020-03-13             -0.115916     1  \n",
      "2020-03-11             -0.095714    -1  \n",
      "2020-03-09              0.051166     0  \n",
      "2020-02-26             -0.044937     0  \n",
      "2020-02-07              0.007495     0  \n",
      "2020-01-27              0.010417    -1  \n",
      "2020-01-17             -0.002108    -1  \n",
      "2019-10-10              0.010299     1  \n",
      "2019-10-08              0.009288     0  \n",
      "2019-10-04             -0.004280     0  \n",
      "2019-07-18             -0.005789    -1  \n",
      "2019-06-12              0.004474     0  \n",
      "2019-06-03              0.021782    -1  \n",
      "2019-05-28             -0.006885     0  \n",
      "2019-05-15              0.009016     0  \n",
      "2019-04-09              0.003428     0  \n",
      "2019-02-01              0.006332     0  \n",
      "2019-01-28             -0.001233     0  \n",
      "\n",
      "[187 rows x 7 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.489\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      1.00      0.66        23\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.24      0.50      0.33        47\n",
      "weighted avg       0.24      0.49      0.32        47\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0    SPY\n",
      "[0.48936170212765956]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.511\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.91      0.65        23\n",
      "    positive       0.60      0.12      0.21        24\n",
      "\n",
      "    accuracy                           0.51        47\n",
      "   macro avg       0.55      0.52      0.43        47\n",
      "weighted avg       0.55      0.51      0.42        47\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0    SPY      0.489362\n",
      "[0.5106382978723404]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.489\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      1.00      0.66        23\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.24      0.50      0.33        47\n",
      "weighted avg       0.24      0.49      0.32        47\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0    SPY      0.489362   0.510638\n",
      "[0.48936170212765956]\n",
      "            compound  positive   neutral  negative     close  \\\n",
      "Date                                                           \n",
      "2020-11-16  0.109900  0.062000  0.912200  0.025800  120.3500   \n",
      "2020-11-13 -0.043083  0.032667  0.918167  0.049167  119.4900   \n",
      "2020-11-12  0.324450  0.228000  0.772000  0.000000  119.2350   \n",
      "2020-11-11  0.192778  0.158111  0.782778  0.059111  119.5100   \n",
      "2020-11-10  0.187700  0.111714  0.888286  0.000000  115.9700   \n",
      "2020-11-09  0.133167  0.104667  0.848333  0.047000  116.3200   \n",
      "2020-11-06 -0.099333  0.054833  0.845500  0.099500  118.6850   \n",
      "2020-11-05  0.304967  0.202333  0.747333  0.050333  118.9900   \n",
      "2020-11-04  0.011500  0.098273  0.792455  0.109364  114.9400   \n",
      "2020-11-03  0.181500  0.119000  0.842000  0.038833  110.3750   \n",
      "2020-11-02 -0.085300  0.032167  0.913667  0.054167  108.7700   \n",
      "2020-10-30 -0.102568  0.068800  0.812000  0.119160  108.9000   \n",
      "2020-10-29  0.234180  0.169200  0.802550  0.028250  114.5200   \n",
      "2020-10-28 -0.208286  0.051286  0.797000  0.151714  111.1500   \n",
      "2020-10-27 -0.078186  0.019571  0.904714  0.075714  116.5300   \n",
      "2020-10-26  0.044190  0.083100  0.872300  0.044600  115.0600   \n",
      "2020-10-23 -0.130520  0.080800  0.763400  0.155600  115.0400   \n",
      "2020-10-22  0.006620  0.028800  0.951400  0.019800  115.7737   \n",
      "2020-10-21 -0.164925  0.000000  0.906250  0.093750  116.8600   \n",
      "2020-10-20  0.091967  0.122778  0.804889  0.072333  117.5000   \n",
      "2020-10-19  0.171550  0.199000  0.726000  0.075000  116.0000   \n",
      "2020-10-16 -0.120700  0.013000  0.920667  0.066333  118.9600   \n",
      "2020-10-15  0.059656  0.105444  0.815333  0.079222  120.7453   \n",
      "2020-10-14  0.187375  0.114000  0.867125  0.018875  121.2900   \n",
      "2020-10-13  0.001110  0.084333  0.840143  0.075476  121.0500   \n",
      "2020-10-12  0.312075  0.171583  0.828417  0.000000  124.4200   \n",
      "2020-10-09 -0.004725  0.054750  0.878750  0.066500  116.9800   \n",
      "2020-10-08  0.095450  0.056000  0.944000  0.000000  114.9700   \n",
      "2020-10-07  0.048760  0.077900  0.860200  0.061900  115.0500   \n",
      "2020-10-06  0.007000  0.061692  0.861615  0.076692  113.1600   \n",
      "...              ...       ...       ...       ...       ...   \n",
      "2016-02-16  0.017536  0.078909  0.828545  0.092545   96.6200   \n",
      "2016-02-12  0.680800  0.355000  0.645000  0.000000   93.9900   \n",
      "2016-02-11 -0.159100  0.142500  0.646500  0.211000   93.6600   \n",
      "2016-02-10  0.284720  0.249600  0.691200  0.059200   94.2700   \n",
      "2016-02-09  0.038600  0.105500  0.797500  0.097500   95.0200   \n",
      "2016-02-08  0.161575  0.203500  0.735000  0.061500   95.0100   \n",
      "2016-02-05  0.035020  0.104800  0.827600  0.067600   94.0100   \n",
      "2016-02-04  0.165500  0.204000  0.639000  0.157500   96.6000   \n",
      "2016-02-03  0.104533  0.268000  0.501667  0.230333   96.3400   \n",
      "2016-02-02  0.173530  0.151400  0.806900  0.041700   94.4800   \n",
      "2016-02-01  0.188858  0.176833  0.777083  0.046083   96.3800   \n",
      "2016-01-29  0.242800  0.338333  0.573333  0.088333   97.1400   \n",
      "2016-01-28  0.094677  0.141692  0.770769  0.087538   94.0800   \n",
      "2016-01-27 -0.059054  0.076808  0.812846  0.110308   93.4300   \n",
      "2016-01-26  0.046086  0.104643  0.853571  0.041714   99.9800   \n",
      "2016-01-25  0.201286  0.199429  0.766429  0.034143   99.4660   \n",
      "2016-01-22  0.187875  0.134625  0.831750  0.033625  101.4200   \n",
      "2016-01-21  0.382450  0.269500  0.632500  0.098000   96.2600   \n",
      "2016-01-20  0.237133  0.211000  0.716667  0.072333   96.8100   \n",
      "2016-01-19 -0.063633  0.000000  0.965667  0.034333   96.6800   \n",
      "2016-01-15  0.483250  0.281000  0.719000  0.000000   97.0900   \n",
      "2016-01-14 -0.232650  0.000000  0.873500  0.126500   99.5100   \n",
      "2016-01-13  0.244367  0.219778  0.712000  0.068222   97.4100   \n",
      "2016-01-12  0.088433  0.098000  0.852667  0.049333   99.9600   \n",
      "2016-01-11  0.106518  0.113765  0.825353  0.060882   98.5300   \n",
      "2016-01-08 -0.040440  0.087000  0.780500  0.132500   96.9800   \n",
      "2016-01-07  0.022617  0.131500  0.746500  0.121833   96.5400   \n",
      "2016-01-06 -0.079677  0.103615  0.778385  0.118000  100.7000   \n",
      "2016-01-05 -0.194557  0.000000  0.886143  0.113857  102.7100   \n",
      "2016-01-04 -0.180600  0.000000  0.914000  0.086000  105.3300   \n",
      "\n",
      "            predicted pct change SCORE  \n",
      "Date                                    \n",
      "2020-11-16             -0.008226     1  \n",
      "2020-11-13              0.007197     0  \n",
      "2020-11-12              0.002139     1  \n",
      "2020-11-11             -0.002301     1  \n",
      "2020-11-10              0.030525     1  \n",
      "2020-11-09             -0.003009     1  \n",
      "2020-11-06             -0.019927    -1  \n",
      "2020-11-05             -0.002563     1  \n",
      "2020-11-04              0.035236     0  \n",
      "2020-11-03              0.041359     1  \n",
      "2020-11-02              0.014756    -1  \n",
      "2020-10-30             -0.001194    -1  \n",
      "2020-10-29             -0.049074     1  \n",
      "2020-10-28              0.030319    -1  \n",
      "2020-10-27             -0.046168    -1  \n",
      "2020-10-26              0.012776     0  \n",
      "2020-10-23              0.000174    -1  \n",
      "2020-10-22             -0.006337     0  \n",
      "2020-10-21             -0.009296    -1  \n",
      "2020-10-20             -0.005447     1  \n",
      "2020-10-19              0.012931     1  \n",
      "2020-10-16             -0.024882    -1  \n",
      "2020-10-15             -0.014786     1  \n",
      "2020-10-14             -0.004491     1  \n",
      "2020-10-13              0.001983     0  \n",
      "2020-10-12             -0.027086     1  \n",
      "2020-10-09              0.063601     0  \n",
      "2020-10-08              0.017483     1  \n",
      "2020-10-07             -0.000695     0  \n",
      "2020-10-06              0.016702     0  \n",
      "...                          ...   ...  \n",
      "2016-02-16              0.015421     0  \n",
      "2016-02-12              0.027982     1  \n",
      "2016-02-11              0.003523    -1  \n",
      "2016-02-10             -0.006471     1  \n",
      "2016-02-09             -0.007893     0  \n",
      "2016-02-08              0.000105     1  \n",
      "2016-02-05              0.010637     0  \n",
      "2016-02-04             -0.026812     1  \n",
      "2016-02-03              0.002699     1  \n",
      "2016-02-02              0.019687     1  \n",
      "2016-02-01             -0.019714     1  \n",
      "2016-01-29             -0.007824     1  \n",
      "2016-01-28              0.032526     1  \n",
      "2016-01-27              0.006957    -1  \n",
      "2016-01-26             -0.065513     0  \n",
      "2016-01-25              0.005168     1  \n",
      "2016-01-22             -0.019266     1  \n",
      "2016-01-21              0.053605     1  \n",
      "2016-01-20             -0.005681     1  \n",
      "2016-01-19              0.001345    -1  \n",
      "2016-01-15             -0.004223     1  \n",
      "2016-01-14             -0.024319    -1  \n",
      "2016-01-13              0.021558     1  \n",
      "2016-01-12             -0.025510     1  \n",
      "2016-01-11              0.014513     1  \n",
      "2016-01-08              0.015983     0  \n",
      "2016-01-07              0.004558     0  \n",
      "2016-01-06             -0.041311    -1  \n",
      "2016-01-05             -0.019570    -1  \n",
      "2016-01-04             -0.024874    -1  \n",
      "\n",
      "[1211 rows x 7 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.531\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      1.00      0.69       160\n",
      "    positive       1.00      0.01      0.01       143\n",
      "\n",
      "    accuracy                           0.53       303\n",
      "   macro avg       0.76      0.50      0.35       303\n",
      "weighted avg       0.75      0.53      0.37       303\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   AAPL\n",
      "[0.5313531353135313]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.495\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.57      0.55       160\n",
      "    positive       0.46      0.41      0.43       143\n",
      "\n",
      "    accuracy                           0.50       303\n",
      "   macro avg       0.49      0.49      0.49       303\n",
      "weighted avg       0.49      0.50      0.49       303\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   AAPL      0.531353\n",
      "[0.49504950495049505]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.528\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      1.00      0.69       160\n",
      "    positive       0.00      0.00      0.00       143\n",
      "\n",
      "    accuracy                           0.53       303\n",
      "   macro avg       0.26      0.50      0.35       303\n",
      "weighted avg       0.28      0.53      0.36       303\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   AAPL      0.531353    0.49505\n",
      "[0.528052805280528]\n",
      "            compound  positive   neutral  negative     close  \\\n",
      "Date                                                           \n",
      "2020-11-16  0.000000  0.000000  1.000000  0.000000  142.4900   \n",
      "2020-11-13  0.510600  0.268000  0.732000  0.000000  143.8025   \n",
      "2020-11-12  0.000000  0.000000  1.000000  0.000000  142.1500   \n",
      "2020-11-06  0.440400  0.326000  0.674000  0.000000  143.1800   \n",
      "2020-11-04  0.510600  0.202000  0.798000  0.000000  140.7300   \n",
      "2020-10-30 -0.299700  0.000000  0.859500  0.140500  137.0700   \n",
      "2020-10-28 -0.263350  0.000000  0.907500  0.092500  137.7800   \n",
      "2020-10-27 -0.023400  0.180000  0.683000  0.137500  142.3600   \n",
      "2020-10-23  0.000000  0.000000  1.000000  0.000000  142.3900   \n",
      "2020-10-21  0.224350  0.147750  0.852250  0.000000  143.2300   \n",
      "2020-10-20  0.241638  0.165125  0.820625  0.014125  142.4400   \n",
      "2020-10-19  0.117233  0.153333  0.757667  0.089000  141.9900   \n",
      "2020-10-16  0.185800  0.144667  0.855333  0.000000  144.4500   \n",
      "2020-10-15  0.153100  0.167000  0.714000  0.119000  143.8600   \n",
      "2020-10-12 -0.153100  0.000000  0.849000  0.151000  144.4600   \n",
      "2020-10-08  0.000000  0.000000  1.000000  0.000000  141.6500   \n",
      "2020-10-01 -0.177900  0.000000  0.884000  0.116000  139.2400   \n",
      "2020-09-22  0.437800  0.291500  0.708500  0.000000  137.9600   \n",
      "2020-09-17  0.177900  0.159000  0.841000  0.000000  137.5200   \n",
      "2020-09-16  0.296000  0.128000  0.872000  0.000000  137.7200   \n",
      "2020-09-04 -0.526700  0.000000  0.779000  0.221000  137.9500   \n",
      "2020-09-03  0.000000  0.000000  1.000000  0.000000  138.2100   \n",
      "2020-09-01  0.476700  0.237000  0.763000  0.000000  138.1700   \n",
      "2020-08-21  0.557400  0.397000  0.603000  0.000000  137.4200   \n",
      "2020-08-18  0.296000  0.155000  0.845000  0.000000  136.5000   \n",
      "2020-08-14 -0.169575  0.099000  0.704000  0.197500  135.1000   \n",
      "2020-08-13  0.000000  0.000000  1.000000  0.000000  135.7900   \n",
      "2020-08-11  0.636900  0.244000  0.756000  0.000000  133.2400   \n",
      "2020-08-06 -0.064400  0.129000  0.723000  0.149000  132.7100   \n",
      "2020-08-05 -0.421500  0.000000  0.678000  0.322000  133.4500   \n",
      "...              ...       ...       ...       ...       ...   \n",
      "2008-06-04  0.084300  0.117444  0.848111  0.034444   66.3800   \n",
      "2008-06-03  0.000000  0.000000  1.000000  0.000000   65.3400   \n",
      "2008-05-29 -0.128000  0.000000  0.727000  0.273000   65.4400   \n",
      "2008-05-28  0.000000  0.000000  1.000000  0.000000   64.9300   \n",
      "2008-05-23  0.000000  0.000000  1.000000  0.000000   65.2900   \n",
      "2008-05-14  0.000000  0.000000  1.000000  0.000000   65.4600   \n",
      "2008-05-06  0.051600  0.287000  0.500000  0.212000   66.7200   \n",
      "2008-05-05  0.000000  0.000000  1.000000  0.000000   66.5900   \n",
      "2008-05-02 -0.158900  0.000000  0.886333  0.113667   66.7500   \n",
      "2008-04-30  0.015222  0.059111  0.890889  0.050000   67.0500   \n",
      "2008-04-29 -0.136600  0.105000  0.690500  0.205000   65.8700   \n",
      "2008-04-28  0.000000  0.000000  1.000000  0.000000   66.2400   \n",
      "2008-04-17  0.000000  0.000000  1.000000  0.000000   67.5100   \n",
      "2008-04-09  0.000000  0.000000  1.000000  0.000000   70.1600   \n",
      "2008-04-08  0.000000  0.000000  1.000000  0.000000   70.6400   \n",
      "2008-04-04 -0.128000  0.000000  0.769000  0.231000   70.6100   \n",
      "2008-04-02  0.000000  0.000000  1.000000  0.000000   70.4700   \n",
      "2008-04-01  0.133967  0.117000  0.883000  0.000000   71.1600   \n",
      "2008-03-27  0.493900  0.583000  0.417000  0.000000   69.4100   \n",
      "2008-03-19  0.599400  0.551000  0.449000  0.000000   67.8000   \n",
      "2008-03-11  0.000000  0.000000  1.000000  0.000000   66.7000   \n",
      "2008-02-29  0.000000  0.000000  1.000000  0.000000   66.1200   \n",
      "2008-02-28  0.421500  0.319000  0.517000  0.164000   66.9700   \n",
      "2008-02-11  0.493900  0.583000  0.417000  0.000000   65.9800   \n",
      "2008-02-08  0.000000  0.000000  1.000000  0.000000   64.9900   \n",
      "2008-02-01  0.440400  0.326000  0.674000  0.000000   66.0300   \n",
      "2008-01-31  0.080575  0.130250  0.810000  0.059750   65.4200   \n",
      "2008-01-30  0.599400  0.358000  0.642000  0.000000   65.0300   \n",
      "2008-01-24  0.421500  0.359000  0.641000  0.000000   66.1100   \n",
      "2008-01-16  0.000000  0.000000  1.000000  0.000000   68.8400   \n",
      "\n",
      "            predicted pct change SCORE  \n",
      "Date                                    \n",
      "2020-11-16             -0.004281     0  \n",
      "2020-11-13             -0.009127     1  \n",
      "2020-11-12              0.011625     0  \n",
      "2020-11-06             -0.035899     1  \n",
      "2020-11-04              0.011725     1  \n",
      "2020-10-30              0.010360    -1  \n",
      "2020-10-28             -0.001524    -1  \n",
      "2020-10-27             -0.032172     0  \n",
      "2020-10-23             -0.007655     0  \n",
      "2020-10-21             -0.012428     1  \n",
      "2020-10-20              0.005546     1  \n",
      "2020-10-19              0.003169     1  \n",
      "2020-10-16             -0.017030     1  \n",
      "2020-10-15              0.004101     1  \n",
      "2020-10-12             -0.002077    -1  \n",
      "2020-10-08              0.009178     0  \n",
      "2020-10-01             -0.008044    -1  \n",
      "2020-09-22             -0.012032     1  \n",
      "2020-09-17             -0.001309     1  \n",
      "2020-09-16             -0.001452     1  \n",
      "2020-09-04             -0.014643    -1  \n",
      "2020-09-03             -0.001881     0  \n",
      "2020-09-01              0.016863     1  \n",
      "2020-08-21              0.008223     1  \n",
      "2020-08-18             -0.005055     1  \n",
      "2020-08-14              0.002665    -1  \n",
      "2020-08-13             -0.005081     0  \n",
      "2020-08-11              0.016662     1  \n",
      "2020-08-06              0.006631    -1  \n",
      "2020-08-05             -0.005545    -1  \n",
      "...                          ...   ...  \n",
      "2008-06-04              0.002410     1  \n",
      "2008-06-03              0.015917     0  \n",
      "2008-05-29              0.007793    -1  \n",
      "2008-05-28              0.007855     0  \n",
      "2008-05-23             -0.002144     0  \n",
      "2008-05-14              0.013138     0  \n",
      "2008-05-06             -0.014388     1  \n",
      "2008-05-05              0.001952     0  \n",
      "2008-05-02             -0.002397    -1  \n",
      "2008-04-30             -0.000447     0  \n",
      "2008-04-29              0.017914    -1  \n",
      "2008-04-28             -0.005586     0  \n",
      "2008-04-17             -0.004740     0  \n",
      "2008-04-09             -0.000428     0  \n",
      "2008-04-08             -0.006795     0  \n",
      "2008-04-04             -0.001841    -1  \n",
      "2008-04-02             -0.000710     0  \n",
      "2008-04-01             -0.009696     1  \n",
      "2008-03-27              0.000576     1  \n",
      "2008-03-19              0.022271     1  \n",
      "2008-03-11              0.005697     0  \n",
      "2008-02-29              0.003327     0  \n",
      "2008-02-28             -0.012692     1  \n",
      "2008-02-11              0.018339     1  \n",
      "2008-02-08              0.015233     0  \n",
      "2008-02-01              0.002120     1  \n",
      "2008-01-31              0.009324     1  \n",
      "2008-01-30              0.005997     1  \n",
      "2008-01-24             -0.012101     1  \n",
      "2008-01-16             -0.011766     0  \n",
      "\n",
      "[947 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- -----------------------------\n",
      "Test Acc: 0.578\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.66      0.64       132\n",
      "    positive       0.53      0.48      0.50       105\n",
      "\n",
      "    accuracy                           0.58       237\n",
      "   macro avg       0.57      0.57      0.57       237\n",
      "weighted avg       0.57      0.58      0.58       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   AMZN\n",
      "[0.5780590717299579]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.464\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.45      0.48       132\n",
      "    positive       0.41      0.49      0.45       105\n",
      "\n",
      "    accuracy                           0.46       237\n",
      "   macro avg       0.47      0.47      0.46       237\n",
      "weighted avg       0.47      0.46      0.47       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   AMZN      0.578059\n",
      "[0.4641350210970464]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.557\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      1.00      0.72       132\n",
      "    positive       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.56       237\n",
      "   macro avg       0.28      0.50      0.36       237\n",
      "weighted avg       0.31      0.56      0.40       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   AMZN      0.578059   0.464135\n",
      "[0.5569620253164557]\n",
      "            compound  positive   neutral  negative     close  \\\n",
      "Date                                                           \n",
      "2020-11-04  0.510600  0.202000  0.798000  0.000000  235.0100   \n",
      "2020-10-26 -0.726900  0.000000  0.596000  0.404000  220.2103   \n",
      "2020-10-14  0.000000  0.000000  1.000000  0.000000  237.0900   \n",
      "2020-10-05  0.000000  0.000000  1.000000  0.000000  225.9200   \n",
      "2020-09-18  0.526700  0.403000  0.448000  0.149000  194.9600   \n",
      "2020-09-17  0.000000  0.000000  1.000000  0.000000  193.3900   \n",
      "2020-09-15  0.000000  0.000000  1.000000  0.000000  202.0200   \n",
      "2020-09-09  0.421500  0.219000  0.781000  0.000000  211.7900   \n",
      "2020-09-04 -0.240440  0.000000  0.881600  0.118400  216.2531   \n",
      "2020-09-03  0.000000  0.000000  1.000000  0.000000  242.0900   \n",
      "2020-09-02  0.000000  0.000000  1.000000  0.000000  265.0000   \n",
      "2020-09-01  0.247300  0.161000  0.839000  0.000000  268.7900   \n",
      "2020-08-17  0.659700  0.278000  0.722000  0.000000  207.7500   \n",
      "2020-08-11  0.636900  0.244000  0.756000  0.000000  191.5400   \n",
      "2020-08-05  0.000000  0.000000  1.000000  0.000000  226.0400   \n",
      "2020-07-21  0.743000  0.510000  0.490000  0.000000  201.1000   \n",
      "2020-07-20  0.700300  0.420000  0.580000  0.000000  201.9900   \n",
      "2020-07-17 -0.273200  0.000000  0.890000  0.110000  196.4900   \n",
      "2020-07-09  0.000000  0.000000  1.000000  0.000000  212.4200   \n",
      "2020-06-25  0.000000  0.000000  1.000000  0.000000  168.4700   \n",
      "2020-06-18  0.421500  0.203000  0.797000  0.000000  162.3000   \n",
      "2020-06-16 -0.421500  0.000000  0.843000  0.157000  163.0600   \n",
      "2020-06-05  0.062500  0.047750  0.917500  0.034750  139.6500   \n",
      "2020-06-04  0.101150  0.076500  0.923500  0.000000  139.9750   \n",
      "2020-06-03 -0.025800  0.152000  0.690000  0.159000  147.4500   \n",
      "2020-06-02  0.000000  0.000000  1.000000  0.000000  147.7800   \n",
      "2020-05-28  0.000000  0.000000  1.000000  0.000000  127.7800   \n",
      "2020-04-24 -0.077200  0.214000  0.552000  0.234000  105.0700   \n",
      "2020-04-22  0.318200  0.119000  0.881000  0.000000  101.6620   \n",
      "2020-04-17  0.000000  0.000000  1.000000  0.000000  100.2600   \n",
      "...              ...       ...       ...       ...       ...   \n",
      "2019-10-16  0.000000  0.000000  1.000000  0.000000   67.0500   \n",
      "2019-09-06  0.266267  0.115333  0.884667  0.000000   56.2800   \n",
      "2019-09-05  0.670500  0.478000  0.522000  0.000000   46.2400   \n",
      "2019-09-04  0.000000  0.000000  1.000000  0.000000   46.8500   \n",
      "2019-07-02 -0.504600  0.000000  0.811000  0.189000   51.3500   \n",
      "2019-06-07 -0.093450  0.054500  0.756000  0.189750   48.1600   \n",
      "2019-06-06 -0.106067  0.000000  0.917667  0.082333   54.6800   \n",
      "2019-06-03  0.000000  0.000000  1.000000  0.000000   50.5600   \n",
      "2019-05-16  0.526700  0.362000  0.638000  0.000000   54.7400   \n",
      "2019-04-18  0.648600  0.398000  0.602000  0.000000   52.7600   \n",
      "2019-03-14  0.000000  0.000000  1.000000  0.000000   58.8200   \n",
      "2019-02-13  0.318000  0.262500  0.737500  0.000000   53.6600   \n",
      "2019-01-28 -0.361200  0.000000  0.815000  0.185000   48.7754   \n",
      "2019-01-15  0.000000  0.000000  1.000000  0.000000   45.0000   \n",
      "2018-12-21  0.220200  0.091000  0.909000  0.000000   37.9500   \n",
      "2018-12-07  0.000000  0.000000  1.000000  0.000000   39.6500   \n",
      "2018-11-12  0.000000  0.000000  1.000000  0.000000   41.3200   \n",
      "2018-10-29  0.000000  0.000000  1.000000  0.000000   40.4100   \n",
      "2018-10-19  0.000000  0.000000  1.000000  0.000000   41.9700   \n",
      "2018-09-05  0.000000  0.000000  1.000000  0.000000   63.1000   \n",
      "2018-07-26 -0.659700  0.000000  0.714000  0.286000   55.4750   \n",
      "2018-07-24  0.624900  0.291000  0.709000  0.000000   53.7800   \n",
      "2018-07-11 -0.051600  0.000000  0.833000  0.167000   51.9900   \n",
      "2018-07-05  0.000000  0.000000  1.000000  0.000000   54.5700   \n",
      "2018-06-08  0.437660  0.297200  0.651400  0.051600   57.9500   \n",
      "2018-06-07  0.000000  0.000000  1.000000  0.000000   56.0000   \n",
      "2018-05-29  0.000000  0.000000  1.000000  0.000000   48.4000   \n",
      "2018-05-22 -0.361200  0.000000  0.800000  0.200000   43.0800   \n",
      "2018-05-01  0.153100  0.191000  0.658000  0.151000   39.4200   \n",
      "2018-04-27  0.000000  0.000000  1.000000  0.000000   39.7500   \n",
      "\n",
      "            predicted pct change SCORE  \n",
      "Date                                    \n",
      "2020-11-04              0.008170     1  \n",
      "2020-10-26              0.008945    -1  \n",
      "2020-10-14              0.013666     0  \n",
      "2020-10-05             -0.020892     0  \n",
      "2020-09-18              0.051293     1  \n",
      "2020-09-17              0.008118     0  \n",
      "2020-09-15             -0.024602     0  \n",
      "2020-09-09             -0.027999     1  \n",
      "2020-09-04             -0.049123    -1  \n",
      "2020-09-03             -0.106724     0  \n",
      "2020-09-02             -0.086453     0  \n",
      "2020-09-01             -0.014100     1  \n",
      "2020-08-17              0.008327     1  \n",
      "2020-08-11              0.008980     1  \n",
      "2020-08-05             -0.045567     0  \n",
      "2020-07-21              0.014421     1  \n",
      "2020-07-20             -0.004406     1  \n",
      "2020-07-17              0.027991    -1  \n",
      "2020-07-09             -0.010922     0  \n",
      "2020-06-25              0.054075     0  \n",
      "2020-06-18             -0.007763     1  \n",
      "2020-06-16             -0.009076    -1  \n",
      "2020-06-05              0.049552     1  \n",
      "2020-06-04             -0.002322     1  \n",
      "2020-06-03             -0.050695     0  \n",
      "2020-06-02             -0.002233     0  \n",
      "2020-05-28              0.094459     0  \n",
      "2020-04-24              0.030361    -1  \n",
      "2020-04-22              0.005194     1  \n",
      "2020-04-17              0.015161     0  \n",
      "...                          ...   ...  \n",
      "2019-10-16             -0.004922     0  \n",
      "2019-09-06              0.040512     1  \n",
      "2019-09-05              0.217128     1  \n",
      "2019-09-04             -0.013020     0  \n",
      "2019-07-02              0.005842    -1  \n",
      "2019-06-07              0.003115    -1  \n",
      "2019-06-06             -0.119239    -1  \n",
      "2019-06-03              0.046677     0  \n",
      "2019-05-16             -0.021008     1  \n",
      "2019-04-18              0.025777     1  \n",
      "2019-03-14             -0.039612     0  \n",
      "2019-02-13             -0.004659     1  \n",
      "2019-01-28             -0.010157    -1  \n",
      "2019-01-15             -0.042000     0  \n",
      "2018-12-21             -0.010277     1  \n",
      "2018-12-07              0.026482     0  \n",
      "2018-11-12              0.031946     0  \n",
      "2018-10-29              0.032913     0  \n",
      "2018-10-19              0.011198     0  \n",
      "2018-09-05             -0.083360     0  \n",
      "2018-07-26             -0.011807    -1  \n",
      "2018-07-24              0.016735     1  \n",
      "2018-07-11              0.065205    -1  \n",
      "2018-07-05              0.006231     0  \n",
      "2018-06-08              0.027437     1  \n",
      "2018-06-07              0.034821     0  \n",
      "2018-05-29              0.033471     0  \n",
      "2018-05-22              0.054318    -1  \n",
      "2018-05-01              0.002537     1  \n",
      "2018-04-27             -0.031195     0  \n",
      "\n",
      "[68 rows x 7 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.588\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.70      0.67        10\n",
      "    positive       0.50      0.43      0.46         7\n",
      "\n",
      "    accuracy                           0.59        17\n",
      "   macro avg       0.57      0.56      0.56        17\n",
      "weighted avg       0.58      0.59      0.58        17\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   DOCU\n",
      "[0.5882352941176471]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.647\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      1.00      0.77        10\n",
      "    positive       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.65        17\n",
      "   macro avg       0.81      0.57      0.51        17\n",
      "weighted avg       0.78      0.65      0.56        17\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   DOCU      0.588235\n",
      "[0.6470588235294118]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.588\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      1.00      0.74        10\n",
      "    positive       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.59        17\n",
      "   macro avg       0.29      0.50      0.37        17\n",
      "weighted avg       0.35      0.59      0.44        17\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   DOCU      0.588235   0.647059\n",
      "[0.5882352941176471]\n",
      "            compound  positive   neutral  negative     close  \\\n",
      "Date                                                           \n",
      "2020-11-16  0.000000  0.000000  1.000000  0.000000  478.0400   \n",
      "2020-11-13 -0.051033  0.059000  0.870000  0.071000  482.8200   \n",
      "2020-11-12 -0.106067  0.000000  0.932000  0.068000  486.7700   \n",
      "2020-11-11  0.255300  0.146000  0.854000  0.000000  490.7600   \n",
      "2020-11-10  0.000000  0.000000  1.000000  0.000000  480.2600   \n",
      "2020-11-09  0.000000  0.000000  1.000000  0.000000  470.7900   \n",
      "2020-11-06  0.136600  0.094500  0.905500  0.000000  514.8600   \n",
      "2020-11-05  0.510600  0.292000  0.708000  0.000000  513.6500   \n",
      "2020-11-04  0.000000  0.000000  1.000000  0.000000  496.9600   \n",
      "2020-11-03  0.000000  0.000000  1.000000  0.000000  487.2200   \n",
      "2020-11-02  0.000000  0.000000  1.000000  0.000000  484.1201   \n",
      "2020-10-30  0.000000  0.000000  1.000000  0.000000  475.7900   \n",
      "2020-10-29  0.000000  0.000000  1.000000  0.000000  504.2800   \n",
      "2020-10-28  0.255300  0.146000  0.854000  0.000000  485.9400   \n",
      "2020-10-27  0.283150  0.158000  0.842000  0.000000  488.9700   \n",
      "2020-10-26  0.000000  0.000000  1.000000  0.000000  488.4503   \n",
      "2020-10-23  0.000000  0.000000  1.000000  0.000000  488.2700   \n",
      "2020-10-22  0.164550  0.072000  0.928000  0.000000  485.2899   \n",
      "2020-10-21 -0.055192  0.044750  0.886500  0.068750  489.0700   \n",
      "2020-10-20  0.120957  0.100286  0.878143  0.021571  525.4000   \n",
      "2020-10-19 -0.043833  0.067000  0.769000  0.164000  530.9200   \n",
      "2020-10-16  0.059200  0.033400  0.966600  0.000000  530.7000   \n",
      "2020-10-15  0.085100  0.048667  0.951333  0.000000  542.0900   \n",
      "2020-10-14  0.190000  0.125800  0.842800  0.031400  541.2600   \n",
      "2020-10-13  0.356000  0.200667  0.799333  0.000000  554.0800   \n",
      "2020-10-12  0.000000  0.000000  1.000000  0.000000  539.7000   \n",
      "2020-10-09  0.245700  0.134750  0.825250  0.040000  539.4500   \n",
      "2020-10-08  0.140500  0.067667  0.932333  0.000000  531.7900   \n",
      "2020-10-07 -0.002229  0.070714  0.857857  0.071429  534.5450   \n",
      "2020-10-06  0.000000  0.000000  1.000000  0.000000  505.8700   \n",
      "...              ...       ...       ...       ...       ...   \n",
      "2008-07-25  0.101655  0.077364  0.922636  0.000000    3.9800   \n",
      "2008-07-24  0.000000  0.000000  1.000000  0.000000    3.8190   \n",
      "2008-07-16  0.202300  0.184000  0.816000  0.000000    4.0140   \n",
      "2008-07-14  0.000000  0.000000  1.000000  0.000000    3.9740   \n",
      "2008-05-20  0.000000  0.000000  1.000000  0.000000    4.5090   \n",
      "2008-05-15  0.778300  0.694000  0.306000  0.000000    4.2740   \n",
      "2008-05-09  0.690800  0.487000  0.513000  0.000000    4.3860   \n",
      "2008-04-22  0.200344  0.271000  0.680667  0.048222    4.2860   \n",
      "2008-04-21  0.054542  0.113833  0.851167  0.035000    5.6200   \n",
      "2008-04-16  0.000000  0.000000  1.000000  0.000000    5.5640   \n",
      "2008-04-14  0.588900  0.460667  0.472000  0.067333    5.1190   \n",
      "2008-04-11 -0.421500  0.000000  0.568000  0.432000    5.0740   \n",
      "2008-03-27  0.000000  0.000000  1.000000  0.000000    5.1940   \n",
      "2008-03-24 -0.381800  0.000000  0.698000  0.302000    5.4390   \n",
      "2008-03-20  0.557400  0.535000  0.465000  0.000000    5.1800   \n",
      "2008-03-07  0.000000  0.000000  1.000000  0.000000    4.4630   \n",
      "2008-03-06 -0.004440  0.053600  0.883400  0.063000    4.3740   \n",
      "2008-03-05  0.000000  0.000000  1.000000  0.000000    4.5860   \n",
      "2008-02-27  0.172643  0.131429  0.868571  0.000000    4.6330   \n",
      "2008-02-22  0.000000  0.000000  1.000000  0.000000    3.9740   \n",
      "2008-02-15 -0.381800  0.000000  0.698000  0.302000    3.8030   \n",
      "2008-02-11  0.318450  0.256000  0.744000  0.000000    3.8400   \n",
      "2008-01-24  0.148000  0.153000  0.847000  0.000000    3.1460   \n",
      "2008-01-23  0.287733  0.317667  0.682333  0.000000    3.3940   \n",
      "2008-01-16 -0.585900  0.085000  0.615000  0.300000    3.1540   \n",
      "2008-01-15  0.223850  0.166000  0.834000  0.000000    3.1500   \n",
      "2008-01-10  0.340000  0.286000  0.714000  0.000000    3.5290   \n",
      "2008-01-09  0.375300  0.280500  0.719500  0.000000    3.2900   \n",
      "2008-01-04  0.210750  0.206000  0.794000  0.000000    3.5160   \n",
      "2008-01-03  0.226300  0.174000  0.826000  0.000000    3.7210   \n",
      "\n",
      "            predicted pct change SCORE  \n",
      "Date                                    \n",
      "2020-11-16              0.005418     0  \n",
      "2020-11-13             -0.009900    -1  \n",
      "2020-11-12             -0.008115    -1  \n",
      "2020-11-11             -0.008130     1  \n",
      "2020-11-10              0.021863     0  \n",
      "2020-11-09              0.020115     0  \n",
      "2020-11-06             -0.085596     1  \n",
      "2020-11-05              0.002356     1  \n",
      "2020-11-04              0.033584     0  \n",
      "2020-11-03              0.019991     0  \n",
      "2020-11-02              0.006403     0  \n",
      "2020-10-30              0.017508     0  \n",
      "2020-10-29             -0.056496     0  \n",
      "2020-10-28              0.037741     1  \n",
      "2020-10-27             -0.006197     1  \n",
      "2020-10-26              0.001064     0  \n",
      "2020-10-23              0.000369     0  \n",
      "2020-10-22              0.006141     1  \n",
      "2020-10-21             -0.007729    -1  \n",
      "2020-10-20             -0.069147     1  \n",
      "2020-10-19             -0.010397     0  \n",
      "2020-10-16              0.000415     1  \n",
      "2020-10-15             -0.021011     1  \n",
      "2020-10-14              0.001533     1  \n",
      "2020-10-13             -0.023137     1  \n",
      "2020-10-12              0.026644     0  \n",
      "2020-10-09              0.000463     1  \n",
      "2020-10-08              0.014404     1  \n",
      "2020-10-07             -0.005154     0  \n",
      "2020-10-06              0.056685     0  \n",
      "...                          ...   ...  \n",
      "2008-07-25              0.027638     1  \n",
      "2008-07-24              0.042158     0  \n",
      "2008-07-16             -0.011211     1  \n",
      "2008-07-14             -0.004278     0  \n",
      "2008-05-20              0.003770     0  \n",
      "2008-05-15              0.059663     1  \n",
      "2008-05-09              0.000228     1  \n",
      "2008-04-22              0.042231     1  \n",
      "2008-04-21             -0.237367     1  \n",
      "2008-04-16              0.044932     0  \n",
      "2008-04-14              0.016996     1  \n",
      "2008-04-11              0.008869    -1  \n",
      "2008-03-27             -0.021371     0  \n",
      "2008-03-24             -0.024269    -1  \n",
      "2008-03-20              0.050000     1  \n",
      "2008-03-07              0.064082     0  \n",
      "2008-03-06              0.020348     0  \n",
      "2008-03-05             -0.046228     0  \n",
      "2008-02-27             -0.027196     1  \n",
      "2008-02-22              0.006543     0  \n",
      "2008-02-15              0.007363    -1  \n",
      "2008-02-11             -0.000260     1  \n",
      "2008-01-24             -0.013350     1  \n",
      "2008-01-23             -0.073070     1  \n",
      "2008-01-16             -0.013951    -1  \n",
      "2008-01-15              0.001270     1  \n",
      "2008-01-10             -0.051856     1  \n",
      "2008-01-09              0.072644     1  \n",
      "2008-01-04              0.011661     1  \n",
      "2008-01-03             -0.055093     1  \n",
      "\n",
      "[1981 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- -----------------------------\n",
      "Test Acc: 0.863\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.85      0.86       236\n",
      "    positive       0.87      0.87      0.87       260\n",
      "\n",
      "    accuracy                           0.86       496\n",
      "   macro avg       0.86      0.86      0.86       496\n",
      "weighted avg       0.86      0.86      0.86       496\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   NFLX\n",
      "[0.8629032258064516]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.482\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.46      0.45      0.45       236\n",
      "    positive       0.51      0.51      0.51       260\n",
      "\n",
      "    accuracy                           0.48       496\n",
      "   macro avg       0.48      0.48      0.48       496\n",
      "weighted avg       0.48      0.48      0.48       496\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   NFLX      0.862903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48185483870967744]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.476\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      1.00      0.64       236\n",
      "    positive       0.00      0.00      0.00       260\n",
      "\n",
      "    accuracy                           0.48       496\n",
      "   macro avg       0.24      0.50      0.32       496\n",
      "weighted avg       0.23      0.48      0.31       496\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   NFLX      0.862903   0.481855\n",
      "[0.47580645161290325]\n",
      "            compound  positive   neutral  negative    close  \\\n",
      "Date                                                          \n",
      "2020-11-13  0.148000  0.083500  0.916500  0.000000  128.060   \n",
      "2020-11-12  0.220200  0.104500  0.895500  0.000000  126.670   \n",
      "2020-11-11  0.624000  0.253000  0.747000  0.000000  127.660   \n",
      "2020-11-04  0.510600  0.202000  0.798000  0.000000  127.340   \n",
      "2020-11-03 -0.136600  0.000000  0.925500  0.074500  124.610   \n",
      "2020-11-02  0.278700  0.157500  0.842500  0.000000  122.380   \n",
      "2020-10-30 -0.085333  0.118667  0.736000  0.145333  120.140   \n",
      "2020-10-28 -0.324480  0.066400  0.712200  0.221400  122.110   \n",
      "2020-10-23 -0.361200  0.160000  0.533000  0.308000  130.000   \n",
      "2020-10-21  0.484933  0.341667  0.658333  0.000000  129.430   \n",
      "2020-10-20  0.557400  0.315000  0.685000  0.000000  128.480   \n",
      "2020-10-19  0.557400  0.338000  0.662000  0.000000  127.480   \n",
      "2020-10-13  0.401900  0.316000  0.684000  0.000000  129.190   \n",
      "2020-10-09  0.311900  0.223000  0.693500  0.083500  130.980   \n",
      "2020-10-08 -0.177900  0.146000  0.662000  0.192000  129.700   \n",
      "2020-10-06  0.508100  0.245500  0.754500  0.000000  127.650   \n",
      "2020-09-30  0.000000  0.000000  1.000000  0.000000  125.530   \n",
      "2020-09-25  0.440400  0.293000  0.707000  0.000000  124.230   \n",
      "2020-09-24 -0.128000  0.148000  0.671000  0.181000  124.760   \n",
      "2020-09-23  0.144653  0.101294  0.892235  0.006471  127.120   \n",
      "2020-09-22  0.283650  0.190875  0.796625  0.012500  116.950   \n",
      "2020-09-21  0.000000  0.000000  1.000000  0.000000  113.370   \n",
      "2020-09-17 -0.254900  0.143000  0.616500  0.240500  116.360   \n",
      "2020-09-16  0.000000  0.000000  1.000000  0.000000  118.570   \n",
      "2020-09-15  0.285950  0.111000  0.889000  0.000000  119.270   \n",
      "2020-09-11  0.491900  0.302500  0.697500  0.000000  118.020   \n",
      "2020-09-10  0.203000  0.195333  0.738000  0.066667  114.810   \n",
      "2020-09-09  0.296000  0.167000  0.833000  0.000000  114.900   \n",
      "2020-09-04  0.000000  0.193000  0.696500  0.110500  112.400   \n",
      "2020-09-03  0.000000  0.000000  1.000000  0.000000  112.840   \n",
      "...              ...       ...       ...       ...      ...   \n",
      "2008-10-01  0.000000  0.000000  1.000000  0.000000   16.563   \n",
      "2008-09-25  0.353143  0.258714  0.699000  0.042143   16.273   \n",
      "2008-09-24  0.251043  0.234286  0.765714  0.000000   14.850   \n",
      "2008-09-23  0.000000  0.000000  1.000000  0.000000   15.170   \n",
      "2008-09-22  0.074800  0.067800  0.888400  0.043800   15.778   \n",
      "2008-08-19 -0.051600  0.322000  0.333000  0.344000   15.268   \n",
      "2008-08-05  0.000000  0.000000  1.000000  0.000000   15.378   \n",
      "2008-07-18 -0.401900  0.000000  0.722000  0.278000   14.535   \n",
      "2008-06-26  0.035157  0.070000  0.872714  0.057143   14.875   \n",
      "2008-06-25  0.184333  0.205333  0.766556  0.028111   16.500   \n",
      "2008-06-24  0.000000  0.000000  1.000000  0.000000   16.495   \n",
      "2008-06-18  0.000000  0.000000  1.000000  0.000000   16.990   \n",
      "2008-06-12  0.401900  0.278000  0.722000  0.000000   16.708   \n",
      "2008-05-28  0.000000  0.000000  1.000000  0.000000   16.855   \n",
      "2008-05-20  0.202300  0.290000  0.500000  0.210000   16.775   \n",
      "2008-05-15  0.000000  0.000000  1.000000  0.000000   17.030   \n",
      "2008-05-09  0.000000  0.000000  1.000000  0.000000   16.230   \n",
      "2008-04-30  0.000000  0.000000  1.000000  0.000000   16.700   \n",
      "2008-04-29 -0.120400  0.000000  0.889000  0.111000   17.063   \n",
      "2008-04-10  0.128000  0.158000  0.842000  0.000000   16.845   \n",
      "2008-03-20  0.073362  0.197000  0.704750  0.098250   16.800   \n",
      "2008-03-19  0.295487  0.269250  0.697875  0.032875   15.473   \n",
      "2008-03-18  0.000000  0.000000  1.000000  0.000000   15.393   \n",
      "2008-03-07  0.175850  0.280000  0.616000  0.104000   14.575   \n",
      "2008-03-05  0.440400  0.420000  0.580000  0.000000   15.210   \n",
      "2008-02-29 -0.296000  0.000000  0.312000  0.688000   15.045   \n",
      "2008-02-21  0.000000  0.000000  1.000000  0.000000   15.050   \n",
      "2008-02-20  0.000000  0.000000  1.000000  0.000000   15.310   \n",
      "2008-01-31  0.177900  0.298000  0.702000  0.000000   15.333   \n",
      "2008-01-16  0.077200  0.178000  0.822000  0.000000   14.420   \n",
      "\n",
      "            predicted pct change SCORE  \n",
      "Date                                    \n",
      "2020-11-13              0.004607     1  \n",
      "2020-11-12              0.010973     1  \n",
      "2020-11-11             -0.007755     1  \n",
      "2020-11-04              0.018140     1  \n",
      "2020-11-03              0.021908    -1  \n",
      "2020-11-02              0.018222     1  \n",
      "2020-10-30              0.018645    -1  \n",
      "2020-10-28              0.006224    -1  \n",
      "2020-10-23             -0.012000    -1  \n",
      "2020-10-21              0.004713     1  \n",
      "2020-10-20              0.007394     1  \n",
      "2020-10-19              0.007844     1  \n",
      "2020-10-13             -0.011843     1  \n",
      "2020-10-09             -0.011681     1  \n",
      "2020-10-08              0.009869    -1  \n",
      "2020-10-06              0.018410     1  \n",
      "2020-09-30              0.009081     0  \n",
      "2020-09-25              0.000966     1  \n",
      "2020-09-24             -0.004248    -1  \n",
      "2020-09-23             -0.018565     1  \n",
      "2020-09-22              0.086960     1  \n",
      "2020-09-21              0.031578     0  \n",
      "2020-09-17             -0.014782    -1  \n",
      "2020-09-16             -0.018639     0  \n",
      "2020-09-15             -0.005869     1  \n",
      "2020-09-11              0.010591     1  \n",
      "2020-09-10              0.027959     1  \n",
      "2020-09-09             -0.000783     1  \n",
      "2020-09-04              0.002847     0  \n",
      "2020-09-03             -0.003899     0  \n",
      "...                          ...   ...  \n",
      "2008-10-01             -0.037131     0  \n",
      "2008-09-25              0.042217     1  \n",
      "2008-09-24              0.095825     1  \n",
      "2008-09-23             -0.021094     0  \n",
      "2008-09-22             -0.038535     1  \n",
      "2008-08-19             -0.006419    -1  \n",
      "2008-08-05              0.011510     0  \n",
      "2008-07-18             -0.011146    -1  \n",
      "2008-06-26              0.012773     0  \n",
      "2008-06-25             -0.098485     1  \n",
      "2008-06-24              0.000303     0  \n",
      "2008-06-18             -0.002943     0  \n",
      "2008-06-12              0.031123     1  \n",
      "2008-05-28              0.013527     0  \n",
      "2008-05-20             -0.028316     1  \n",
      "2008-05-15             -0.009513     0  \n",
      "2008-05-09              0.005853     0  \n",
      "2008-04-30              0.013952     0  \n",
      "2008-04-29             -0.021274    -1  \n",
      "2008-04-10             -0.009795     1  \n",
      "2008-03-20              0.028869     1  \n",
      "2008-03-19              0.085762     1  \n",
      "2008-03-18              0.005197     0  \n",
      "2008-03-07             -0.021612     1  \n",
      "2008-03-05             -0.022025     1  \n",
      "2008-02-29             -0.000997    -1  \n",
      "2008-02-21              0.008306     0  \n",
      "2008-02-20             -0.016982     0  \n",
      "2008-01-31              0.018261     1  \n",
      "2008-01-16             -0.029265     1  \n",
      "\n",
      "[1145 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- -----------------------------\n",
      "Test Acc: 0.523\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68       148\n",
      "    positive       1.00      0.01      0.03       139\n",
      "\n",
      "    accuracy                           0.52       287\n",
      "   macro avg       0.76      0.51      0.36       287\n",
      "weighted avg       0.75      0.52      0.37       287\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0    NKE\n",
      "[0.5226480836236934]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.537\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.54      0.71      0.61       148\n",
      "    positive       0.53      0.35      0.42       139\n",
      "\n",
      "    accuracy                           0.54       287\n",
      "   macro avg       0.54      0.53      0.52       287\n",
      "weighted avg       0.54      0.54      0.52       287\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0    NKE      0.522648\n",
      "[0.5365853658536586]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.516\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68       148\n",
      "    positive       0.00      0.00      0.00       139\n",
      "\n",
      "    accuracy                           0.52       287\n",
      "   macro avg       0.26      0.50      0.34       287\n",
      "weighted avg       0.27      0.52      0.35       287\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0    NKE      0.522648   0.536585\n",
      "[0.5156794425087108]\n",
      "            compound  positive   neutral  negative     close  \\\n",
      "Date                                                           \n",
      "2020-11-16  0.000000  0.000000  1.000000  0.000000  142.4900   \n",
      "2020-11-13  0.510600  0.268000  0.732000  0.000000  143.8025   \n",
      "2020-11-12  0.000000  0.000000  1.000000  0.000000  142.1500   \n",
      "2020-11-06  0.440400  0.326000  0.674000  0.000000  143.1800   \n",
      "2020-11-04  0.510600  0.202000  0.798000  0.000000  140.7300   \n",
      "2020-10-30 -0.299700  0.000000  0.859500  0.140500  137.0700   \n",
      "2020-10-28 -0.263350  0.000000  0.907500  0.092500  137.7800   \n",
      "2020-10-27 -0.023400  0.180000  0.683000  0.137500  142.3600   \n",
      "2020-10-23  0.000000  0.000000  1.000000  0.000000  142.3900   \n",
      "2020-10-21  0.224350  0.147750  0.852250  0.000000  143.2300   \n",
      "2020-10-20  0.241638  0.165125  0.820625  0.014125  142.4400   \n",
      "2020-10-19  0.117233  0.153333  0.757667  0.089000  141.9900   \n",
      "2020-10-16  0.185800  0.144667  0.855333  0.000000  144.4500   \n",
      "2020-10-15  0.153100  0.167000  0.714000  0.119000  143.8600   \n",
      "2020-10-12 -0.153100  0.000000  0.849000  0.151000  144.4600   \n",
      "2020-10-08  0.000000  0.000000  1.000000  0.000000  141.6500   \n",
      "2020-10-01 -0.177900  0.000000  0.884000  0.116000  139.2400   \n",
      "2020-09-22  0.437800  0.291500  0.708500  0.000000  137.9600   \n",
      "2020-09-17  0.177900  0.159000  0.841000  0.000000  137.5200   \n",
      "2020-09-16  0.296000  0.128000  0.872000  0.000000  137.7200   \n",
      "2020-09-04 -0.526700  0.000000  0.779000  0.221000  137.9500   \n",
      "2020-09-03  0.000000  0.000000  1.000000  0.000000  138.2100   \n",
      "2020-09-01  0.476700  0.237000  0.763000  0.000000  138.1700   \n",
      "2020-08-21  0.557400  0.397000  0.603000  0.000000  137.4200   \n",
      "2020-08-18  0.296000  0.155000  0.845000  0.000000  136.5000   \n",
      "2020-08-14 -0.169575  0.099000  0.704000  0.197500  135.1000   \n",
      "2020-08-13  0.000000  0.000000  1.000000  0.000000  135.7900   \n",
      "2020-08-11  0.636900  0.244000  0.756000  0.000000  133.2400   \n",
      "2020-08-06 -0.064400  0.129000  0.723000  0.149000  132.7100   \n",
      "2020-08-05 -0.421500  0.000000  0.678000  0.322000  133.4500   \n",
      "...              ...       ...       ...       ...       ...   \n",
      "2008-06-04  0.084300  0.117444  0.848111  0.034444   66.3800   \n",
      "2008-06-03  0.000000  0.000000  1.000000  0.000000   65.3400   \n",
      "2008-05-29 -0.128000  0.000000  0.727000  0.273000   65.4400   \n",
      "2008-05-28  0.000000  0.000000  1.000000  0.000000   64.9300   \n",
      "2008-05-23  0.000000  0.000000  1.000000  0.000000   65.2900   \n",
      "2008-05-14  0.000000  0.000000  1.000000  0.000000   65.4600   \n",
      "2008-05-06  0.051600  0.287000  0.500000  0.212000   66.7200   \n",
      "2008-05-05  0.000000  0.000000  1.000000  0.000000   66.5900   \n",
      "2008-05-02 -0.158900  0.000000  0.886333  0.113667   66.7500   \n",
      "2008-04-30  0.015222  0.059111  0.890889  0.050000   67.0500   \n",
      "2008-04-29 -0.136600  0.105000  0.690500  0.205000   65.8700   \n",
      "2008-04-28  0.000000  0.000000  1.000000  0.000000   66.2400   \n",
      "2008-04-17  0.000000  0.000000  1.000000  0.000000   67.5100   \n",
      "2008-04-09  0.000000  0.000000  1.000000  0.000000   70.1600   \n",
      "2008-04-08  0.000000  0.000000  1.000000  0.000000   70.6400   \n",
      "2008-04-04 -0.128000  0.000000  0.769000  0.231000   70.6100   \n",
      "2008-04-02  0.000000  0.000000  1.000000  0.000000   70.4700   \n",
      "2008-04-01  0.133967  0.117000  0.883000  0.000000   71.1600   \n",
      "2008-03-27  0.493900  0.583000  0.417000  0.000000   69.4100   \n",
      "2008-03-19  0.599400  0.551000  0.449000  0.000000   67.8000   \n",
      "2008-03-11  0.000000  0.000000  1.000000  0.000000   66.7000   \n",
      "2008-02-29  0.000000  0.000000  1.000000  0.000000   66.1200   \n",
      "2008-02-28  0.421500  0.319000  0.517000  0.164000   66.9700   \n",
      "2008-02-11  0.493900  0.583000  0.417000  0.000000   65.9800   \n",
      "2008-02-08  0.000000  0.000000  1.000000  0.000000   64.9900   \n",
      "2008-02-01  0.440400  0.326000  0.674000  0.000000   66.0300   \n",
      "2008-01-31  0.080575  0.130250  0.810000  0.059750   65.4200   \n",
      "2008-01-30  0.599400  0.358000  0.642000  0.000000   65.0300   \n",
      "2008-01-24  0.421500  0.359000  0.641000  0.000000   66.1100   \n",
      "2008-01-16  0.000000  0.000000  1.000000  0.000000   68.8400   \n",
      "\n",
      "            predicted pct change SCORE  \n",
      "Date                                    \n",
      "2020-11-16             -0.004281     0  \n",
      "2020-11-13             -0.009127     1  \n",
      "2020-11-12              0.011625     0  \n",
      "2020-11-06             -0.035899     1  \n",
      "2020-11-04              0.011725     1  \n",
      "2020-10-30              0.010360    -1  \n",
      "2020-10-28             -0.001524    -1  \n",
      "2020-10-27             -0.032172     0  \n",
      "2020-10-23             -0.007655     0  \n",
      "2020-10-21             -0.012428     1  \n",
      "2020-10-20              0.005546     1  \n",
      "2020-10-19              0.003169     1  \n",
      "2020-10-16             -0.017030     1  \n",
      "2020-10-15              0.004101     1  \n",
      "2020-10-12             -0.002077    -1  \n",
      "2020-10-08              0.009178     0  \n",
      "2020-10-01             -0.008044    -1  \n",
      "2020-09-22             -0.012032     1  \n",
      "2020-09-17             -0.001309     1  \n",
      "2020-09-16             -0.001452     1  \n",
      "2020-09-04             -0.014643    -1  \n",
      "2020-09-03             -0.001881     0  \n",
      "2020-09-01              0.016863     1  \n",
      "2020-08-21              0.008223     1  \n",
      "2020-08-18             -0.005055     1  \n",
      "2020-08-14              0.002665    -1  \n",
      "2020-08-13             -0.005081     0  \n",
      "2020-08-11              0.016662     1  \n",
      "2020-08-06              0.006631    -1  \n",
      "2020-08-05             -0.005545    -1  \n",
      "...                          ...   ...  \n",
      "2008-06-04              0.002410     1  \n",
      "2008-06-03              0.015917     0  \n",
      "2008-05-29              0.007793    -1  \n",
      "2008-05-28              0.007855     0  \n",
      "2008-05-23             -0.002144     0  \n",
      "2008-05-14              0.013138     0  \n",
      "2008-05-06             -0.014388     1  \n",
      "2008-05-05              0.001952     0  \n",
      "2008-05-02             -0.002397    -1  \n",
      "2008-04-30             -0.000447     0  \n",
      "2008-04-29              0.017914    -1  \n",
      "2008-04-28             -0.005586     0  \n",
      "2008-04-17             -0.004740     0  \n",
      "2008-04-09             -0.000428     0  \n",
      "2008-04-08             -0.006795     0  \n",
      "2008-04-04             -0.001841    -1  \n",
      "2008-04-02             -0.000710     0  \n",
      "2008-04-01             -0.009696     1  \n",
      "2008-03-27              0.000576     1  \n",
      "2008-03-19              0.022271     1  \n",
      "2008-03-11              0.005697     0  \n",
      "2008-02-29              0.003327     0  \n",
      "2008-02-28             -0.012692     1  \n",
      "2008-02-11              0.018339     1  \n",
      "2008-02-08              0.015233     0  \n",
      "2008-02-01              0.002120     1  \n",
      "2008-01-31              0.009324     1  \n",
      "2008-01-30              0.005997     1  \n",
      "2008-01-24             -0.012101     1  \n",
      "2008-01-16             -0.011766     0  \n",
      "\n",
      "[947 rows x 7 columns]\n",
      "-------- -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.578\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.61      0.66      0.64       132\n",
      "    positive       0.53      0.48      0.50       105\n",
      "\n",
      "    accuracy                           0.58       237\n",
      "   macro avg       0.57      0.57      0.57       237\n",
      "weighted avg       0.57      0.58      0.58       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0     PG\n",
      "[0.5780590717299579]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.464\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.45      0.48       132\n",
      "    positive       0.41      0.49      0.45       105\n",
      "\n",
      "    accuracy                           0.46       237\n",
      "   macro avg       0.47      0.47      0.46       237\n",
      "weighted avg       0.47      0.46      0.47       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0     PG      0.578059\n",
      "[0.4641350210970464]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.557\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      1.00      0.72       132\n",
      "    positive       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.56       237\n",
      "   macro avg       0.28      0.50      0.36       237\n",
      "weighted avg       0.31      0.56      0.40       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0     PG      0.578059   0.464135\n",
      "[0.5569620253164557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "TSLA_SVM = svm_all('TSLA',tsla_complete)\n",
    "SPY_SVM = svm_all('SPY',spy_complete)\n",
    "AAPL_SVM = svm_all(\"AAPL\",aapl_complete)\n",
    "AMZN_SVM = svm_all(\"AMZN\",amzn_complete)\n",
    "DOCU_SVM = svm_all('DOCU',docu_complete)\n",
    "NFLX_SVM = svm_all('NFLX',nflx_complete)\n",
    "NKE_SVM = svm_all(\"NKE\",nke_complete)\n",
    "PG_SVM = svm_all(\"PG\",pg_complete)\n",
    "\n",
    "SVM_SUMMARY_2 = pd.concat([AAPL_SVM,TSLA_SVM,\n",
    "                         AMZN_SVM,SPY_SVM,\n",
    "                        DOCU_SVM,NFLX_SVM,\n",
    "                        NKE_SVM,PG_SVM]).reset_index(drop=True)\n",
    "\n",
    "SVM_SUMMARY_2 = SVM_SUMMARY_2.set_index('TICKER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           SCORE\n",
      "Date            \n",
      "2020-11-16     1\n",
      "2020-11-13     1\n",
      "2020-11-12     1\n",
      "2020-11-11     0\n",
      "2020-11-10     1\n",
      "2020-11-09    -1\n",
      "2020-11-06     0\n",
      "2020-11-05     0\n",
      "2020-11-04     1\n",
      "2020-11-03     1\n",
      "2020-11-02     1\n",
      "2020-10-30     0\n",
      "2020-10-29    -1\n",
      "2020-10-28     0\n",
      "2020-10-27     0\n",
      "2020-10-26    -1\n",
      "2020-10-23     1\n",
      "2020-10-22     1\n",
      "2020-10-21     1\n",
      "2020-10-20     1\n",
      "2020-10-19     0\n",
      "2020-10-16     1\n",
      "2020-10-15     1\n",
      "2020-10-14     1\n",
      "2020-10-13     1\n",
      "2020-10-12     1\n",
      "2020-10-09     1\n",
      "2020-10-08     0\n",
      "2020-10-07     1\n",
      "2020-10-06     0\n",
      "...          ...\n",
      "2011-04-05     1\n",
      "2011-03-31     0\n",
      "2011-02-22     0\n",
      "2011-02-16     1\n",
      "2011-02-15    -1\n",
      "2011-02-14     0\n",
      "2011-02-09     1\n",
      "2011-01-27     0\n",
      "2010-12-27    -1\n",
      "2010-12-23     1\n",
      "2010-12-02    -1\n",
      "2010-11-23     1\n",
      "2010-11-10     1\n",
      "2010-11-09    -1\n",
      "2010-11-04     0\n",
      "2010-11-03     0\n",
      "2010-10-13     0\n",
      "2010-10-08     0\n",
      "2010-08-04    -1\n",
      "2010-07-27     0\n",
      "2010-07-21     0\n",
      "2010-07-15     0\n",
      "2010-07-14     1\n",
      "2010-07-08     0\n",
      "2010-07-07     0\n",
      "2010-07-06     1\n",
      "2010-07-02     1\n",
      "2010-07-01     0\n",
      "2010-06-30     0\n",
      "2010-06-29     1\n",
      "\n",
      "[1612 rows x 1 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.499\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      1.00      0.67       201\n",
      "    positive       0.00      0.00      0.00       202\n",
      "\n",
      "    accuracy                           0.50       403\n",
      "   macro avg       0.25      0.50      0.33       403\n",
      "weighted avg       0.25      0.50      0.33       403\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   TSLA\n",
      "[0.4987593052109181]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.499\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      1.00      0.67       201\n",
      "    positive       0.00      0.00      0.00       202\n",
      "\n",
      "    accuracy                           0.50       403\n",
      "   macro avg       0.25      0.50      0.33       403\n",
      "weighted avg       0.25      0.50      0.33       403\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   TSLA      0.498759\n",
      "[0.4987593052109181]\n",
      "-------- -----------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Acc: 0.499\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      1.00      0.67       201\n",
      "    positive       0.00      0.00      0.00       202\n",
      "\n",
      "    accuracy                           0.50       403\n",
      "   macro avg       0.25      0.50      0.33       403\n",
      "weighted avg       0.25      0.50      0.33       403\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   TSLA      0.498759   0.498759\n",
      "[0.4987593052109181]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.499\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      1.00      0.67       201\n",
      "    positive       0.00      0.00      0.00       202\n",
      "\n",
      "    accuracy                           0.50       403\n",
      "   macro avg       0.25      0.50      0.33       403\n",
      "weighted avg       0.25      0.50      0.33       403\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0   TSLA      0.498759   0.498759    0.498759\n",
      "[0.4987593052109181]\n",
      "           SCORE\n",
      "Date            \n",
      "2020-11-16     1\n",
      "2020-11-13     1\n",
      "2020-11-12     1\n",
      "2020-11-11     1\n",
      "2020-11-10     0\n",
      "2020-11-09     1\n",
      "2020-11-06     1\n",
      "2020-11-05     1\n",
      "2020-11-04     1\n",
      "2020-11-03     1\n",
      "2020-11-02     1\n",
      "2020-10-30     0\n",
      "2020-10-29     1\n",
      "2020-10-28     1\n",
      "2020-10-27     1\n",
      "2020-10-26     0\n",
      "2020-10-23     0\n",
      "2020-10-22     1\n",
      "2020-10-21     1\n",
      "2020-10-20     1\n",
      "2020-10-19     1\n",
      "2020-10-16     1\n",
      "2020-10-15     0\n",
      "2020-10-14     1\n",
      "2020-10-13     1\n",
      "2020-10-12     1\n",
      "2020-10-09     1\n",
      "2020-10-08     0\n",
      "2020-10-07     1\n",
      "2020-10-06     0\n",
      "...          ...\n",
      "2020-04-02     0\n",
      "2020-04-01    -1\n",
      "2020-03-31     0\n",
      "2020-03-30     1\n",
      "2020-03-27     0\n",
      "2020-03-26     1\n",
      "2020-03-25     1\n",
      "2020-03-24     1\n",
      "2020-03-23     0\n",
      "2020-03-20     0\n",
      "2020-03-17     1\n",
      "2020-03-16    -1\n",
      "2020-03-13     1\n",
      "2020-03-11    -1\n",
      "2020-03-09     0\n",
      "2020-02-26     0\n",
      "2020-02-07     0\n",
      "2020-01-27    -1\n",
      "2020-01-17    -1\n",
      "2019-10-10     1\n",
      "2019-10-08     0\n",
      "2019-10-04     0\n",
      "2019-07-18    -1\n",
      "2019-06-12     0\n",
      "2019-06-03    -1\n",
      "2019-05-28     0\n",
      "2019-05-15     0\n",
      "2019-04-09     0\n",
      "2019-02-01     0\n",
      "2019-01-28     0\n",
      "\n",
      "[187 rows x 1 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.489\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      1.00      0.66        23\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.24      0.50      0.33        47\n",
      "weighted avg       0.24      0.49      0.32        47\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0    SPY\n",
      "[0.48936170212765956]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.489\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      1.00      0.66        23\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.24      0.50      0.33        47\n",
      "weighted avg       0.24      0.49      0.32        47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0    SPY      0.489362\n",
      "[0.48936170212765956]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.489\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      1.00      0.66        23\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.24      0.50      0.33        47\n",
      "weighted avg       0.24      0.49      0.32        47\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0    SPY      0.489362   0.489362\n",
      "[0.48936170212765956]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.489\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      1.00      0.66        23\n",
      "    positive       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.49        47\n",
      "   macro avg       0.24      0.50      0.33        47\n",
      "weighted avg       0.24      0.49      0.32        47\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0    SPY      0.489362   0.489362    0.489362\n",
      "[0.48936170212765956]\n",
      "           SCORE\n",
      "Date            \n",
      "2020-11-16     1\n",
      "2020-11-13     0\n",
      "2020-11-12     1\n",
      "2020-11-11     1\n",
      "2020-11-10     1\n",
      "2020-11-09     1\n",
      "2020-11-06    -1\n",
      "2020-11-05     1\n",
      "2020-11-04     0\n",
      "2020-11-03     1\n",
      "2020-11-02    -1\n",
      "2020-10-30    -1\n",
      "2020-10-29     1\n",
      "2020-10-28    -1\n",
      "2020-10-27    -1\n",
      "2020-10-26     0\n",
      "2020-10-23    -1\n",
      "2020-10-22     0\n",
      "2020-10-21    -1\n",
      "2020-10-20     1\n",
      "2020-10-19     1\n",
      "2020-10-16    -1\n",
      "2020-10-15     1\n",
      "2020-10-14     1\n",
      "2020-10-13     0\n",
      "2020-10-12     1\n",
      "2020-10-09     0\n",
      "2020-10-08     1\n",
      "2020-10-07     0\n",
      "2020-10-06     0\n",
      "...          ...\n",
      "2016-02-16     0\n",
      "2016-02-12     1\n",
      "2016-02-11    -1\n",
      "2016-02-10     1\n",
      "2016-02-09     0\n",
      "2016-02-08     1\n",
      "2016-02-05     0\n",
      "2016-02-04     1\n",
      "2016-02-03     1\n",
      "2016-02-02     1\n",
      "2016-02-01     1\n",
      "2016-01-29     1\n",
      "2016-01-28     1\n",
      "2016-01-27    -1\n",
      "2016-01-26     0\n",
      "2016-01-25     1\n",
      "2016-01-22     1\n",
      "2016-01-21     1\n",
      "2016-01-20     1\n",
      "2016-01-19    -1\n",
      "2016-01-15     1\n",
      "2016-01-14    -1\n",
      "2016-01-13     1\n",
      "2016-01-12     1\n",
      "2016-01-11     1\n",
      "2016-01-08     0\n",
      "2016-01-07     0\n",
      "2016-01-06    -1\n",
      "2016-01-05    -1\n",
      "2016-01-04    -1\n",
      "\n",
      "[1211 rows x 1 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.528\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      1.00      0.69       160\n",
      "    positive       0.00      0.00      0.00       143\n",
      "\n",
      "    accuracy                           0.53       303\n",
      "   macro avg       0.26      0.50      0.35       303\n",
      "weighted avg       0.28      0.53      0.36       303\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   AAPL\n",
      "[0.528052805280528]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.528\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      1.00      0.69       160\n",
      "    positive       0.00      0.00      0.00       143\n",
      "\n",
      "    accuracy                           0.53       303\n",
      "   macro avg       0.26      0.50      0.35       303\n",
      "weighted avg       0.28      0.53      0.36       303\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   AAPL      0.528053\n",
      "[0.528052805280528]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.528\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      1.00      0.69       160\n",
      "    positive       0.00      0.00      0.00       143\n",
      "\n",
      "    accuracy                           0.53       303\n",
      "   macro avg       0.26      0.50      0.35       303\n",
      "weighted avg       0.28      0.53      0.36       303\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   AAPL      0.528053   0.528053\n",
      "[0.528052805280528]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.528\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      1.00      0.69       160\n",
      "    positive       0.00      0.00      0.00       143\n",
      "\n",
      "    accuracy                           0.53       303\n",
      "   macro avg       0.26      0.50      0.35       303\n",
      "weighted avg       0.28      0.53      0.36       303\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0   AAPL      0.528053   0.528053    0.528053\n",
      "[0.528052805280528]\n",
      "           SCORE\n",
      "Date            \n",
      "2020-11-16     0\n",
      "2020-11-13     1\n",
      "2020-11-12     0\n",
      "2020-11-06     1\n",
      "2020-11-04     1\n",
      "2020-10-30    -1\n",
      "2020-10-28    -1\n",
      "2020-10-27     0\n",
      "2020-10-23     0\n",
      "2020-10-21     1\n",
      "2020-10-20     1\n",
      "2020-10-19     1\n",
      "2020-10-16     1\n",
      "2020-10-15     1\n",
      "2020-10-12    -1\n",
      "2020-10-08     0\n",
      "2020-10-01    -1\n",
      "2020-09-22     1\n",
      "2020-09-17     1\n",
      "2020-09-16     1\n",
      "2020-09-04    -1\n",
      "2020-09-03     0\n",
      "2020-09-01     1\n",
      "2020-08-21     1\n",
      "2020-08-18     1\n",
      "2020-08-14    -1\n",
      "2020-08-13     0\n",
      "2020-08-11     1\n",
      "2020-08-06    -1\n",
      "2020-08-05    -1\n",
      "...          ...\n",
      "2008-06-04     1\n",
      "2008-06-03     0\n",
      "2008-05-29    -1\n",
      "2008-05-28     0\n",
      "2008-05-23     0\n",
      "2008-05-14     0\n",
      "2008-05-06     1\n",
      "2008-05-05     0\n",
      "2008-05-02    -1\n",
      "2008-04-30     0\n",
      "2008-04-29    -1\n",
      "2008-04-28     0\n",
      "2008-04-17     0\n",
      "2008-04-09     0\n",
      "2008-04-08     0\n",
      "2008-04-04    -1\n",
      "2008-04-02     0\n",
      "2008-04-01     1\n",
      "2008-03-27     1\n",
      "2008-03-19     1\n",
      "2008-03-11     0\n",
      "2008-02-29     0\n",
      "2008-02-28     1\n",
      "2008-02-11     1\n",
      "2008-02-08     0\n",
      "2008-02-01     1\n",
      "2008-01-31     1\n",
      "2008-01-30     1\n",
      "2008-01-24     1\n",
      "2008-01-16     0\n",
      "\n",
      "[947 rows x 1 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.443\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.17      0.26       132\n",
      "    positive       0.43      0.78      0.55       105\n",
      "\n",
      "    accuracy                           0.44       237\n",
      "   macro avg       0.46      0.48      0.41       237\n",
      "weighted avg       0.47      0.44      0.39       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   AMZN\n",
      "[0.4430379746835443]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.561\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.67      0.63       132\n",
      "    positive       0.51      0.42      0.46       105\n",
      "\n",
      "    accuracy                           0.56       237\n",
      "   macro avg       0.55      0.55      0.54       237\n",
      "weighted avg       0.55      0.56      0.55       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   AMZN      0.443038\n",
      "[0.5611814345991561]\n",
      "-------- -----------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Acc: 0.443\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.17      0.26       132\n",
      "    positive       0.43      0.78      0.55       105\n",
      "\n",
      "    accuracy                           0.44       237\n",
      "   macro avg       0.46      0.48      0.41       237\n",
      "weighted avg       0.47      0.44      0.39       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   AMZN      0.443038   0.561181\n",
      "[0.4430379746835443]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.443\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.17      0.26       132\n",
      "    positive       0.43      0.78      0.55       105\n",
      "\n",
      "    accuracy                           0.44       237\n",
      "   macro avg       0.46      0.48      0.41       237\n",
      "weighted avg       0.47      0.44      0.39       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0   AMZN      0.443038   0.561181    0.443038\n",
      "[0.4430379746835443]\n",
      "           SCORE\n",
      "Date            \n",
      "2020-11-04     1\n",
      "2020-10-26    -1\n",
      "2020-10-14     0\n",
      "2020-10-05     0\n",
      "2020-09-18     1\n",
      "2020-09-17     0\n",
      "2020-09-15     0\n",
      "2020-09-09     1\n",
      "2020-09-04    -1\n",
      "2020-09-03     0\n",
      "2020-09-02     0\n",
      "2020-09-01     1\n",
      "2020-08-17     1\n",
      "2020-08-11     1\n",
      "2020-08-05     0\n",
      "2020-07-21     1\n",
      "2020-07-20     1\n",
      "2020-07-17    -1\n",
      "2020-07-09     0\n",
      "2020-06-25     0\n",
      "2020-06-18     1\n",
      "2020-06-16    -1\n",
      "2020-06-05     1\n",
      "2020-06-04     1\n",
      "2020-06-03     0\n",
      "2020-06-02     0\n",
      "2020-05-28     0\n",
      "2020-04-24    -1\n",
      "2020-04-22     1\n",
      "2020-04-17     0\n",
      "...          ...\n",
      "2019-10-16     0\n",
      "2019-09-06     1\n",
      "2019-09-05     1\n",
      "2019-09-04     0\n",
      "2019-07-02    -1\n",
      "2019-06-07    -1\n",
      "2019-06-06    -1\n",
      "2019-06-03     0\n",
      "2019-05-16     1\n",
      "2019-04-18     1\n",
      "2019-03-14     0\n",
      "2019-02-13     1\n",
      "2019-01-28    -1\n",
      "2019-01-15     0\n",
      "2018-12-21     1\n",
      "2018-12-07     0\n",
      "2018-11-12     0\n",
      "2018-10-29     0\n",
      "2018-10-19     0\n",
      "2018-09-05     0\n",
      "2018-07-26    -1\n",
      "2018-07-24     1\n",
      "2018-07-11    -1\n",
      "2018-07-05     0\n",
      "2018-06-08     1\n",
      "2018-06-07     0\n",
      "2018-05-29     0\n",
      "2018-05-22    -1\n",
      "2018-05-01     1\n",
      "2018-04-27     0\n",
      "\n",
      "[68 rows x 1 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.529\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.50      0.56        10\n",
      "    positive       0.44      0.57      0.50         7\n",
      "\n",
      "    accuracy                           0.53        17\n",
      "   macro avg       0.53      0.54      0.53        17\n",
      "weighted avg       0.55      0.53      0.53        17\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   DOCU\n",
      "[0.5294117647058824]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.529\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.60      0.60        10\n",
      "    positive       0.43      0.43      0.43         7\n",
      "\n",
      "    accuracy                           0.53        17\n",
      "   macro avg       0.51      0.51      0.51        17\n",
      "weighted avg       0.53      0.53      0.53        17\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   DOCU      0.529412\n",
      "[0.5294117647058824]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.529\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.50      0.56        10\n",
      "    positive       0.44      0.57      0.50         7\n",
      "\n",
      "    accuracy                           0.53        17\n",
      "   macro avg       0.53      0.54      0.53        17\n",
      "weighted avg       0.55      0.53      0.53        17\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   DOCU      0.529412   0.529412\n",
      "[0.5294117647058824]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.529\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.62      0.50      0.56        10\n",
      "    positive       0.44      0.57      0.50         7\n",
      "\n",
      "    accuracy                           0.53        17\n",
      "   macro avg       0.53      0.54      0.53        17\n",
      "weighted avg       0.55      0.53      0.53        17\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0   DOCU      0.529412   0.529412    0.529412\n",
      "[0.5294117647058824]\n",
      "           SCORE\n",
      "Date            \n",
      "2020-11-16     0\n",
      "2020-11-13    -1\n",
      "2020-11-12    -1\n",
      "2020-11-11     1\n",
      "2020-11-10     0\n",
      "2020-11-09     0\n",
      "2020-11-06     1\n",
      "2020-11-05     1\n",
      "2020-11-04     0\n",
      "2020-11-03     0\n",
      "2020-11-02     0\n",
      "2020-10-30     0\n",
      "2020-10-29     0\n",
      "2020-10-28     1\n",
      "2020-10-27     1\n",
      "2020-10-26     0\n",
      "2020-10-23     0\n",
      "2020-10-22     1\n",
      "2020-10-21    -1\n",
      "2020-10-20     1\n",
      "2020-10-19     0\n",
      "2020-10-16     1\n",
      "2020-10-15     1\n",
      "2020-10-14     1\n",
      "2020-10-13     1\n",
      "2020-10-12     0\n",
      "2020-10-09     1\n",
      "2020-10-08     1\n",
      "2020-10-07     0\n",
      "2020-10-06     0\n",
      "...          ...\n",
      "2008-07-25     1\n",
      "2008-07-24     0\n",
      "2008-07-16     1\n",
      "2008-07-14     0\n",
      "2008-05-20     0\n",
      "2008-05-15     1\n",
      "2008-05-09     1\n",
      "2008-04-22     1\n",
      "2008-04-21     1\n",
      "2008-04-16     0\n",
      "2008-04-14     1\n",
      "2008-04-11    -1\n",
      "2008-03-27     0\n",
      "2008-03-24    -1\n",
      "2008-03-20     1\n",
      "2008-03-07     0\n",
      "2008-03-06     0\n",
      "2008-03-05     0\n",
      "2008-02-27     1\n",
      "2008-02-22     0\n",
      "2008-02-15    -1\n",
      "2008-02-11     1\n",
      "2008-01-24     1\n",
      "2008-01-23     1\n",
      "2008-01-16    -1\n",
      "2008-01-15     1\n",
      "2008-01-10     1\n",
      "2008-01-09     1\n",
      "2008-01-04     1\n",
      "2008-01-03     1\n",
      "\n",
      "[1981 rows x 1 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.476\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      1.00      0.64       236\n",
      "    positive       0.00      0.00      0.00       260\n",
      "\n",
      "    accuracy                           0.48       496\n",
      "   macro avg       0.24      0.50      0.32       496\n",
      "weighted avg       0.23      0.48      0.31       496\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   NFLX\n",
      "[0.47580645161290325]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.476\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      1.00      0.64       236\n",
      "    positive       0.00      0.00      0.00       260\n",
      "\n",
      "    accuracy                           0.48       496\n",
      "   macro avg       0.24      0.50      0.32       496\n",
      "weighted avg       0.23      0.48      0.31       496\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   NFLX      0.475806\n",
      "[0.47580645161290325]\n",
      "-------- -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.476\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      1.00      0.64       236\n",
      "    positive       0.00      0.00      0.00       260\n",
      "\n",
      "    accuracy                           0.48       496\n",
      "   macro avg       0.24      0.50      0.32       496\n",
      "weighted avg       0.23      0.48      0.31       496\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   NFLX      0.475806   0.475806\n",
      "[0.47580645161290325]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.476\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      1.00      0.64       236\n",
      "    positive       0.00      0.00      0.00       260\n",
      "\n",
      "    accuracy                           0.48       496\n",
      "   macro avg       0.24      0.50      0.32       496\n",
      "weighted avg       0.23      0.48      0.31       496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0   NFLX      0.475806   0.475806    0.475806\n",
      "[0.47580645161290325]\n",
      "           SCORE\n",
      "Date            \n",
      "2020-11-13     1\n",
      "2020-11-12     1\n",
      "2020-11-11     1\n",
      "2020-11-04     1\n",
      "2020-11-03    -1\n",
      "2020-11-02     1\n",
      "2020-10-30    -1\n",
      "2020-10-28    -1\n",
      "2020-10-23    -1\n",
      "2020-10-21     1\n",
      "2020-10-20     1\n",
      "2020-10-19     1\n",
      "2020-10-13     1\n",
      "2020-10-09     1\n",
      "2020-10-08    -1\n",
      "2020-10-06     1\n",
      "2020-09-30     0\n",
      "2020-09-25     1\n",
      "2020-09-24    -1\n",
      "2020-09-23     1\n",
      "2020-09-22     1\n",
      "2020-09-21     0\n",
      "2020-09-17    -1\n",
      "2020-09-16     0\n",
      "2020-09-15     1\n",
      "2020-09-11     1\n",
      "2020-09-10     1\n",
      "2020-09-09     1\n",
      "2020-09-04     0\n",
      "2020-09-03     0\n",
      "...          ...\n",
      "2008-10-01     0\n",
      "2008-09-25     1\n",
      "2008-09-24     1\n",
      "2008-09-23     0\n",
      "2008-09-22     1\n",
      "2008-08-19    -1\n",
      "2008-08-05     0\n",
      "2008-07-18    -1\n",
      "2008-06-26     0\n",
      "2008-06-25     1\n",
      "2008-06-24     0\n",
      "2008-06-18     0\n",
      "2008-06-12     1\n",
      "2008-05-28     0\n",
      "2008-05-20     1\n",
      "2008-05-15     0\n",
      "2008-05-09     0\n",
      "2008-04-30     0\n",
      "2008-04-29    -1\n",
      "2008-04-10     1\n",
      "2008-03-20     1\n",
      "2008-03-19     1\n",
      "2008-03-18     0\n",
      "2008-03-07     1\n",
      "2008-03-05     1\n",
      "2008-02-29    -1\n",
      "2008-02-21     0\n",
      "2008-02-20     0\n",
      "2008-01-31     1\n",
      "2008-01-16     1\n",
      "\n",
      "[1145 rows x 1 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.516\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68       148\n",
      "    positive       0.00      0.00      0.00       139\n",
      "\n",
      "    accuracy                           0.52       287\n",
      "   macro avg       0.26      0.50      0.34       287\n",
      "weighted avg       0.27      0.52      0.35       287\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0    NKE\n",
      "[0.5156794425087108]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.516\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68       148\n",
      "    positive       0.00      0.00      0.00       139\n",
      "\n",
      "    accuracy                           0.52       287\n",
      "   macro avg       0.26      0.50      0.34       287\n",
      "weighted avg       0.27      0.52      0.35       287\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0    NKE      0.515679\n",
      "[0.5156794425087108]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.516\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68       148\n",
      "    positive       0.00      0.00      0.00       139\n",
      "\n",
      "    accuracy                           0.52       287\n",
      "   macro avg       0.26      0.50      0.34       287\n",
      "weighted avg       0.27      0.52      0.35       287\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0    NKE      0.515679   0.515679\n",
      "[0.5156794425087108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- -----------------------------\n",
      "Test Acc: 0.516\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68       148\n",
      "    positive       0.00      0.00      0.00       139\n",
      "\n",
      "    accuracy                           0.52       287\n",
      "   macro avg       0.26      0.50      0.34       287\n",
      "weighted avg       0.27      0.52      0.35       287\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0    NKE      0.515679   0.515679    0.515679\n",
      "[0.5156794425087108]\n",
      "           SCORE\n",
      "Date            \n",
      "2020-11-16     0\n",
      "2020-11-13     1\n",
      "2020-11-12     0\n",
      "2020-11-06     1\n",
      "2020-11-04     1\n",
      "2020-10-30    -1\n",
      "2020-10-28    -1\n",
      "2020-10-27     0\n",
      "2020-10-23     0\n",
      "2020-10-21     1\n",
      "2020-10-20     1\n",
      "2020-10-19     1\n",
      "2020-10-16     1\n",
      "2020-10-15     1\n",
      "2020-10-12    -1\n",
      "2020-10-08     0\n",
      "2020-10-01    -1\n",
      "2020-09-22     1\n",
      "2020-09-17     1\n",
      "2020-09-16     1\n",
      "2020-09-04    -1\n",
      "2020-09-03     0\n",
      "2020-09-01     1\n",
      "2020-08-21     1\n",
      "2020-08-18     1\n",
      "2020-08-14    -1\n",
      "2020-08-13     0\n",
      "2020-08-11     1\n",
      "2020-08-06    -1\n",
      "2020-08-05    -1\n",
      "...          ...\n",
      "2008-06-04     1\n",
      "2008-06-03     0\n",
      "2008-05-29    -1\n",
      "2008-05-28     0\n",
      "2008-05-23     0\n",
      "2008-05-14     0\n",
      "2008-05-06     1\n",
      "2008-05-05     0\n",
      "2008-05-02    -1\n",
      "2008-04-30     0\n",
      "2008-04-29    -1\n",
      "2008-04-28     0\n",
      "2008-04-17     0\n",
      "2008-04-09     0\n",
      "2008-04-08     0\n",
      "2008-04-04    -1\n",
      "2008-04-02     0\n",
      "2008-04-01     1\n",
      "2008-03-27     1\n",
      "2008-03-19     1\n",
      "2008-03-11     0\n",
      "2008-02-29     0\n",
      "2008-02-28     1\n",
      "2008-02-11     1\n",
      "2008-02-08     0\n",
      "2008-02-01     1\n",
      "2008-01-31     1\n",
      "2008-01-30     1\n",
      "2008-01-24     1\n",
      "2008-01-16     0\n",
      "\n",
      "[947 rows x 1 columns]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.443\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.17      0.26       132\n",
      "    positive       0.43      0.78      0.55       105\n",
      "\n",
      "    accuracy                           0.44       237\n",
      "   macro avg       0.46      0.48      0.41       237\n",
      "weighted avg       0.47      0.44      0.39       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0     PG\n",
      "[0.4430379746835443]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.561\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.67      0.63       132\n",
      "    positive       0.51      0.42      0.46       105\n",
      "\n",
      "    accuracy                           0.56       237\n",
      "   macro avg       0.55      0.55      0.54       237\n",
      "weighted avg       0.55      0.56      0.55       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0     PG      0.443038\n",
      "[0.5611814345991561]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.443\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.17      0.26       132\n",
      "    positive       0.43      0.78      0.55       105\n",
      "\n",
      "    accuracy                           0.44       237\n",
      "   macro avg       0.46      0.48      0.41       237\n",
      "weighted avg       0.47      0.44      0.39       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0     PG      0.443038   0.561181\n",
      "[0.4430379746835443]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.443\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.17      0.26       132\n",
      "    positive       0.43      0.78      0.55       105\n",
      "\n",
      "    accuracy                           0.44       237\n",
      "   macro avg       0.46      0.48      0.41       237\n",
      "weighted avg       0.47      0.44      0.39       237\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0     PG      0.443038   0.561181    0.443038\n",
      "[0.4430379746835443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "TSLA_SVM = svm('TSLA',tsla_complete)\n",
    "SPY_SVM = svm('SPY',spy_complete)\n",
    "AAPL_SVM = svm(\"AAPL\",aapl_complete)\n",
    "AMZN_SVM = svm(\"AMZN\",amzn_complete)\n",
    "DOCU_SVM = svm('DOCU',docu_complete)\n",
    "NFLX_SVM = svm('NFLX',nflx_complete)\n",
    "NKE_SVM = svm(\"NKE\",nke_complete)\n",
    "PG_SVM = svm(\"PG\",pg_complete)\n",
    "\n",
    "SVM_SUMMARY = pd.concat([AAPL_SVM,TSLA_SVM,\n",
    "                         AMZN_SVM,SPY_SVM,\n",
    "                        DOCU_SVM,NFLX_SVM,\n",
    "                        NKE_SVM,PG_SVM]).reset_index(drop=True)\n",
    "\n",
    "SVM_SUMMARY = SVM_SUMMARY.set_index('TICKER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM - linear</th>\n",
       "      <th>SVM - rbf</th>\n",
       "      <th>SVM - sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TICKER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.528053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.856079</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.498759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.578059</td>\n",
       "      <td>0.464135</td>\n",
       "      <td>0.556962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.489362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOCU</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.481855</td>\n",
       "      <td>0.475806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NKE</th>\n",
       "      <td>0.522648</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.515679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG</th>\n",
       "      <td>0.578059</td>\n",
       "      <td>0.464135</td>\n",
       "      <td>0.556962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SVM - linear  SVM - rbf  SVM - sigmoid\n",
       "TICKER                                        \n",
       "AAPL        0.531353   0.495050       0.528053\n",
       "TSLA        0.856079   0.516129       0.498759\n",
       "AMZN        0.578059   0.464135       0.556962\n",
       "SPY         0.489362   0.510638       0.489362\n",
       "DOCU        0.588235   0.647059       0.588235\n",
       "NFLX        0.862903   0.481855       0.475806\n",
       "NKE         0.522648   0.536585       0.515679\n",
       "PG          0.578059   0.464135       0.556962"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_SUMMARY_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM - linear</th>\n",
       "      <th>SVM - rbf</th>\n",
       "      <th>SVM - poly</th>\n",
       "      <th>SVM - sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TICKER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.561181</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOCU</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NKE</th>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG</th>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.561181</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SVM - linear  SVM - rbf  SVM - poly  SVM - sigmoid\n",
       "TICKER                                                    \n",
       "AAPL        0.528053   0.528053    0.528053       0.528053\n",
       "TSLA        0.498759   0.498759    0.498759       0.498759\n",
       "AMZN        0.443038   0.561181    0.443038       0.443038\n",
       "SPY         0.489362   0.489362    0.489362       0.489362\n",
       "DOCU        0.529412   0.529412    0.529412       0.529412\n",
       "NFLX        0.475806   0.475806    0.475806       0.475806\n",
       "NKE         0.515679   0.515679    0.515679       0.515679\n",
       "PG          0.443038   0.561181    0.443038       0.443038"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='7143' style='display: table; margin: 0 auto;'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"4c1c91f5-ff2f-429e-8d57-a8808b5e0880\" data-root-id=\"7143\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"ac5a8dbe-3959-4b65-bf25-9982155a818c\":{\"roots\":{\"references\":[{\"attributes\":{\"align\":null,\"below\":[{\"id\":\"7152\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"7155\",\"type\":\"Grid\"},{\"id\":\"7160\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"7156\",\"type\":\"LinearAxis\"}],\"margin\":null,\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"plot_height\":300,\"plot_width\":700,\"renderers\":[{\"id\":\"7180\",\"type\":\"GlyphRenderer\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"7144\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"7166\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"7140\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"7148\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"7141\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"7150\",\"type\":\"LinearScale\"}},\"id\":\"7143\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"7163\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"axis_label\":\"\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7183\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7153\",\"type\":\"CategoricalTicker\"}},\"id\":\"7152\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"7162\",\"type\":\"PanTool\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"7157\",\"type\":\"BasicTicker\"}},\"id\":\"7160\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"7150\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"7175\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"7180\",\"type\":\"GlyphRenderer\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"index\",\"@{index}\"],[\"Variable\",\"@{Variable}\"],[\"value\",\"@{value}\"]]},\"id\":\"7142\",\"type\":\"HoverTool\"},{\"attributes\":{\"factors\":[\"SVM - linear\",\"SVM - rbf\",\"SVM - poly\",\"SVM - sigmoid\"],\"palette\":[\"#1f77b3\",\"#ff7e0e\",\"#2ba02b\",\"#d62628\"]},\"id\":\"7173\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{},\"id\":\"7157\",\"type\":\"BasicTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"7191\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"axis_label\":\"\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7185\",\"type\":\"BasicTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7157\",\"type\":\"BasicTicker\"}},\"id\":\"7156\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"7183\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"7153\",\"type\":\"CategoricalTicker\"}},\"id\":\"7155\",\"type\":\"Grid\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7173\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7178\",\"type\":\"VBar\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"7142\",\"type\":\"HoverTool\"},{\"id\":\"7161\",\"type\":\"SaveTool\"},{\"id\":\"7162\",\"type\":\"PanTool\"},{\"id\":\"7163\",\"type\":\"WheelZoomTool\"},{\"id\":\"7164\",\"type\":\"BoxZoomTool\"},{\"id\":\"7165\",\"type\":\"ResetTool\"}]},\"id\":\"7166\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"7153\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"7148\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"7165\",\"type\":\"ResetTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7173\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7179\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"7161\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7173\",\"type\":\"CategoricalColorMapper\"}},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7177\",\"type\":\"VBar\"},{\"attributes\":{\"overlay\":{\"id\":\"7191\",\"type\":\"BoxAnnotation\"}},\"id\":\"7164\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"callback\":null,\"data\":{\"Variable\":[\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\"],\"index\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"],\"value\":{\"__ndarray__\":\"prUn/87l4D+JOB0mrOvfP7W4CfO7Wtw/fWejvrNR3z/x8PDw8PDgP3TOOeecc94/7muZLHKA4D+1uAnzu1rcP6a1J//O5eA/iTgdJqzr3z9AOZfEMvXhP31no76zUd8/8fDw8PDw4D90zjnnnHPeP+5rmSxygOA/QDmXxDL14T+mtSf/zuXgP4k4HSas698/tbgJ87ta3D99Z6O+s1HfP/Hw8PDw8OA/dM4555xz3j/ua5kscoDgP7W4CfO7Wtw/prUn/87l4D+JOB0mrOvfP7W4CfO7Wtw/fWejvrNR3z/x8PDw8PDgP3TOOeecc94/7muZLHKA4D+1uAnzu1rcPw==\",\"dtype\":\"float64\",\"shape\":[32]},\"xoffsets\":[[\"0\",\"SVM - linear\"],[\"1\",\"SVM - linear\"],[\"2\",\"SVM - linear\"],[\"3\",\"SVM - linear\"],[\"4\",\"SVM - linear\"],[\"5\",\"SVM - linear\"],[\"6\",\"SVM - linear\"],[\"7\",\"SVM - linear\"],[\"0\",\"SVM - rbf\"],[\"1\",\"SVM - rbf\"],[\"2\",\"SVM - rbf\"],[\"3\",\"SVM - rbf\"],[\"4\",\"SVM - rbf\"],[\"5\",\"SVM - rbf\"],[\"6\",\"SVM - rbf\"],[\"7\",\"SVM - rbf\"],[\"0\",\"SVM - poly\"],[\"1\",\"SVM - poly\"],[\"2\",\"SVM - poly\"],[\"3\",\"SVM - poly\"],[\"4\",\"SVM - poly\"],[\"5\",\"SVM - poly\"],[\"6\",\"SVM - poly\"],[\"7\",\"SVM - poly\"],[\"0\",\"SVM - sigmoid\"],[\"1\",\"SVM - sigmoid\"],[\"2\",\"SVM - sigmoid\"],[\"3\",\"SVM - sigmoid\"],[\"4\",\"SVM - sigmoid\"],[\"5\",\"SVM - sigmoid\"],[\"6\",\"SVM - sigmoid\"],[\"7\",\"SVM - sigmoid\"]]},\"selected\":{\"id\":\"7175\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"7193\",\"type\":\"UnionRenderers\"}},\"id\":\"7174\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"7174\",\"type\":\"ColumnDataSource\"}},\"id\":\"7181\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"factors\":[[\"0\",\"SVM - linear\"],[\"0\",\"SVM - poly\"],[\"0\",\"SVM - rbf\"],[\"0\",\"SVM - sigmoid\"],[\"1\",\"SVM - linear\"],[\"1\",\"SVM - poly\"],[\"1\",\"SVM - rbf\"],[\"1\",\"SVM - sigmoid\"],[\"2\",\"SVM - linear\"],[\"2\",\"SVM - poly\"],[\"2\",\"SVM - rbf\"],[\"2\",\"SVM - sigmoid\"],[\"3\",\"SVM - linear\"],[\"3\",\"SVM - poly\"],[\"3\",\"SVM - rbf\"],[\"3\",\"SVM - sigmoid\"],[\"4\",\"SVM - linear\"],[\"4\",\"SVM - poly\"],[\"4\",\"SVM - rbf\"],[\"4\",\"SVM - sigmoid\"],[\"5\",\"SVM - linear\"],[\"5\",\"SVM - poly\"],[\"5\",\"SVM - rbf\"],[\"5\",\"SVM - sigmoid\"],[\"6\",\"SVM - linear\"],[\"6\",\"SVM - poly\"],[\"6\",\"SVM - rbf\"],[\"6\",\"SVM - sigmoid\"],[\"7\",\"SVM - linear\"],[\"7\",\"SVM - poly\"],[\"7\",\"SVM - rbf\"],[\"7\",\"SVM - sigmoid\"]],\"tags\":[[[\"index\",\"index\",null],[\"Variable\",\"Variable\",null]]]},\"id\":\"7140\",\"type\":\"FactorRange\"},{\"attributes\":{\"data_source\":{\"id\":\"7174\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7177\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"7179\",\"type\":\"VBar\"},\"nonselection_glyph\":{\"id\":\"7178\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"7181\",\"type\":\"CDSView\"}},\"id\":\"7180\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"end\":0.5611814345991561,\"reset_end\":0.5611814345991561,\"reset_start\":0.0,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"7141\",\"type\":\"Range1d\"},{\"attributes\":{\"text\":\"Support Vector Machine\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12pt\"}},\"id\":\"7144\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"7185\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"7193\",\"type\":\"UnionRenderers\"}],\"root_ids\":[\"7143\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"ac5a8dbe-3959-4b65-bf25-9982155a818c\",\"roots\":{\"7143\":\"4c1c91f5-ff2f-429e-8d57-a8808b5e0880\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":Bars   [index,Variable]   (value)"
      ]
     },
     "execution_count": 685,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "7143"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_SUMMARY.hvplot.bar(label =\"Support Vector Machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM - linear</th>\n",
       "      <th>SVM - rbf</th>\n",
       "      <th>SVM - sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TICKER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.528053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.856079</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.498759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.578059</td>\n",
       "      <td>0.464135</td>\n",
       "      <td>0.556962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.489362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOCU</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.481855</td>\n",
       "      <td>0.475806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NKE</th>\n",
       "      <td>0.522648</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.515679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG</th>\n",
       "      <td>0.578059</td>\n",
       "      <td>0.464135</td>\n",
       "      <td>0.556962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SVM - linear  SVM - rbf  SVM - sigmoid\n",
       "TICKER                                        \n",
       "AAPL        0.531353   0.495050       0.528053\n",
       "TSLA        0.856079   0.516129       0.498759\n",
       "AMZN        0.578059   0.464135       0.556962\n",
       "SPY         0.489362   0.510638       0.489362\n",
       "DOCU        0.588235   0.647059       0.588235\n",
       "NFLX        0.862903   0.481855       0.475806\n",
       "NKE         0.522648   0.536585       0.515679\n",
       "PG          0.578059   0.464135       0.556962"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_SUMMARY_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM - linear</th>\n",
       "      <th>SVM - rbf</th>\n",
       "      <th>SVM - poly</th>\n",
       "      <th>SVM - sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TICKER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.561181</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOCU</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NKE</th>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PG</th>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.561181</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SVM - linear  SVM - rbf  SVM - poly  SVM - sigmoid\n",
       "TICKER                                                    \n",
       "AAPL        0.528053   0.528053    0.528053       0.528053\n",
       "TSLA        0.498759   0.498759    0.498759       0.498759\n",
       "AMZN        0.443038   0.561181    0.443038       0.443038\n",
       "SPY         0.489362   0.489362    0.489362       0.489362\n",
       "DOCU        0.529412   0.529412    0.529412       0.529412\n",
       "NFLX        0.475806   0.475806    0.475806       0.475806\n",
       "NKE         0.515679   0.515679    0.515679       0.515679\n",
       "PG          0.443038   0.561181    0.443038       0.443038"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='7033' style='display: table; margin: 0 auto;'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"65055e58-d6d3-45f5-a959-db4dba0bf399\" data-root-id=\"7033\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"d1b24998-8708-4a5c-a059-082ba0c55ea7\":{\"roots\":{\"references\":[{\"attributes\":{\"align\":null,\"below\":[{\"id\":\"7042\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"7045\",\"type\":\"Grid\"},{\"id\":\"7050\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"7046\",\"type\":\"LinearAxis\"}],\"margin\":null,\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"plot_height\":300,\"plot_width\":700,\"renderers\":[{\"id\":\"7070\",\"type\":\"GlyphRenderer\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"7034\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"7056\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"7030\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"7038\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"7031\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"7040\",\"type\":\"LinearScale\"}},\"id\":\"7033\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"7040\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\",\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\",\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\"],\"value\":{\"__ndarray__\":\"yY5ZStgA4T8Wz56iAGXrP6qPB711f+I/fWejvrNR3z/T0tLS0tLiP51zzjnnnOs/A2NPeYi54D+qjwe9dX/iP5h0ah7krt8/hBBCCCGE4D++kCJgY7TdP0FMriAmV+A/tbS0tLS05D/XWmuttdbePyxRuxK1K+E/vpAiYGO03T+mtSf/zuXgP4k4HSas698/pSN7BqLS4T99Z6O+s1HfP9PS0tLS0uI/dM4555xz3j/ua5kscoDgP6Ujewai0uE/\",\"dtype\":\"float64\",\"shape\":[24]},\"xoffsets\":[[\"AAPL\",\"SVM - linear\"],[\"TSLA\",\"SVM - linear\"],[\"AMZN\",\"SVM - linear\"],[\"SPY\",\"SVM - linear\"],[\"DOCU\",\"SVM - linear\"],[\"NFLX\",\"SVM - linear\"],[\"NKE\",\"SVM - linear\"],[\"PG\",\"SVM - linear\"],[\"AAPL\",\"SVM - rbf\"],[\"TSLA\",\"SVM - rbf\"],[\"AMZN\",\"SVM - rbf\"],[\"SPY\",\"SVM - rbf\"],[\"DOCU\",\"SVM - rbf\"],[\"NFLX\",\"SVM - rbf\"],[\"NKE\",\"SVM - rbf\"],[\"PG\",\"SVM - rbf\"],[\"AAPL\",\"SVM - sigmoid\"],[\"TSLA\",\"SVM - sigmoid\"],[\"AMZN\",\"SVM - sigmoid\"],[\"SPY\",\"SVM - sigmoid\"],[\"DOCU\",\"SVM - sigmoid\"],[\"NFLX\",\"SVM - sigmoid\"],[\"NKE\",\"SVM - sigmoid\"],[\"PG\",\"SVM - sigmoid\"]]},\"selected\":{\"id\":\"7065\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"7083\",\"type\":\"UnionRenderers\"}},\"id\":\"7064\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"text\":\"Support Vector Machine - ALL Variables\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12pt\"}},\"id\":\"7034\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"7073\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"axis_label\":\"\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7075\",\"type\":\"BasicTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7047\",\"type\":\"BasicTicker\"}},\"id\":\"7046\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"7081\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"data_source\":{\"id\":\"7064\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7067\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"7069\",\"type\":\"VBar\"},\"nonselection_glyph\":{\"id\":\"7068\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"7071\",\"type\":\"CDSView\"}},\"id\":\"7070\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"7053\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"overlay\":{\"id\":\"7081\",\"type\":\"BoxAnnotation\"}},\"id\":\"7054\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"7043\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"callback\":null,\"end\":0.8629032258064516,\"reset_end\":0.8629032258064516,\"reset_start\":0.0,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"7031\",\"type\":\"Range1d\"},{\"attributes\":{\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7063\",\"type\":\"CategoricalColorMapper\"}},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7067\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"7038\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"callback\":null,\"factors\":[[\"AAPL\",\"SVM - linear\"],[\"AAPL\",\"SVM - rbf\"],[\"AAPL\",\"SVM - sigmoid\"],[\"AMZN\",\"SVM - linear\"],[\"AMZN\",\"SVM - rbf\"],[\"AMZN\",\"SVM - sigmoid\"],[\"DOCU\",\"SVM - linear\"],[\"DOCU\",\"SVM - rbf\"],[\"DOCU\",\"SVM - sigmoid\"],[\"NFLX\",\"SVM - linear\"],[\"NFLX\",\"SVM - rbf\"],[\"NFLX\",\"SVM - sigmoid\"],[\"NKE\",\"SVM - linear\"],[\"NKE\",\"SVM - rbf\"],[\"NKE\",\"SVM - sigmoid\"],[\"PG\",\"SVM - linear\"],[\"PG\",\"SVM - rbf\"],[\"PG\",\"SVM - sigmoid\"],[\"SPY\",\"SVM - linear\"],[\"SPY\",\"SVM - rbf\"],[\"SPY\",\"SVM - sigmoid\"],[\"TSLA\",\"SVM - linear\"],[\"TSLA\",\"SVM - rbf\"],[\"TSLA\",\"SVM - sigmoid\"]],\"tags\":[[[\"TICKER\",\"TICKER\",null],[\"Variable\",\"Variable\",null]]]},\"id\":\"7030\",\"type\":\"FactorRange\"},{\"attributes\":{\"factors\":[\"SVM - linear\",\"SVM - rbf\",\"SVM - sigmoid\"],\"palette\":[\"#1f77b3\",\"#ff7e0e\",\"#2ba02b\"]},\"id\":\"7063\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{},\"id\":\"7065\",\"type\":\"Selection\"},{\"attributes\":{\"axis_label\":\"TICKER, Variable\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7073\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7043\",\"type\":\"CategoricalTicker\"}},\"id\":\"7042\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"7083\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"7051\",\"type\":\"SaveTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"7032\",\"type\":\"HoverTool\"},{\"id\":\"7051\",\"type\":\"SaveTool\"},{\"id\":\"7052\",\"type\":\"PanTool\"},{\"id\":\"7053\",\"type\":\"WheelZoomTool\"},{\"id\":\"7054\",\"type\":\"BoxZoomTool\"},{\"id\":\"7055\",\"type\":\"ResetTool\"}]},\"id\":\"7056\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7063\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7069\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"7075\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"7043\",\"type\":\"CategoricalTicker\"}},\"id\":\"7045\",\"type\":\"Grid\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7063\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7068\",\"type\":\"VBar\"},{\"attributes\":{\"source\":{\"id\":\"7064\",\"type\":\"ColumnDataSource\"}},\"id\":\"7071\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"7052\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"7047\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"7047\",\"type\":\"BasicTicker\"}},\"id\":\"7050\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"7055\",\"type\":\"ResetTool\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"7070\",\"type\":\"GlyphRenderer\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"TICKER\",\"@{TICKER}\"],[\"Variable\",\"@{Variable}\"],[\"value\",\"@{value}\"]]},\"id\":\"7032\",\"type\":\"HoverTool\"}],\"root_ids\":[\"7033\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"d1b24998-8708-4a5c-a059-082ba0c55ea7\",\"roots\":{\"7033\":\"65055e58-d6d3-45f5-a959-db4dba0bf399\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":Bars   [TICKER,Variable]   (value)"
      ]
     },
     "execution_count": 684,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "7033"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_SUMMARY_2.hvplot.bar(label =\"Support Vector Machine - ALL Variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKER</th>\n",
       "      <th>NN - linear</th>\n",
       "      <th>NN - sigmoid</th>\n",
       "      <th>NN - tanh</th>\n",
       "      <th>NN - relu</th>\n",
       "      <th>SVM - linear</th>\n",
       "      <th>SVM - rbf</th>\n",
       "      <th>SVM - poly</th>\n",
       "      <th>SVM - sigmoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.541254</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.541254</td>\n",
       "      <td>0.478548</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>0.528053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.488834</td>\n",
       "      <td>0.501241</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "      <td>0.498759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>0.493671</td>\n",
       "      <td>0.514768</td>\n",
       "      <td>0.527426</td>\n",
       "      <td>0.527426</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.561181</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPY</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.489362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOCU</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NFLX</td>\n",
       "      <td>0.514113</td>\n",
       "      <td>0.493952</td>\n",
       "      <td>0.506048</td>\n",
       "      <td>0.465726</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.475806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NKE</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.533101</td>\n",
       "      <td>0.533101</td>\n",
       "      <td>0.494774</td>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "      <td>0.515679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PG</td>\n",
       "      <td>0.523207</td>\n",
       "      <td>0.514768</td>\n",
       "      <td>0.544304</td>\n",
       "      <td>0.523207</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.561181</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.443038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TICKER  NN - linear  NN - sigmoid  NN - tanh  NN - relu  SVM - linear  \\\n",
       "0   AAPL     0.541254      0.521452   0.541254   0.478548      0.528053   \n",
       "1   TSLA     0.538462      0.488834   0.501241   0.461538      0.498759   \n",
       "2   AMZN     0.493671      0.514768   0.527426   0.527426      0.443038   \n",
       "3    SPY     0.595745      0.595745   0.617021   0.468085      0.489362   \n",
       "4   DOCU     0.588235      0.529412   0.411765   0.352941      0.529412   \n",
       "5   NFLX     0.514113      0.493952   0.506048   0.465726      0.475806   \n",
       "6    NKE     0.463415      0.533101   0.533101   0.494774      0.515679   \n",
       "7     PG     0.523207      0.514768   0.544304   0.523207      0.443038   \n",
       "\n",
       "   SVM - rbf  SVM - poly  SVM - sigmoid  \n",
       "0   0.528053    0.528053       0.528053  \n",
       "1   0.498759    0.498759       0.498759  \n",
       "2   0.561181    0.443038       0.443038  \n",
       "3   0.489362    0.489362       0.489362  \n",
       "4   0.529412    0.529412       0.529412  \n",
       "5   0.475806    0.475806       0.475806  \n",
       "6   0.515679    0.515679       0.515679  \n",
       "7   0.561181    0.443038       0.443038  "
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_SUMMARY = NN_SUMMARY.merge(SVM_SUMMARY,on='TICKER',how='left')\n",
    "ML_SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='5586' style='display: table; margin: 0 auto;'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"e1f84320-702f-48f3-bea1-373eb5c9f750\" data-root-id=\"5586\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"5a6a449f-aeff-4f90-8757-7726bbc84eb4\":{\"roots\":{\"references\":[{\"attributes\":{\"align\":null,\"below\":[{\"id\":\"5595\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"5598\",\"type\":\"Grid\"},{\"id\":\"5603\",\"type\":\"Grid\"},{\"id\":\"5634\",\"type\":\"Legend\"}],\"left\":[{\"id\":\"5599\",\"type\":\"LinearAxis\"}],\"margin\":null,\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"plot_height\":300,\"plot_width\":700,\"renderers\":[{\"id\":\"5626\",\"type\":\"GlyphRenderer\"},{\"id\":\"5642\",\"type\":\"GlyphRenderer\"},{\"id\":\"5659\",\"type\":\"GlyphRenderer\"},{\"id\":\"5678\",\"type\":\"GlyphRenderer\"},{\"id\":\"5699\",\"type\":\"GlyphRenderer\"},{\"id\":\"5722\",\"type\":\"GlyphRenderer\"},{\"id\":\"5747\",\"type\":\"GlyphRenderer\"},{\"id\":\"5774\",\"type\":\"GlyphRenderer\"}],\"right\":[{\"id\":\"5634\",\"type\":\"Legend\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"5587\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"5609\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"5576\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"5591\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"5577\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"5593\",\"type\":\"LinearScale\"}},\"id\":\"5586\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis_label\":\"\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"5619\",\"type\":\"BasicTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"5600\",\"type\":\"BasicTicker\"}},\"id\":\"5599\",\"type\":\"LinearAxis\"},{\"attributes\":{\"axis_label\":\"TICKER\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"5617\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"5596\",\"type\":\"CategoricalTicker\"}},\"id\":\"5595\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#1f77b3\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5625\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"#2ba02b\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5656\",\"type\":\"Line\"},{\"attributes\":{\"text\":\"\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12pt\"}},\"id\":\"5587\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"5600\",\"type\":\"BasicTicker\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#9367bc\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5697\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5606\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#ff7e0e\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5640\",\"type\":\"Line\"},{\"attributes\":{\"label\":{\"value\":\"SVM - poly\"},\"renderers\":[{\"id\":\"5747\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5767\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#2ba02b\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5657\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"5768\",\"type\":\"ColumnDataSource\"}},\"id\":\"5775\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"5605\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"5604\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"5621\",\"type\":\"Selection\"},{\"attributes\":{\"line_color\":\"#8c564b\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5719\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#9367bc\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5698\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5637\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"5795\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"5600\",\"type\":\"BasicTicker\"}},\"id\":\"5603\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"5653\",\"type\":\"ColumnDataSource\"}},\"id\":\"5660\",\"type\":\"CDSView\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#8c564b\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5720\",\"type\":\"Line\"},{\"attributes\":{\"overlay\":{\"id\":\"5633\",\"type\":\"BoxAnnotation\"}},\"id\":\"5607\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"data_source\":{\"id\":\"5693\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5696\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"5698\",\"type\":\"Line\"},\"nonselection_glyph\":{\"id\":\"5697\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5700\",\"type\":\"CDSView\"}},\"id\":\"5699\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"5693\",\"type\":\"ColumnDataSource\"}},\"id\":\"5700\",\"type\":\"CDSView\"},{\"attributes\":{\"line_color\":\"#1f77b3\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5623\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5739\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"data_source\":{\"id\":\"5620\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5623\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"5625\",\"type\":\"Line\"},\"nonselection_glyph\":{\"id\":\"5624\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5627\",\"type\":\"CDSView\"}},\"id\":\"5626\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\",\"SVM - sigmoid\"],\"value\":{\"__ndarray__\":\"prUn/87l4D+JOB0mrOvfP7W4CfO7Wtw/fWejvrNR3z/x8PDw8PDgP3TOOeecc94/7muZLHKA4D+1uAnzu1rcPw==\",\"dtype\":\"float64\",\"shape\":[8]}},\"selected\":{\"id\":\"5769\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"5818\",\"type\":\"UnionRenderers\"}},\"id\":\"5768\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":\"#9367bc\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5696\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5694\",\"type\":\"Selection\"},{\"attributes\":{\"label\":{\"value\":\"NN - relu\"},\"renderers\":[{\"id\":\"5678\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5692\",\"type\":\"LegendItem\"},{\"attributes\":{\"data_source\":{\"id\":\"5653\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5656\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"5658\",\"type\":\"Line\"},\"nonselection_glyph\":{\"id\":\"5657\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5660\",\"type\":\"CDSView\"}},\"id\":\"5659\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"label\":{\"value\":\"SVM - rbf\"},\"renderers\":[{\"id\":\"5722\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5740\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#2ba02b\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5658\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b3\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5624\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5717\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"5714\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"5742\",\"type\":\"Selection\"},{\"attributes\":{\"label\":{\"value\":\"SVM - linear\"},\"renderers\":[{\"id\":\"5699\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5715\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"5818\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"5596\",\"type\":\"CategoricalTicker\"}},\"id\":\"5598\",\"type\":\"Grid\"},{\"attributes\":{\"click_policy\":\"mute\",\"items\":[{\"id\":\"5635\",\"type\":\"LegendItem\"},{\"id\":\"5652\",\"type\":\"LegendItem\"},{\"id\":\"5671\",\"type\":\"LegendItem\"},{\"id\":\"5692\",\"type\":\"LegendItem\"},{\"id\":\"5715\",\"type\":\"LegendItem\"},{\"id\":\"5740\",\"type\":\"LegendItem\"},{\"id\":\"5767\",\"type\":\"LegendItem\"},{\"id\":\"5796\",\"type\":\"LegendItem\"}],\"location\":[0,0]},\"id\":\"5634\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"5593\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"5591\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"source\":{\"id\":\"5672\",\"type\":\"ColumnDataSource\"}},\"id\":\"5679\",\"type\":\"CDSView\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#7e7e7e\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5773\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - rbf\"],\"value\":{\"__ndarray__\":\"prUn/87l4D+JOB0mrOvfP0A5l8Qy9eE/fWejvrNR3z/x8PDw8PDgP3TOOeecc94/7muZLHKA4D9AOZfEMvXhPw==\",\"dtype\":\"float64\",\"shape\":[8]}},\"selected\":{\"id\":\"5717\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"5766\",\"type\":\"UnionRenderers\"}},\"id\":\"5716\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"label\":{\"value\":\"NN - linear\"},\"renderers\":[{\"id\":\"5626\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5635\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#7e7e7e\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5772\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5670\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"data_source\":{\"id\":\"5636\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5639\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"5641\",\"type\":\"Line\"},\"nonselection_glyph\":{\"id\":\"5640\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5643\",\"type\":\"CDSView\"}},\"id\":\"5642\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"5766\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"5617\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#ff7e0e\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5641\",\"type\":\"Line\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"5633\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"source\":{\"id\":\"5636\",\"type\":\"ColumnDataSource\"}},\"id\":\"5643\",\"type\":\"CDSView\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"5578\",\"type\":\"HoverTool\"},{\"id\":\"5604\",\"type\":\"SaveTool\"},{\"id\":\"5605\",\"type\":\"PanTool\"},{\"id\":\"5606\",\"type\":\"WheelZoomTool\"},{\"id\":\"5607\",\"type\":\"BoxZoomTool\"},{\"id\":\"5608\",\"type\":\"ResetTool\"}]},\"id\":\"5609\",\"type\":\"Toolbar\"},{\"attributes\":{\"callback\":null,\"end\":0.5957446694374084,\"reset_end\":0.5957446694374084,\"reset_start\":0.4430379746835443,\"start\":0.4430379746835443,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"5577\",\"type\":\"Range1d\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\",\"NN - tanh\"],\"value\":{\"__ndarray__\":\"AAAAwMXK4D8AAADAXJrfPwAAAICAjeE/AAAAYHIF4T8AAADg0tLiPwAAAADfe98/AAAAIHKA4D8AAABAziXhPw==\",\"dtype\":\"float64\",\"shape\":[8]}},\"selected\":{\"id\":\"5654\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"5691\",\"type\":\"UnionRenderers\"}},\"id\":\"5653\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"label\":{\"value\":\"SVM - sigmoid\"},\"renderers\":[{\"id\":\"5774\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5796\",\"type\":\"LegendItem\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"5626\",\"type\":\"GlyphRenderer\"},{\"id\":\"5642\",\"type\":\"GlyphRenderer\"},{\"id\":\"5659\",\"type\":\"GlyphRenderer\"},{\"id\":\"5678\",\"type\":\"GlyphRenderer\"},{\"id\":\"5699\",\"type\":\"GlyphRenderer\"},{\"id\":\"5722\",\"type\":\"GlyphRenderer\"},{\"id\":\"5747\",\"type\":\"GlyphRenderer\"},{\"id\":\"5774\",\"type\":\"GlyphRenderer\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"Variable\",\"@{Variable}\"],[\"TICKER\",\"@{TICKER}\"],[\"value\",\"@{value}\"]]},\"id\":\"5578\",\"type\":\"HoverTool\"},{\"attributes\":{\"line_color\":\"#ff7e0e\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5639\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5654\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\",\"NN - linear\"],\"value\":{\"__ndarray__\":\"AAAAIPRR4T8AAADgxn3ePwAAAABfSOE/AAAAIFcQ4z8AAADg0tLiPwAAAOCcc+A/AAAAgF6q3z8AAABAziXhPw==\",\"dtype\":\"float64\",\"shape\":[8]}},\"selected\":{\"id\":\"5621\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"5651\",\"type\":\"UnionRenderers\"}},\"id\":\"5620\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":\"#d62628\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5675\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"5620\",\"type\":\"ColumnDataSource\"}},\"id\":\"5627\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"5716\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5719\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"5721\",\"type\":\"Line\"},\"nonselection_glyph\":{\"id\":\"5720\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5723\",\"type\":\"CDSView\"}},\"id\":\"5722\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\",\"SVM - linear\"],\"value\":{\"__ndarray__\":\"prUn/87l4D+JOB0mrOvfP7W4CfO7Wtw/fWejvrNR3z/x8PDw8PDgP3TOOeecc94/7muZLHKA4D+1uAnzu1rcPw==\",\"dtype\":\"float64\",\"shape\":[8]}},\"selected\":{\"id\":\"5694\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"5739\",\"type\":\"UnionRenderers\"}},\"id\":\"5693\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"label\":{\"value\":\"NN - sigmoid\"},\"renderers\":[{\"id\":\"5642\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5652\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#e277c1\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5745\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"factors\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"tags\":[[[\"TICKER\",\"TICKER\",null]]]},\"id\":\"5576\",\"type\":\"FactorRange\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#d62628\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5676\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"5741\",\"type\":\"ColumnDataSource\"}},\"id\":\"5748\",\"type\":\"CDSView\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#d62628\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5677\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5769\",\"type\":\"Selection\"},{\"attributes\":{\"data_source\":{\"id\":\"5672\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5675\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"5677\",\"type\":\"Line\"},\"nonselection_glyph\":{\"id\":\"5676\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5679\",\"type\":\"CDSView\"}},\"id\":\"5678\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\",\"SVM - poly\"],\"value\":{\"__ndarray__\":\"prUn/87l4D+JOB0mrOvfP7W4CfO7Wtw/fWejvrNR3z/x8PDw8PDgP3TOOeecc94/7muZLHKA4D+1uAnzu1rcPw==\",\"dtype\":\"float64\",\"shape\":[8]}},\"selected\":{\"id\":\"5742\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"5795\",\"type\":\"UnionRenderers\"}},\"id\":\"5741\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"label\":{\"value\":\"NN - tanh\"},\"renderers\":[{\"id\":\"5659\",\"type\":\"GlyphRenderer\"}]},\"id\":\"5671\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"5691\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"5596\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"5741\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5744\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"5746\",\"type\":\"Line\"},\"nonselection_glyph\":{\"id\":\"5745\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5748\",\"type\":\"CDSView\"}},\"id\":\"5747\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#8c564b\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5721\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"5768\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5771\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"5773\",\"type\":\"Line\"},\"nonselection_glyph\":{\"id\":\"5772\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5775\",\"type\":\"CDSView\"}},\"id\":\"5774\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"5619\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"source\":{\"id\":\"5716\",\"type\":\"ColumnDataSource\"}},\"id\":\"5723\",\"type\":\"CDSView\"},{\"attributes\":{\"line_color\":\"#e277c1\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5744\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"5673\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"5651\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"5608\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_color\":\"#7e7e7e\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5771\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - sigmoid\"],\"value\":{\"__ndarray__\":\"AAAA4Oo24T8AAAAAKNvdPwAAAEAsU98/AAAAoL6z4T8AAADg0tLiPwAAAMDWWt8/AAAAwNAq4D8AAAAgHL7gPw==\",\"dtype\":\"float64\",\"shape\":[8]}},\"selected\":{\"id\":\"5637\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"5670\",\"type\":\"UnionRenderers\"}},\"id\":\"5636\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_alpha\":0.2,\"line_color\":\"#e277c1\",\"line_width\":2,\"x\":{\"field\":\"TICKER\"},\"y\":{\"field\":\"value\"}},\"id\":\"5746\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AMZN\",\"SPY\",\"DOCU\",\"NFLX\",\"NKE\",\"PG\"],\"Variable\":[\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\",\"NN - relu\"],\"value\":{\"__ndarray__\":\"AAAAIOSu3z8AAADAXJrfPwAAAKA9A+E/AAAAICZX4D8AAAAgHh7ePwAAACCllN4/AAAAwJ7y4D8AAADgrODgPw==\",\"dtype\":\"float64\",\"shape\":[8]}},\"selected\":{\"id\":\"5673\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"5714\",\"type\":\"UnionRenderers\"}},\"id\":\"5672\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"5586\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"5a6a449f-aeff-4f90-8757-7726bbc84eb4\",\"roots\":{\"5586\":\"e1f84320-702f-48f3-bea1-373eb5c9f750\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":NdOverlay   [Variable]\n",
       "   :Curve   [TICKER]   (value)"
      ]
     },
     "execution_count": 654,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "5586"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_SUMMARY.hvplot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL - COVID19 HEADLINE EFFECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           SCORE\n",
      "Date            \n",
      "2020-09-22     1\n",
      "2020-09-02     1\n",
      "2020-08-11     1\n",
      "2020-06-30     0\n",
      "2020-06-04    -1\n",
      "2020-05-12     0\n",
      "2020-04-30     0\n",
      "2020-04-14     1\n",
      "2020-03-20     0\n",
      "2020-03-19    -1\n",
      "2020-03-18    -1\n",
      "2020-03-13     1\n",
      "2020-03-10    -1\n",
      "2020-03-03     1\n",
      "2020-03-02     0\n",
      "2020-02-24    -1\n",
      "2020-02-18    -1\n",
      "2020-02-07     1\n",
      "2020-02-06     0\n",
      "2020-02-05     1\n",
      "2020-02-04    -1\n",
      "2020-01-31    -1\n",
      "2020-01-30     0\n",
      "2020-01-28     1\n",
      "2020-01-27    -1\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.714\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      1.00      0.83         5\n",
      "    positive       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.71         7\n",
      "   macro avg       0.36      0.50      0.42         7\n",
      "weighted avg       0.51      0.71      0.60         7\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   TSLA\n",
      "[0.7142857142857143]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.714\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      1.00      0.83         5\n",
      "    positive       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.71         7\n",
      "   macro avg       0.36      0.50      0.42         7\n",
      "weighted avg       0.51      0.71      0.60         7\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   TSLA      0.714286\n",
      "[0.7142857142857143]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.714\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      1.00      0.83         5\n",
      "    positive       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.71         7\n",
      "   macro avg       0.36      0.50      0.42         7\n",
      "weighted avg       0.51      0.71      0.60         7\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   TSLA      0.714286   0.714286\n",
      "[0.7142857142857143]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.714\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      1.00      0.83         5\n",
      "    positive       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.71         7\n",
      "   macro avg       0.36      0.50      0.42         7\n",
      "weighted avg       0.51      0.71      0.60         7\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0   TSLA      0.714286   0.714286    0.714286\n",
      "[0.7142857142857143]\n",
      "           SCORE\n",
      "Date            \n",
      "2020-10-13    -1\n",
      "2020-09-16    -1\n",
      "2020-09-02     1\n",
      "2020-09-01     1\n",
      "2020-08-11     1\n",
      "2020-07-28    -1\n",
      "2020-07-02     1\n",
      "2020-06-25     0\n",
      "2020-06-24    -1\n",
      "2020-06-19    -1\n",
      "2020-05-26    -1\n",
      "2020-05-20     1\n",
      "2020-05-13     1\n",
      "2020-05-12     0\n",
      "2020-05-06    -1\n",
      "2020-04-03    -1\n",
      "2020-04-02    -1\n",
      "2020-04-01     0\n",
      "2020-03-26     1\n",
      "2020-03-20     1\n",
      "2020-03-19    -1\n",
      "2020-03-18     0\n",
      "2020-03-16    -1\n",
      "2020-03-10    -1\n",
      "2020-03-09     0\n",
      "2020-03-06    -1\n",
      "2020-03-05     0\n",
      "2020-03-04    -1\n",
      "2020-03-02     1\n",
      "2020-02-28     0\n",
      "2020-02-27     0\n",
      "2020-02-26     0\n",
      "2020-02-25     0\n",
      "2020-02-24    -1\n",
      "2020-02-21    -1\n",
      "2020-02-19     0\n",
      "2020-02-18    -1\n",
      "2020-02-13     0\n",
      "2020-02-10    -1\n",
      "2020-02-07     1\n",
      "2020-02-05    -1\n",
      "2020-02-03    -1\n",
      "2020-01-31    -1\n",
      "2020-01-30    -1\n",
      "2020-01-29     1\n",
      "2020-01-28    -1\n",
      "2020-01-27    -1\n",
      "2020-01-24     1\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.417\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.33      0.36         6\n",
      "    positive       0.43      0.50      0.46         6\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.41      0.42      0.41        12\n",
      "weighted avg       0.41      0.42      0.41        12\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER\n",
      "0   AAPL\n",
      "[0.4166666666666667]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.417\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.33      0.36         6\n",
      "    positive       0.43      0.50      0.46         6\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.41      0.42      0.41        12\n",
      "weighted avg       0.41      0.42      0.41        12\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear\n",
      "0   AAPL      0.416667\n",
      "[0.4166666666666667]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.417\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.33      0.36         6\n",
      "    positive       0.43      0.50      0.46         6\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.41      0.42      0.41        12\n",
      "weighted avg       0.41      0.42      0.41        12\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf\n",
      "0   AAPL      0.416667   0.416667\n",
      "[0.4166666666666667]\n",
      "-------- -----------------------------\n",
      "Test Acc: 0.417\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.33      0.36         6\n",
      "    positive       0.43      0.50      0.46         6\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.41      0.42      0.41        12\n",
      "weighted avg       0.41      0.42      0.41        12\n",
      "\n",
      "***\n",
      "[]\n",
      "***\n",
      "  TICKER  SVM - linear  SVM - rbf  SVM - poly\n",
      "0   AAPL      0.416667   0.416667    0.416667\n",
      "[0.4166666666666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\godz7\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#COVID\n",
    "TSLA_SVM = svm('TSLA',tsla_complete)\n",
    "#SPY_SVM = svm('SPY',spy_complete)\n",
    "AAPL_SVM = svm(\"AAPL\",aapl_complete)\n",
    "#AMZN_SVM = svm(\"AMZN\",amzn_complete)\n",
    "#DOCU_SVM = svm('DOCU',docu_complete)\n",
    "#NFLX_SVM = svm('NFLX',nflx_complete)\n",
    "#NKE_SVM = svm(\"NKE\",nke_complete)\n",
    "#PG_SVM = svm(\"PG\",pg_complete)\n",
    "\n",
    "SVM_SUMMARY_COVID = pd.concat([AAPL_SVM,TSLA_SVM]).reset_index(drop=True)\n",
    "\n",
    "SVM_SUMMARY_COVID = SVM_SUMMARY_COVID.set_index('TICKER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM - linear</th>\n",
       "      <th>SVM - rbf</th>\n",
       "      <th>SVM - poly</th>\n",
       "      <th>SVM - sigmoid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TICKER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SVM - linear  SVM - rbf  SVM - poly  SVM - sigmoid\n",
       "TICKER                                                    \n",
       "AAPL        0.416667   0.416667    0.416667       0.416667\n",
       "TSLA        0.714286   0.714286    0.714286       0.714286"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_SUMMARY_COVID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='7363' style='display: table; margin: 0 auto;'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"7ae14627-fdfc-44b5-97a4-ec9c0042e7d3\" data-root-id=\"7363\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"4265e416-c941-4c0d-86fd-a0c2a337c199\":{\"roots\":{\"references\":[{\"attributes\":{\"align\":null,\"below\":[{\"id\":\"7372\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"7375\",\"type\":\"Grid\"},{\"id\":\"7380\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"7376\",\"type\":\"LinearAxis\"}],\"margin\":null,\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"plot_height\":300,\"plot_width\":700,\"renderers\":[{\"id\":\"7400\",\"type\":\"GlyphRenderer\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"7364\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"7386\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"7360\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"7368\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"7361\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"7370\",\"type\":\"LinearScale\"}},\"id\":\"7363\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"7373\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"7411\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"7413\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"7400\",\"type\":\"GlyphRenderer\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"index\",\"@{index}\"],[\"Variable\",\"@{Variable}\"],[\"value\",\"@{value}\"]]},\"id\":\"7362\",\"type\":\"HoverTool\"},{\"attributes\":{\"source\":{\"id\":\"7394\",\"type\":\"ColumnDataSource\"}},\"id\":\"7401\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"factors\":[[\"0\",\"NN - linear\"],[\"0\",\"NN - relu\"],[\"0\",\"NN - sigmoid\"],[\"0\",\"NN - tanh\"],[\"1\",\"NN - linear\"],[\"1\",\"NN - relu\"],[\"1\",\"NN - sigmoid\"],[\"1\",\"NN - tanh\"]],\"tags\":[[[\"index\",\"index\",null],[\"Variable\",\"Variable\",null]]]},\"id\":\"7360\",\"type\":\"FactorRange\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"7362\",\"type\":\"HoverTool\"},{\"id\":\"7381\",\"type\":\"SaveTool\"},{\"id\":\"7382\",\"type\":\"PanTool\"},{\"id\":\"7383\",\"type\":\"WheelZoomTool\"},{\"id\":\"7384\",\"type\":\"BoxZoomTool\"},{\"id\":\"7385\",\"type\":\"ResetTool\"}]},\"id\":\"7386\",\"type\":\"Toolbar\"},{\"attributes\":{\"callback\":null,\"end\":0.75,\"reset_end\":0.75,\"reset_start\":0.0,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"7361\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"7405\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7393\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7399\",\"type\":\"VBar\"},{\"attributes\":{\"factors\":[\"NN - linear\",\"NN - sigmoid\",\"NN - tanh\",\"NN - relu\"],\"palette\":[\"#1f77b3\",\"#ff7e0e\",\"#2ba02b\",\"#d62628\"]},\"id\":\"7393\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{\"axis_label\":\"\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7403\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7373\",\"type\":\"CategoricalTicker\"}},\"id\":\"7372\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"7370\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"7383\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"7395\",\"type\":\"Selection\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"7373\",\"type\":\"CategoricalTicker\"}},\"id\":\"7375\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"data\":{\"Variable\":[\"NN - linear\",\"NN - linear\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - tanh\",\"NN - tanh\",\"NN - relu\",\"NN - relu\"],\"index\":[\"0\",\"1\",\"0\",\"1\",\"0\",\"1\",\"0\",\"1\"],\"value\":{\"__ndarray__\":\"AAAAAAAA6D8AAACgJEnSPwAAAAAAAOA/AAAAoCRJ4j8AAAAAAADgPwAAAOC2bds/AAAAYFVV5T8AAACgJEniPw==\",\"dtype\":\"float64\",\"shape\":[8]},\"xoffsets\":[[\"0\",\"NN - linear\"],[\"1\",\"NN - linear\"],[\"0\",\"NN - sigmoid\"],[\"1\",\"NN - sigmoid\"],[\"0\",\"NN - tanh\"],[\"1\",\"NN - tanh\"],[\"0\",\"NN - relu\"],[\"1\",\"NN - relu\"]]},\"selected\":{\"id\":\"7395\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"7413\",\"type\":\"UnionRenderers\"}},\"id\":\"7394\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"7381\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7393\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7398\",\"type\":\"VBar\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"7377\",\"type\":\"BasicTicker\"}},\"id\":\"7380\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"7368\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"7377\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis_label\":\"\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7405\",\"type\":\"BasicTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7377\",\"type\":\"BasicTicker\"}},\"id\":\"7376\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7393\",\"type\":\"CategoricalColorMapper\"}},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7397\",\"type\":\"VBar\"},{\"attributes\":{\"data_source\":{\"id\":\"7394\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7397\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"7399\",\"type\":\"VBar\"},\"nonselection_glyph\":{\"id\":\"7398\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"7401\",\"type\":\"CDSView\"}},\"id\":\"7400\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"overlay\":{\"id\":\"7411\",\"type\":\"BoxAnnotation\"}},\"id\":\"7384\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"7382\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"7403\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"7385\",\"type\":\"ResetTool\"},{\"attributes\":{\"text\":\"\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12pt\"}},\"id\":\"7364\",\"type\":\"Title\"}],\"root_ids\":[\"7363\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"4265e416-c941-4c0d-86fd-a0c2a337c199\",\"roots\":{\"7363\":\"7ae14627-fdfc-44b5-97a4-ec9c0042e7d3\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":Bars   [index,Variable]   (value)"
      ]
     },
     "execution_count": 751,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "7363"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_SUMMARY_COVID.hvplot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKER</th>\n",
       "      <th>NN - linear</th>\n",
       "      <th>NN - sigmoid</th>\n",
       "      <th>NN - tanh</th>\n",
       "      <th>NN - relu</th>\n",
       "      <th>SVM - linear</th>\n",
       "      <th>SVM - rbf</th>\n",
       "      <th>SVM - poly</th>\n",
       "      <th>SVM - sigmoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TICKER  NN - linear  NN - sigmoid  NN - tanh  NN - relu  SVM - linear  \\\n",
       "0   AAPL     0.750000      0.500000   0.500000   0.666667      0.416667   \n",
       "1   TSLA     0.285714      0.571429   0.428571   0.571429      0.714286   \n",
       "\n",
       "   SVM - rbf  SVM - poly  SVM - sigmoid  \n",
       "0   0.416667    0.416667       0.416667  \n",
       "1   0.714286    0.714286       0.714286  "
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_SUMMARY_COVID = NN_SUMMARY_COVID.merge(SVM_SUMMARY_COVID,on='TICKER',how='left')\n",
    "ML_SUMMARY_COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_SUMMARY_COVID = ML_SUMMARY_COVID.set_index('TICKER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='7583' style='display: table; margin: 0 auto;'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"cb4cfe92-f906-40e3-b80d-51ffb7188aba\" data-root-id=\"7583\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"b51bd2f5-26be-4c82-b6ca-836737955b48\":{\"roots\":{\"references\":[{\"attributes\":{\"align\":null,\"below\":[{\"id\":\"7592\",\"type\":\"CategoricalAxis\"}],\"center\":[{\"id\":\"7595\",\"type\":\"Grid\"},{\"id\":\"7600\",\"type\":\"Grid\"}],\"left\":[{\"id\":\"7596\",\"type\":\"LinearAxis\"}],\"margin\":null,\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"plot_height\":300,\"plot_width\":700,\"renderers\":[{\"id\":\"7620\",\"type\":\"GlyphRenderer\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"7584\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"7606\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"7580\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"7588\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"7581\",\"type\":\"Range1d\"},\"y_scale\":{\"id\":\"7590\",\"type\":\"LinearScale\"}},\"id\":\"7583\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"7615\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"7623\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"axis_label\":\"\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7625\",\"type\":\"BasicTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7597\",\"type\":\"BasicTicker\"}},\"id\":\"7596\",\"type\":\"LinearAxis\"},{\"attributes\":{\"grid_line_color\":null,\"ticker\":{\"id\":\"7593\",\"type\":\"CategoricalTicker\"}},\"id\":\"7595\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"7605\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"7602\",\"type\":\"PanTool\"},{\"attributes\":{\"source\":{\"id\":\"7614\",\"type\":\"ColumnDataSource\"}},\"id\":\"7621\",\"type\":\"CDSView\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"7631\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"7593\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"7601\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7613\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7619\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"end\":0.75,\"reset_end\":0.75,\"reset_start\":0.0,\"tags\":[[[\"value\",\"value\",null]]]},\"id\":\"7581\",\"type\":\"Range1d\"},{\"attributes\":{\"callback\":null,\"factors\":[[\"AAPL\",\"NN - linear\"],[\"AAPL\",\"NN - relu\"],[\"AAPL\",\"NN - sigmoid\"],[\"AAPL\",\"NN - tanh\"],[\"AAPL\",\"SVM - linear\"],[\"AAPL\",\"SVM - poly\"],[\"AAPL\",\"SVM - rbf\"],[\"AAPL\",\"SVM - sigmoid\"],[\"TSLA\",\"NN - linear\"],[\"TSLA\",\"NN - relu\"],[\"TSLA\",\"NN - sigmoid\"],[\"TSLA\",\"NN - tanh\"],[\"TSLA\",\"SVM - linear\"],[\"TSLA\",\"SVM - poly\"],[\"TSLA\",\"SVM - rbf\"],[\"TSLA\",\"SVM - sigmoid\"]],\"tags\":[[[\"TICKER\",\"TICKER\",null],[\"Variable\",\"Variable\",null]]]},\"id\":\"7580\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"7625\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"7597\",\"type\":\"BasicTicker\"},{\"attributes\":{\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7613\",\"type\":\"CategoricalColorMapper\"}},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7617\",\"type\":\"VBar\"},{\"attributes\":{\"text\":\"\",\"text_color\":{\"value\":\"black\"},\"text_font_size\":{\"value\":\"12pt\"}},\"id\":\"7584\",\"type\":\"Title\"},{\"attributes\":{\"overlay\":{\"id\":\"7631\",\"type\":\"BoxAnnotation\"}},\"id\":\"7604\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"7620\",\"type\":\"GlyphRenderer\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"TICKER\",\"@{TICKER}\"],[\"Variable\",\"@{Variable}\"],[\"value\",\"@{value}\"]]},\"id\":\"7582\",\"type\":\"HoverTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"Variable\",\"transform\":{\"id\":\"7613\",\"type\":\"CategoricalColorMapper\"}},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"black\"},\"top\":{\"field\":\"value\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"xoffsets\"}},\"id\":\"7618\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"7633\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"7590\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis_label\":\"TICKER, Variable\",\"bounds\":\"auto\",\"formatter\":{\"id\":\"7623\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"horizontal\",\"ticker\":{\"id\":\"7593\",\"type\":\"CategoricalTicker\"}},\"id\":\"7592\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"callback\":null,\"data\":{\"TICKER\":[\"AAPL\",\"TSLA\",\"AAPL\",\"TSLA\",\"AAPL\",\"TSLA\",\"AAPL\",\"TSLA\",\"AAPL\",\"TSLA\",\"AAPL\",\"TSLA\",\"AAPL\",\"TSLA\",\"AAPL\",\"TSLA\"],\"Variable\":[\"NN - linear\",\"NN - linear\",\"NN - sigmoid\",\"NN - sigmoid\",\"NN - tanh\",\"NN - tanh\",\"NN - relu\",\"NN - relu\",\"SVM - linear\",\"SVM - linear\",\"SVM - rbf\",\"SVM - rbf\",\"SVM - poly\",\"SVM - poly\",\"SVM - sigmoid\",\"SVM - sigmoid\"],\"value\":{\"__ndarray__\":\"AAAAAAAA6D8AAACgJEnSPwAAAAAAAOA/AAAAoCRJ4j8AAAAAAADgPwAAAOC2bds/AAAAYFVV5T8AAACgJEniP6uqqqqqqto/t23btm3b5j+rqqqqqqraP7dt27Zt2+Y/q6qqqqqq2j+3bdu2bdvmP6uqqqqqqto/t23btm3b5j8=\",\"dtype\":\"float64\",\"shape\":[16]},\"xoffsets\":[[\"AAPL\",\"NN - linear\"],[\"TSLA\",\"NN - linear\"],[\"AAPL\",\"NN - sigmoid\"],[\"TSLA\",\"NN - sigmoid\"],[\"AAPL\",\"NN - tanh\"],[\"TSLA\",\"NN - tanh\"],[\"AAPL\",\"NN - relu\"],[\"TSLA\",\"NN - relu\"],[\"AAPL\",\"SVM - linear\"],[\"TSLA\",\"SVM - linear\"],[\"AAPL\",\"SVM - rbf\"],[\"TSLA\",\"SVM - rbf\"],[\"AAPL\",\"SVM - poly\"],[\"TSLA\",\"SVM - poly\"],[\"AAPL\",\"SVM - sigmoid\"],[\"TSLA\",\"SVM - sigmoid\"]]},\"selected\":{\"id\":\"7615\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"7633\",\"type\":\"UnionRenderers\"}},\"id\":\"7614\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"7588\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"factors\":[\"NN - linear\",\"NN - sigmoid\",\"NN - tanh\",\"NN - relu\",\"SVM - linear\",\"SVM - rbf\",\"SVM - poly\",\"SVM - sigmoid\"],\"palette\":[\"#1f77b3\",\"#ff7e0e\",\"#2ba02b\",\"#d62628\",\"#9367bc\",\"#8c564b\",\"#e277c1\",\"#7e7e7e\"]},\"id\":\"7613\",\"type\":\"CategoricalColorMapper\"},{\"attributes\":{\"data_source\":{\"id\":\"7614\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7617\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"7619\",\"type\":\"VBar\"},\"nonselection_glyph\":{\"id\":\"7618\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"7621\",\"type\":\"CDSView\"}},\"id\":\"7620\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"7603\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"7582\",\"type\":\"HoverTool\"},{\"id\":\"7601\",\"type\":\"SaveTool\"},{\"id\":\"7602\",\"type\":\"PanTool\"},{\"id\":\"7603\",\"type\":\"WheelZoomTool\"},{\"id\":\"7604\",\"type\":\"BoxZoomTool\"},{\"id\":\"7605\",\"type\":\"ResetTool\"}]},\"id\":\"7606\",\"type\":\"Toolbar\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":null,\"ticker\":{\"id\":\"7597\",\"type\":\"BasicTicker\"}},\"id\":\"7600\",\"type\":\"Grid\"}],\"root_ids\":[\"7583\"]},\"title\":\"Bokeh Application\",\"version\":\"1.2.0\"}};\n",
       "  var render_items = [{\"docid\":\"b51bd2f5-26be-4c82-b6ca-836737955b48\",\"roots\":{\"7583\":\"cb4cfe92-f906-40e3-b80d-51ffb7188aba\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":Bars   [TICKER,Variable]   (value)"
      ]
     },
     "execution_count": 754,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "7583"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ML_SUMMARY_COVID.hvplot.bar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
