{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from splinter import Browser\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\14694\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv('ALPACA_API_KEY')\n",
    "alpaca_secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "\n",
    "api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version='v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_info_grab(ticker):\n",
    "    \"\"\"\n",
    "    Takes ticker symbol and returns DataFrame with Date, Close, and Pct Change columns.\n",
    "    \"\"\"\n",
    "    # Set timeframe to '1D'\n",
    "    timeframe = \"1D\"\n",
    "    ticker = ticker.upper()\n",
    "    \n",
    "    # Set current date and the date from one month ago using the ISO format\n",
    "    current_date = pd.Timestamp(\"2020-11-09\", tz=\"America/New_York\").isoformat()\n",
    "    past_date = pd.Timestamp(\"2016-08-27\", tz=\"America/New_York\").isoformat()\n",
    "\n",
    "    df = api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit=None,\n",
    "        start=past_date,\n",
    "        end=current_date,\n",
    "        after=None,\n",
    "        until=None,\n",
    "    ).df\n",
    "    df = df.droplevel(axis=1, level=0)\n",
    "    df.index = df.index.date\n",
    "    df['pct change'] = df['close'].pct_change()\n",
    "    df['pct change'].dropna\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['open', 'high', 'low', 'volume'])\n",
    "    df = df.rename(columns={'index':'Date'})\n",
    "    df = df.set_index('Date')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>pct change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-08-29</th>\n",
       "      <td>106.820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-30</th>\n",
       "      <td>105.990</td>\n",
       "      <td>-0.007770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>106.110</td>\n",
       "      <td>0.001132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-01</th>\n",
       "      <td>106.730</td>\n",
       "      <td>0.005843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-02</th>\n",
       "      <td>107.730</td>\n",
       "      <td>0.009369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>110.375</td>\n",
       "      <td>0.014756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-04</th>\n",
       "      <td>114.940</td>\n",
       "      <td>0.041359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05</th>\n",
       "      <td>118.990</td>\n",
       "      <td>0.035236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-06</th>\n",
       "      <td>118.685</td>\n",
       "      <td>-0.002563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-09</th>\n",
       "      <td>116.320</td>\n",
       "      <td>-0.019927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1058 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              close  pct change\n",
       "Date                           \n",
       "2016-08-29  106.820         NaN\n",
       "2016-08-30  105.990   -0.007770\n",
       "2016-08-31  106.110    0.001132\n",
       "2016-09-01  106.730    0.005843\n",
       "2016-09-02  107.730    0.009369\n",
       "...             ...         ...\n",
       "2020-11-03  110.375    0.014756\n",
       "2020-11-04  114.940    0.041359\n",
       "2020-11-05  118.990    0.035236\n",
       "2020-11-06  118.685   -0.002563\n",
       "2020-11-09  116.320   -0.019927\n",
       "\n",
       "[1058 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_stock_info = stock_info_grab(\"AAPL\")\n",
    "amzn_stock_info = stock_info_grab(\"AMZN\")\n",
    "tsla_stock_info = stock_info_grab(\"TSLA\")\n",
    "spy_stock_info = stock_info_grab(\"SPY\")\n",
    "docu_stock_info = stock_info_grab(\"DOCU\")\n",
    "nflx_stock_info = stock_info_grab(\"NFLX\")\n",
    "nke_stock_info = stock_info_grab(\"nke\")\n",
    "pg_stock_info = stock_info_grab(\"PG\")\n",
    "aapl_stock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headlines Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created headlines_webscraper(symbol, pages) function that goes to Market Watch and scrapes all headlines from an infinite scroll frame. Each run takes approximately 15 minutes to complete for larger companies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headlines_webscraper(symbol, pages):\n",
    "    \"\"\"\n",
    "    Req: symbol = ticker symbol\n",
    "         pages = number of pages\n",
    "    Grabs headlines from MarketWatch historical news & creates dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {'Headline': [], 'Date': []}\n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    for x in range(0,pages):\n",
    "        print(f\"Processing page {x}\")\n",
    "        url = f\"https://www.marketwatch.com/investing/stock/{symbol}/moreheadlines?channel=MarketWatch&pageNumber={x}\"\n",
    "        browser.visit(url)\n",
    "\n",
    "        for y in range(0,len(browser.find_by_css('h3[class=\\\"article__headline\\\"]'))):\n",
    "            df = df.append({'Headline':browser.find_by_css('h3[class=\\\"article__headline\\\"]')[y].text,\n",
    "                            'Date':browser.find_by_css('span[class=\\\"article__timestamp\\\"]')[y].text},ignore_index=True)\n",
    "\n",
    "    return df         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 0\n",
      "Processing page 1\n",
      "Processing page 2\n",
      "Processing page 3\n",
      "Processing page 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zoom, Peloton, Netflix stocks among stay-home ...</td>\n",
       "      <td>Nov. 16, 2020 at 8:30 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These stocks rose the most Wednesday as invest...</td>\n",
       "      <td>Nov. 4, 2020 at 5:18 p.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAP’s Grim Warning Is Weighing on Enterprise S...</td>\n",
       "      <td>Oct. 26, 2020 at 3:22 p.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Software-Stock Assessment: 4 to Buy, 4 to Sk...</td>\n",
       "      <td>Oct. 14, 2020 at 2:10 p.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DocuSign stock surges after Morgan Stanley upg...</td>\n",
       "      <td>Oct. 5, 2020 at 11:39 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Google is a great investor, and Alphabet earni...</td>\n",
       "      <td>Jul. 24, 2018 at 7:14 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>DocuSign founder to leave board amid shake-up</td>\n",
       "      <td>Jul. 11, 2018 at 4:48 p.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>IPO market has busiest quarter in three years,...</td>\n",
       "      <td>Jul. 5, 2018 at 7:21 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Why it’s worth holding U.S. stocks even if tra...</td>\n",
       "      <td>Jun. 8, 2018 at 9:46 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>U.S. stocks open slightly lower on trade tensi...</td>\n",
       "      <td>Jun. 8, 2018 at 9:31 a.m. ET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline  \\\n",
       "0   Zoom, Peloton, Netflix stocks among stay-home ...   \n",
       "1   These stocks rose the most Wednesday as invest...   \n",
       "2   SAP’s Grim Warning Is Weighing on Enterprise S...   \n",
       "3   A Software-Stock Assessment: 4 to Buy, 4 to Sk...   \n",
       "4   DocuSign stock surges after Morgan Stanley upg...   \n",
       "..                                                ...   \n",
       "95  Google is a great investor, and Alphabet earni...   \n",
       "96      DocuSign founder to leave board amid shake-up   \n",
       "97  IPO market has busiest quarter in three years,...   \n",
       "98  Why it’s worth holding U.S. stocks even if tra...   \n",
       "99  U.S. stocks open slightly lower on trade tensi...   \n",
       "\n",
       "                             Date  \n",
       "0   Nov. 16, 2020 at 8:30 a.m. ET  \n",
       "1    Nov. 4, 2020 at 5:18 p.m. ET  \n",
       "2   Oct. 26, 2020 at 3:22 p.m. ET  \n",
       "3   Oct. 14, 2020 at 2:10 p.m. ET  \n",
       "4   Oct. 5, 2020 at 11:39 a.m. ET  \n",
       "..                            ...  \n",
       "95  Jul. 24, 2018 at 7:14 a.m. ET  \n",
       "96  Jul. 11, 2018 at 4:48 p.m. ET  \n",
       "97   Jul. 5, 2018 at 7:21 a.m. ET  \n",
       "98   Jun. 8, 2018 at 9:46 a.m. ET  \n",
       "99   Jun. 8, 2018 at 9:31 a.m. ET  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Docu to show how this works\n",
    "docu_headlines = headlines_webscraper(\"docu\", 5)\n",
    "docu_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_headlines = docu_headlines.drop_duplicates(subset=['Headline']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_headlines.to_csv('docu_headlines.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-populated csv files for Apple, Amazon, Docusign, Netflix, Nike, Proctor & Gamble, S&P 500, Tesla\n",
    "aapl_file = Path('Resources/AAPL_HEADLINES.csv')\n",
    "amzn_file = Path('Resources/amzn_headlines.csv')\n",
    "docu_file = Path('Resources/docu_headlines.csv')\n",
    "nflx_file = Path('Resources/nflx_headlines.csv')\n",
    "nke_file = Path('Resources/nke_headlines.csv')\n",
    "pg_file = Path('Resources/pg_headlines.csv')\n",
    "spy_file = Path('Resources/SPY_HEADLINES.csv')\n",
    "tsla_file = Path('Resources/TSLA_HEADLINES.csv')\n",
    "\n",
    "aapl_headlines_df = pd.read_csv(aapl_file)\n",
    "amzn_headlines_df = pd.read_csv(amzn_file)\n",
    "docu_headlines_df = pd.read_csv(docu_file)\n",
    "nflx_headlines_df = pd.read_csv(nflx_file)\n",
    "nke_headlines_df = pd.read_csv(nke_file)\n",
    "pg_headlines_df = pd.read_csv(pg_file)\n",
    "spy_headlines_df = pd.read_csv(spy_file)\n",
    "tsla_headlines_df = pd.read_csv(tsla_file)\n",
    "aapl_headlines_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_df(df):\n",
    "    \"\"\"\n",
    "    Takes headlines DataFrame & creates DataFrame with Sentiment columns.\n",
    "    Splits Date & Time, creates Time column and moves Date to Index.\n",
    "    \"\"\"\n",
    "    title_sent = {\n",
    "        \"compound\": [],\n",
    "        \"positive\": [],\n",
    "        \"neutral\": [],\n",
    "        \"negative\": [],\n",
    "        \"sentiment\": [],\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Sentiment scoring with VADER\n",
    "            title_sentiment = analyzer.polarity_scores(row[\"Headline\"])\n",
    "            title_sent[\"compound\"].append(title_sentiment[\"compound\"])\n",
    "            title_sent[\"positive\"].append(title_sentiment[\"pos\"])\n",
    "            title_sent[\"neutral\"].append(title_sentiment[\"neu\"])\n",
    "            title_sent[\"negative\"].append(title_sentiment[\"neg\"])\n",
    "            title_sent[\"sentiment\"].append(get_sentiment(title_sentiment[\"compound\"]))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    title_sent_df = pd.DataFrame(title_sent)\n",
    "    #title_sent_df.head()\n",
    "\n",
    "    headline_sentiment_df = df.join(title_sent_df)\n",
    "    headline_sentiment_df.dropna()\n",
    "    headline_sentiment_df['Date'] = headline_sentiment_df['Date'].str.replace('at','-')\n",
    "    headline_sentiment_df['Date'] = headline_sentiment_df['Date'].str.split('-').str[0]\n",
    "    headline_sentiment_df = headline_sentiment_df.reindex(columns=['Date', 'Headline', 'compound', 'positive', 'neutral', 'negative', 'sentiment'])\n",
    "    headline_sentiment_df['Date'] = pd.to_datetime(headline_sentiment_df['Date'])\n",
    "    headline_sentiment_df.set_index('Date')\n",
    "    return headline_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(headlines_df, stock_info):\n",
    "    \"\"\"\n",
    "    Takes imported headlines_df, creates sentiment score, restructures data and\n",
    "    concats with stock info. \n",
    "    \"\"\"\n",
    "    headlines = create_sentiment_df(headlines_df)\n",
    "    scores = headlines.groupby('Date').mean().sort_values(by='Date')\n",
    "    scores = scores.drop(columns='compound')\n",
    "    complete = pd.concat([scores,stock_info], join='outer', axis=1).dropna()\n",
    "    complete['predicted pct change'] = complete['pct change'].shift()\n",
    "    complete = complete.dropna()\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data with data_clean() for all stocks\n",
    "aapl_complete = data_clean(aapl_headlines_df,aapl_stock_info)\n",
    "amzn_complete = data_clean(amzn_headlines_df,amzn_stock_info)\n",
    "docu_complete = data_clean(docu_headlines_df,docu_stock_info)\n",
    "nflx_complete = data_clean(nflx_headlines_df,nflx_stock_info)\n",
    "nke_complete = data_clean(nke_headlines_df,nke_stock_info)\n",
    "pg_complete = data_clean(pg_headlines_df,pg_stock_info)\n",
    "spy_complete = data_clean(spy_headlines_df,spy_stock_info)\n",
    "tsla_complete = data_clean(tsla_headlines_df,tsla_stock_info)\n",
    "aapl_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aapl_headlines = create_sentiment_df(aapl_headlines_df)\n",
    "amzn_headlines = create_sentiment_df(amzn_headlines_df)\n",
    "docu_headlines = create_sentiment_df(docu_headlines_df)\n",
    "nflx_headlines = create_sentiment_df(nflx_headlines_df)\n",
    "nke_headlines = create_sentiment_df(nke_headlines_df)\n",
    "pg_headlines = create_sentiment_df(pg_headlines_df)\n",
    "spy_headlines = create_sentiment_df(spy_headlines_df)\n",
    "tsla_headlines = create_sentiment_df(tsla_headlines_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# find average sentiment score by date\n",
    "aapl_scores = aapl_headlines.groupby('Date').mean().sort_values(by='Date')\n",
    "amzn_scores = amzn_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "docu_scores = docu_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "nflx_scores = nflx_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "nke_scores = nke_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "pg_scores = pg_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "spy_scores = spy_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "tsla_scores = tsla_headlines.groupby(['Date']).mean().sort_values(by='Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#drop compund col on all scores\n",
    "aapl_scores = aapl_scores.drop(columns='compound')\n",
    "amzn_scores = amzn_scores.drop(columns='compound')\n",
    "docu_scores = docu_scores.drop(columns='compound')\n",
    "nflx_scores = nflx_scores.drop(columns='compound')\n",
    "nke_scores = nke_scores.drop(columns='compound')\n",
    "pg_scores = pg_scores.drop(columns='compound')\n",
    "spy_scores = spy_scores.drop(columns='compound')\n",
    "tsla_scores = tsla_scores.drop(columns='compound')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sentiment scores distribution across each df poss use histogram, calc meanstd, or percentiles \n",
    "aapl_complete = pd.concat([aapl_scores,aapl_stock_info], join='outer', axis=1).dropna()\n",
    "amzn_complete = pd.concat([amzn_scores,amzn_stock_info], join='outer', axis=1).dropna()\n",
    "docu_complete = pd.concat([docu_scores,docu_stock_info], join='outer', axis=1).dropna()\n",
    "nflx_complete = pd.concat([nflx_scores,nflx_stock_info], join='outer', axis=1).dropna()\n",
    "nke_complete = pd.concat([nke_scores,nke_stock_info], join='outer', axis=1).dropna()\n",
    "pg_complete = pd.concat([pg_scores,pg_stock_info], join='outer', axis=1).dropna()\n",
    "spy_complete = pd.concat([spy_scores,spy_stock_info], join='outer', axis=1).dropna()\n",
    "tsla_complete = pd.concat([tsla_scores,tsla_stock_info], join='outer', axis=1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: shift aapl_complete['pct change'] one day on all dfs\n",
    "# TO DO: dropna() on all df['predicted pct change'] cols \n",
    "aapl_complete['predicted pct change'] = aapl_complete['pct change'].shift()\n",
    "amzn_complete['predicted pct change'] = amzn_complete['pct change'].shift()\n",
    "docu_complete['predicted pct change'] = docu_complete['pct change'].shift()\n",
    "nflx_complete['predicted pct change'] = nflx_complete['pct change'].shift()\n",
    "nke_complete['predicted pct change'] = nke_complete['pct change'].shift()\n",
    "pg_complete['predicted pct change'] = pg_complete['pct change'].shift()\n",
    "spy_complete['predicted pct change'] = spy_complete['pct change'].shift()\n",
    "tsla_complete['predicted pct change'] = tsla_complete['pct change'].shift()\n",
    "\n",
    "aapl_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aapl_complete = aapl_complete.dropna()\n",
    "amzn_complete = amzn_complete.dropna()\n",
    "tsla_complete = tsla_complete.dropna()\n",
    "spy_complete = spy_complete.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(df):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = [\n",
    "        (df.iloc[:,3] >= 0.01),\n",
    "        (df.iloc[:,3] <= 0.00)\n",
    "    ]\n",
    "    \n",
    "    values = [0, 1]\n",
    "    \n",
    "    df['buy/sell'] = np.select(result, values)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_sentiment = get_sentiment(aapl_complete)\n",
    "amzn_sentiment = get_sentiment(amzn_complete)\n",
    "docu_sentiment = get_sentiment(docu_complete)\n",
    "nflx_sentiment = get_sentiment(nflx_complete)\n",
    "nke_sentiment = get_sentiment(nke_complete)\n",
    "pg_sentiment = get_sentiment(pg_complete)\n",
    "spy_sentiment = get_sentiment(spy_complete)\n",
    "tsla_sentiment = get_sentiment(tsla_complete)\n",
    "aapl_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def regression_analysis(df):\n",
    "    y = df['buy/sell']\n",
    "    X = df.drop(columns=['buy/sell', 'pct change', 'close', 'positive', 'neutral', 'negative', 'sentiment'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,  stratify=y)\n",
    "\n",
    "    classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "    print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")\n",
    "    predictions = classifier.predict(X_test)\n",
    "    results = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aapl_analysis = regression_analysis(aapl_sentiment)\n",
    "amzn_analysis = regression_analysis(amzn_sentiment)\n",
    "docu_analysis = regression_analysis(docu_sentiment)\n",
    "nflx_analysis = regression_analysis(nflx_sentiment)\n",
    "nke_analysis = regression_analysis(nke_sentiment)\n",
    "pg_analysis = regression_analysis(pg_sentiment)\n",
    "spy_analysis = regression_analysis(spy_sentiment)\n",
    "tsla_analysis = regression_analysis(tsla_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y = aapl_complete_sentiment['buy/sell']\n",
    "X = aapl_complete_sentiment.drop(columns=['buy/sell', 'pct change', 'close', 'positive', 'neutral', 'negative', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,  stratify=y)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {classifier.accuracy_score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "results = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
