{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from splinter import Browser\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\14694\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv('ALPACA_API_KEY')\n",
    "alpaca_secret_key = os.getenv('ALPACA_SECRET_KEY')\n",
    "\n",
    "api = tradeapi.REST(alpaca_api_key, alpaca_secret_key, api_version='v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_info_grab(ticker):\n",
    "    \"\"\"\n",
    "    Takes ticker symbol and returns DataFrame with Date, Close, and Pct Change columns.\n",
    "    \"\"\"\n",
    "    # Set timeframe to '1D'\n",
    "    timeframe = \"1D\"\n",
    "    ticker = ticker.upper()\n",
    "    \n",
    "    # Set current date and the date from one month ago using the ISO format\n",
    "    current_date = pd.Timestamp(\"2020-11-09\", tz=\"America/New_York\").isoformat()\n",
    "    past_date = pd.Timestamp(\"2016-08-27\", tz=\"America/New_York\").isoformat()\n",
    "\n",
    "    df = api.get_barset(\n",
    "        ticker,\n",
    "        timeframe,\n",
    "        limit=None,\n",
    "        start=past_date,\n",
    "        end=current_date,\n",
    "        after=None,\n",
    "        until=None,\n",
    "    ).df\n",
    "    df = df.droplevel(axis=1, level=0)\n",
    "    df.index = df.index.date\n",
    "    df['pct change'] = df['close'].pct_change()\n",
    "    df['pct change'].dropna\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['open', 'high', 'low', 'volume'])\n",
    "    df = df.rename(columns={'index':'Date'})\n",
    "    df = df.set_index('Date')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>pct change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-08-29</th>\n",
       "      <td>106.820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-30</th>\n",
       "      <td>105.990</td>\n",
       "      <td>-0.007770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>106.110</td>\n",
       "      <td>0.001132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-01</th>\n",
       "      <td>106.730</td>\n",
       "      <td>0.005843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-02</th>\n",
       "      <td>107.730</td>\n",
       "      <td>0.009369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-03</th>\n",
       "      <td>110.375</td>\n",
       "      <td>0.014756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-04</th>\n",
       "      <td>114.940</td>\n",
       "      <td>0.041359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-05</th>\n",
       "      <td>118.990</td>\n",
       "      <td>0.035236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-06</th>\n",
       "      <td>118.685</td>\n",
       "      <td>-0.002563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-09</th>\n",
       "      <td>116.320</td>\n",
       "      <td>-0.019927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1058 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              close  pct change\n",
       "Date                           \n",
       "2016-08-29  106.820         NaN\n",
       "2016-08-30  105.990   -0.007770\n",
       "2016-08-31  106.110    0.001132\n",
       "2016-09-01  106.730    0.005843\n",
       "2016-09-02  107.730    0.009369\n",
       "...             ...         ...\n",
       "2020-11-03  110.375    0.014756\n",
       "2020-11-04  114.940    0.041359\n",
       "2020-11-05  118.990    0.035236\n",
       "2020-11-06  118.685   -0.002563\n",
       "2020-11-09  116.320   -0.019927\n",
       "\n",
       "[1058 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_stock_info = stock_info_grab(\"AAPL\")\n",
    "amzn_stock_info = stock_info_grab(\"AMZN\")\n",
    "tsla_stock_info = stock_info_grab(\"TSLA\")\n",
    "spy_stock_info = stock_info_grab(\"SPY\")\n",
    "docu_stock_info = stock_info_grab(\"DOCU\")\n",
    "nflx_stock_info = stock_info_grab(\"NFLX\")\n",
    "nke_stock_info = stock_info_grab(\"nke\")\n",
    "pg_stock_info = stock_info_grab(\"PG\")\n",
    "aapl_stock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headlines Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created headlines_webscraper(symbol, pages) function that goes to Market Watch and scrapes all headlines from an infinite scroll frame. Each run takes approximately 15 minutes to complete for larger companies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headlines_webscraper(symbol, pages):\n",
    "    \"\"\"\n",
    "    Req: symbol = ticker symbol\n",
    "         pages = number of pages\n",
    "    Grabs headlines from MarketWatch historical news & creates dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    d = {'Headline': [], 'Date': []}\n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    for x in range(0,pages):\n",
    "        print(f\"Processing page {x}\")\n",
    "        url = f\"https://www.marketwatch.com/investing/stock/{symbol}/moreheadlines?channel=MarketWatch&pageNumber={x}\"\n",
    "        browser.visit(url)\n",
    "\n",
    "        for y in range(0,len(browser.find_by_css('h3[class=\\\"article__headline\\\"]'))):\n",
    "            df = df.append({'Headline':browser.find_by_css('h3[class=\\\"article__headline\\\"]')[y].text,\n",
    "                            'Date':browser.find_by_css('span[class=\\\"article__timestamp\\\"]')[y].text},ignore_index=True)\n",
    "\n",
    "    return df         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 0\n",
      "Processing page 1\n",
      "Processing page 2\n",
      "Processing page 3\n",
      "Processing page 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zoom, Peloton, Netflix stocks among stay-home ...</td>\n",
       "      <td>Nov. 16, 2020 at 8:30 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These stocks rose the most Wednesday as invest...</td>\n",
       "      <td>Nov. 4, 2020 at 5:18 p.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAPâ€™s Grim Warning Is Weighing on Enterprise S...</td>\n",
       "      <td>Oct. 26, 2020 at 3:22 p.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Software-Stock Assessment: 4 to Buy, 4 to Sk...</td>\n",
       "      <td>Oct. 14, 2020 at 2:10 p.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DocuSign stock surges after Morgan Stanley upg...</td>\n",
       "      <td>Oct. 5, 2020 at 11:39 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Google is a great investor, and Alphabet earni...</td>\n",
       "      <td>Jul. 24, 2018 at 7:14 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>DocuSign founder to leave board amid shake-up</td>\n",
       "      <td>Jul. 11, 2018 at 4:48 p.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>IPO market has busiest quarter in three years,...</td>\n",
       "      <td>Jul. 5, 2018 at 7:21 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Why itâ€™s worth holding U.S. stocks even if tra...</td>\n",
       "      <td>Jun. 8, 2018 at 9:46 a.m. ET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>U.S. stocks open slightly lower on trade tensi...</td>\n",
       "      <td>Jun. 8, 2018 at 9:31 a.m. ET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline  \\\n",
       "0   Zoom, Peloton, Netflix stocks among stay-home ...   \n",
       "1   These stocks rose the most Wednesday as invest...   \n",
       "2   SAPâ€™s Grim Warning Is Weighing on Enterprise S...   \n",
       "3   A Software-Stock Assessment: 4 to Buy, 4 to Sk...   \n",
       "4   DocuSign stock surges after Morgan Stanley upg...   \n",
       "..                                                ...   \n",
       "95  Google is a great investor, and Alphabet earni...   \n",
       "96      DocuSign founder to leave board amid shake-up   \n",
       "97  IPO market has busiest quarter in three years,...   \n",
       "98  Why itâ€™s worth holding U.S. stocks even if tra...   \n",
       "99  U.S. stocks open slightly lower on trade tensi...   \n",
       "\n",
       "                             Date  \n",
       "0   Nov. 16, 2020 at 8:30 a.m. ET  \n",
       "1    Nov. 4, 2020 at 5:18 p.m. ET  \n",
       "2   Oct. 26, 2020 at 3:22 p.m. ET  \n",
       "3   Oct. 14, 2020 at 2:10 p.m. ET  \n",
       "4   Oct. 5, 2020 at 11:39 a.m. ET  \n",
       "..                            ...  \n",
       "95  Jul. 24, 2018 at 7:14 a.m. ET  \n",
       "96  Jul. 11, 2018 at 4:48 p.m. ET  \n",
       "97   Jul. 5, 2018 at 7:21 a.m. ET  \n",
       "98   Jun. 8, 2018 at 9:46 a.m. ET  \n",
       "99   Jun. 8, 2018 at 9:31 a.m. ET  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Docu to show how this works\n",
    "docu_headlines = headlines_webscraper(\"docu\", 5)\n",
    "docu_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_headlines = docu_headlines.drop_duplicates(subset=['Headline']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu_headlines.to_csv('docu_headlines.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-populated csv files for Apple, Amazon, Docusign, Netflix, Nike, Proctor & Gamble, S&P 500, Tesla\n",
    "aapl_file = Path('Resources/AAPL_HEADLINES.csv')\n",
    "amzn_file = Path('Resources/amzn_headlines.csv')\n",
    "docu_file = Path('Resources/docu_headlines.csv')\n",
    "nflx_file = Path('Resources/nflx_headlines.csv')\n",
    "nke_file = Path('Resources/nke_headlines.csv')\n",
    "pg_file = Path('Resources/pg_headlines.csv')\n",
    "spy_file = Path('Resources/SPY_HEADLINES.csv')\n",
    "tsla_file = Path('Resources/TSLA_HEADLINES.csv')\n",
    "\n",
    "aapl_headlines_df = pd.read_csv(aapl_file)\n",
    "amzn_headlines_df = pd.read_csv(amzn_file)\n",
    "docu_headlines_df = pd.read_csv(docu_file)\n",
    "nflx_headlines_df = pd.read_csv(nflx_file)\n",
    "nke_headlines_df = pd.read_csv(nke_file)\n",
    "pg_headlines_df = pd.read_csv(pg_file)\n",
    "spy_headlines_df = pd.read_csv(spy_file)\n",
    "tsla_headlines_df = pd.read_csv(tsla_file)\n",
    "aapl_headlines_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = 0  # Neutral by default\n",
    "    if score >= 0.05:  # Positive\n",
    "        result = 1\n",
    "    elif score <= -0.05:  # Negative\n",
    "        result = -1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_df(df):\n",
    "    \"\"\"\n",
    "    Takes headlines DataFrame & creates DataFrame with Sentiment columns.\n",
    "    Splits Date & Time, creates Time column and moves Date to Index.\n",
    "    \"\"\"\n",
    "    title_sent = {\n",
    "        \"compound\": [],\n",
    "        \"positive\": [],\n",
    "        \"neutral\": [],\n",
    "        \"negative\": [],\n",
    "        \"sentiment\": [],\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Sentiment scoring with VADER\n",
    "            title_sentiment = analyzer.polarity_scores(row[\"Headline\"])\n",
    "            title_sent[\"compound\"].append(title_sentiment[\"compound\"])\n",
    "            title_sent[\"positive\"].append(title_sentiment[\"pos\"])\n",
    "            title_sent[\"neutral\"].append(title_sentiment[\"neu\"])\n",
    "            title_sent[\"negative\"].append(title_sentiment[\"neg\"])\n",
    "            title_sent[\"sentiment\"].append(get_sentiment(title_sentiment[\"compound\"]))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    title_sent_df = pd.DataFrame(title_sent)\n",
    "    #title_sent_df.head()\n",
    "\n",
    "    headline_sentiment_df = df.join(title_sent_df)\n",
    "    headline_sentiment_df.dropna()\n",
    "    headline_sentiment_df['Date'] = headline_sentiment_df['Date'].str.replace('at','-')\n",
    "    headline_sentiment_df['Date'] = headline_sentiment_df['Date'].str.split('-').str[0]\n",
    "    headline_sentiment_df = headline_sentiment_df.reindex(columns=['Date', 'Headline', 'compound', 'positive', 'neutral', 'negative', 'sentiment'])\n",
    "    headline_sentiment_df['Date'] = pd.to_datetime(headline_sentiment_df['Date'])\n",
    "    headline_sentiment_df.set_index('Date')\n",
    "    return headline_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(headlines_df, stock_info):\n",
    "    \"\"\"\n",
    "    Takes imported headlines_df, creates sentiment score, restructures data and\n",
    "    concats with stock info. \n",
    "    \"\"\"\n",
    "    headlines = create_sentiment_df(headlines_df)\n",
    "    scores = headlines.groupby('Date').mean().sort_values(by='Date')\n",
    "    scores = scores.drop(columns='compound')\n",
    "    complete = pd.concat([scores,stock_info], join='outer', axis=1).dropna()\n",
    "    complete['predicted pct change'] = complete['pct change'].shift()\n",
    "    complete = complete.dropna()\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data with data_clean() for all stocks\n",
    "aapl_complete = data_clean(aapl_headlines_df,aapl_stock_info)\n",
    "amzn_complete = data_clean(amzn_headlines_df,amzn_stock_info)\n",
    "docu_complete = data_clean(docu_headlines_df,docu_stock_info)\n",
    "nflx_complete = data_clean(nflx_headlines_df,nflx_stock_info)\n",
    "nke_complete = data_clean(nke_headlines_df,nke_stock_info)\n",
    "pg_complete = data_clean(pg_headlines_df,pg_stock_info)\n",
    "spy_complete = data_clean(spy_headlines_df,spy_stock_info)\n",
    "tsla_complete = data_clean(tsla_headlines_df,tsla_stock_info)\n",
    "aapl_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aapl_headlines = create_sentiment_df(aapl_headlines_df)\n",
    "amzn_headlines = create_sentiment_df(amzn_headlines_df)\n",
    "docu_headlines = create_sentiment_df(docu_headlines_df)\n",
    "nflx_headlines = create_sentiment_df(nflx_headlines_df)\n",
    "nke_headlines = create_sentiment_df(nke_headlines_df)\n",
    "pg_headlines = create_sentiment_df(pg_headlines_df)\n",
    "spy_headlines = create_sentiment_df(spy_headlines_df)\n",
    "tsla_headlines = create_sentiment_df(tsla_headlines_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# find average sentiment score by date\n",
    "aapl_scores = aapl_headlines.groupby('Date').mean().sort_values(by='Date')\n",
    "amzn_scores = amzn_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "docu_scores = docu_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "nflx_scores = nflx_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "nke_scores = nke_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "pg_scores = pg_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "spy_scores = spy_headlines.groupby(['Date']).mean().sort_values(by='Date')\n",
    "tsla_scores = tsla_headlines.groupby(['Date']).mean().sort_values(by='Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#drop compund col on all scores\n",
    "aapl_scores = aapl_scores.drop(columns='compound')\n",
    "amzn_scores = amzn_scores.drop(columns='compound')\n",
    "docu_scores = docu_scores.drop(columns='compound')\n",
    "nflx_scores = nflx_scores.drop(columns='compound')\n",
    "nke_scores = nke_scores.drop(columns='compound')\n",
    "pg_scores = pg_scores.drop(columns='compound')\n",
    "spy_scores = spy_scores.drop(columns='compound')\n",
    "tsla_scores = tsla_scores.drop(columns='compound')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sentiment scores distribution across each df poss use histogram, calc meanstd, or percentiles \n",
    "aapl_complete = pd.concat([aapl_scores,aapl_stock_info], join='outer', axis=1).dropna()\n",
    "amzn_complete = pd.concat([amzn_scores,amzn_stock_info], join='outer', axis=1).dropna()\n",
    "docu_complete = pd.concat([docu_scores,docu_stock_info], join='outer', axis=1).dropna()\n",
    "nflx_complete = pd.concat([nflx_scores,nflx_stock_info], join='outer', axis=1).dropna()\n",
    "nke_complete = pd.concat([nke_scores,nke_stock_info], join='outer', axis=1).dropna()\n",
    "pg_complete = pd.concat([pg_scores,pg_stock_info], join='outer', axis=1).dropna()\n",
    "spy_complete = pd.concat([spy_scores,spy_stock_info], join='outer', axis=1).dropna()\n",
    "tsla_complete = pd.concat([tsla_scores,tsla_stock_info], join='outer', axis=1).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: shift aapl_complete['pct change'] one day on all dfs\n",
    "# TO DO: dropna() on all df['predicted pct change'] cols \n",
    "aapl_complete['predicted pct change'] = aapl_complete['pct change'].shift()\n",
    "amzn_complete['predicted pct change'] = amzn_complete['pct change'].shift()\n",
    "docu_complete['predicted pct change'] = docu_complete['pct change'].shift()\n",
    "nflx_complete['predicted pct change'] = nflx_complete['pct change'].shift()\n",
    "nke_complete['predicted pct change'] = nke_complete['pct change'].shift()\n",
    "pg_complete['predicted pct change'] = pg_complete['pct change'].shift()\n",
    "spy_complete['predicted pct change'] = spy_complete['pct change'].shift()\n",
    "tsla_complete['predicted pct change'] = tsla_complete['pct change'].shift()\n",
    "\n",
    "aapl_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aapl_complete = aapl_complete.dropna()\n",
    "amzn_complete = amzn_complete.dropna()\n",
    "tsla_complete = tsla_complete.dropna()\n",
    "spy_complete = spy_complete.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(df):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment based on the compound score.\n",
    "    \"\"\"\n",
    "    result = [\n",
    "        (df.iloc[:,3] >= 0.01),\n",
    "        (df.iloc[:,3] <= 0.00)\n",
    "    ]\n",
    "    \n",
    "    values = [0, 1]\n",
    "    \n",
    "    df['buy/sell'] = np.select(result, values)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_sentiment = get_sentiment(aapl_complete)\n",
    "amzn_sentiment = get_sentiment(amzn_complete)\n",
    "docu_sentiment = get_sentiment(docu_complete)\n",
    "nflx_sentiment = get_sentiment(nflx_complete)\n",
    "nke_sentiment = get_sentiment(nke_complete)\n",
    "pg_sentiment = get_sentiment(pg_complete)\n",
    "spy_sentiment = get_sentiment(spy_complete)\n",
    "tsla_sentiment = get_sentiment(tsla_complete)\n",
    "aapl_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def regression_analysis(df):\n",
    "    y = df['buy/sell']\n",
    "    X = df.drop(columns=['buy/sell', 'pct change', 'close', 'positive', 'neutral', 'negative', 'sentiment'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,  stratify=y)\n",
    "\n",
    "    classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "    print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")\n",
    "    predictions = classifier.predict(X_test)\n",
    "    results = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "aapl_analysis = regression_analysis(aapl_sentiment)\n",
    "amzn_analysis = regression_analysis(amzn_sentiment)\n",
    "docu_analysis = regression_analysis(docu_sentiment)\n",
    "nflx_analysis = regression_analysis(nflx_sentiment)\n",
    "nke_analysis = regression_analysis(nke_sentiment)\n",
    "pg_analysis = regression_analysis(pg_sentiment)\n",
    "spy_analysis = regression_analysis(spy_sentiment)\n",
    "tsla_analysis = regression_analysis(tsla_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y = aapl_complete_sentiment['buy/sell']\n",
    "X = aapl_complete_sentiment.drop(columns=['buy/sell', 'pct change', 'close', 'positive', 'neutral', 'negative', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,  stratify=y)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {classifier.accuracy_score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "results = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
